{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_btc_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "SBj8vZYdPqLi",
        "N0EyqtdYPUa1",
        "Q3yq8bytPUa4",
        "5LwUkijVVrQv",
        "fnC2pAmOTTmr",
        "KLNF-FQXzxJm",
        "U1uAB1p33FOk",
        "WgEdy7VP8Y3K",
        "QQzJHt1t04Q0",
        "Ka0gbU_-BwvY",
        "LHfXJv19PUbK",
        "R8ZUFjzAPUba",
        "whJ9TGvXu91T",
        "zEhs--jkfUGa",
        "Ci9PWyjRPUbW",
        "dagH0eMcPUbd",
        "j_Sg1xu6LxqK",
        "C-QvA-Tif6Ea",
        "AFfQuQf0kWzT",
        "DDUupONTK1nL",
        "yJuycwBmK4ss",
        "Dhn37Vq7K8vp",
        "E7fk-6q_JBwg",
        "8Q-u5FZdicm5",
        "0w00r7I1p0Ck"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickvanhulst/twitter_bitcoin_analysis/blob/master/models/CNN_btc_kfold_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SPp1NwV_ytyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TODO"
      ]
    },
    {
      "metadata": {
        "id": "_thxODBvyxmr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Autostop <-- DONE.\n",
        "* Extend K-Fold. <- DONE\n",
        "* Make sure to save results <-- DONE\n",
        "* Add results. <-- retrieve results from npy dict --> https://stackoverflow.com/questions/30811918/saving-dictionary-of-numpy-arrays/45661259 <- DONE\n",
        "* ADD MILESTONE 9 + 10 to BrightSpace.\n",
        "* Visualize network https://github.com/szagoruyko/pytorchviz\n",
        "* Apply styling, see brightspace.\n",
        "* Apply requirements for literature section.\n",
        "* Clean up code so that it can be delivered <-- just deliver K-Fold code + preprocessing stuff.\n",
        "* https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/p152-donkers.pdf <-- go through this paper to check for missing information that I need to add.\n",
        "\n",
        "Experimental setup:\n",
        "----\n",
        "* TFIDF\n",
        "* TFIDF_w_user_data\n",
        "* TFIDF_w_btc_MD\n",
        "* TFIDF_w_all\n",
        "* REPEAT FOR COUNT AND BINARY.\n",
        "----\n",
        "* P_TFIDF\n",
        "* P_TFIDF_w_user_data\n",
        "* P_TFIDF_w_btc_MD\n",
        "* P_TFIDF_w_all\n",
        "* REPEAT FOR COUNT AND BINARY.\n",
        "----\n",
        "* Custom features\n",
        "* w_user_data\n",
        "* w_btc_MD\n",
        "* w_all\n",
        "\n",
        "\n",
        "Probably best to do a table per model.\n",
        "\n",
        "20*3 tests (three batch sizes) in total.\n",
        "\n",
        "Table columns: model, batch_size (32, 64, 128), MAP (TR/VAL), MAP test, LR MAP Test, avg MAP Test.\n",
        "\n",
        "Perhaps something like, scores in order of batch_size 32, 64, 128, so then I can do (0.48, 0.59, 0.6). "
      ]
    },
    {
      "metadata": {
        "id": "SBj8vZYdPqLi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "QCCVsRY6PpRK",
        "colab_type": "code",
        "outputId": "3c3e45bb-f691-476e-96d1-71fa367cbb61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "!pip install git+git://github.com/mickvanhulst/livelossplot.git --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5c176000 @  0x7f9a10fe12a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "0.4.0\n",
            "True\n",
            "Collecting git+git://github.com/mickvanhulst/livelossplot.git\n",
            "  Cloning git://github.com/mickvanhulst/livelossplot.git to /tmp/pip-req-build-3ac2oh1k\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.2.2) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: notebook in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.2.2) (5.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (5.4.0)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.5.3)\n",
            "Requirement already satisfied, skipping upgrade: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.6.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (5.2.4)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->livelossplot==0.2.2) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->livelossplot==0.2.2) (40.6.3)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (0.5.0)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->livelossplot==0.2.2) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->livelossplot==0.2.2) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->livelossplot==0.2.2) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->livelossplot==0.2.2) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook->livelossplot==0.2.2) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->livelossplot==0.2.2) (17.0.0)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.2.2) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (4.6.0)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (1.0.15)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (0.1.7)\n",
            "Building wheels for collected packages: livelossplot\n",
            "  Running setup.py bdist_wheel for livelossplot ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-_yna9y3g/wheels/73/e2/09/79ee9f5b0be18dc9c6ee17f0dc181708c1cf9513c70053ca92\n",
            "Successfully built livelossplot\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "N0EyqtdYPUa1"
      },
      "cell_type": "markdown",
      "source": [
        "# Load packages"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OnJQN5ZkPUa2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as utils\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import os\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bvuNT6g8NkGY",
        "colab_type": "code",
        "outputId": "bb16967f-d760-46d5-b45e-6e1880c9fe93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q3yq8bytPUa4"
      },
      "cell_type": "markdown",
      "source": [
        "# Load matrices"
      ]
    },
    {
      "metadata": {
        "id": "Kbe6Xz6Jdwyu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tryint(s):\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return s\n",
        "     \n",
        "def alphanum_key(s):\n",
        "    \"\"\" Turn a string into a list of string and number chunks.\n",
        "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
        "    \"\"\"\n",
        "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
        "\n",
        "def sort_nicely(l):\n",
        "    \"\"\" Sort the given list in the way that humans expect.\n",
        "    \n",
        "    amazing <3 RegExp\n",
        "    \"\"\"\n",
        "    l.sort(key=alphanum_key)\n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IMcg_Cgzgl3P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_type = ['keywords_tfidf_p', 'keywords_tfidf', 'custom_features'][2]\n",
        "mode = ['count', 'binary'][0]\n",
        "INPUT_SIZE_CNN = 25 if model_type == 'keywords_clusters' else 126 if 'custom' in model_type else 250\n",
        "train, test = None, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9dW6gscJPUa_",
        "outputId": "1a32fab5-d242-4030-f766-58c64fcb6a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_classes, train_user_data, train_matrices, train_dates, train_btc_meta_data, train_classes_t = None, None, None, None, None, None\n",
        "base_str = '/content/gdrive/My Drive/Colab Notebooks/data/btc/{}/{}/train/'.format(model_type, mode)\n",
        "files = os.listdir(base_str)\n",
        "for train_file in sort_nicely(files):\n",
        "    if 'btc' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_btc_meta_data is None:\n",
        "            train_btc_meta_data = temp\n",
        "        else:\n",
        "            train_btc_meta_data = np.concatenate((train_btc_meta_data, temp), axis=0)\n",
        "    elif 'tc_up' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_classes_t is None:\n",
        "            train_classes_t = temp\n",
        "        else:\n",
        "            train_classes_t = np.concatenate((train_classes_t, temp), axis=0)\n",
        "    elif 'c_up' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_classes is None:\n",
        "            train_classes = temp\n",
        "        else:\n",
        "            train_classes = np.concatenate((train_classes, temp), axis=0)\n",
        "    elif 'md_up' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_user_data is None:\n",
        "            train_user_data = temp\n",
        "        else:\n",
        "            train_user_data = np.vstack((train_user_data, temp))\n",
        "    elif 'dp_up' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_matrices is None:\n",
        "            train_matrices = temp\n",
        "        else:\n",
        "            train_matrices = np.vstack((train_matrices, temp))\n",
        "    elif 'dt_up' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_dates is None:\n",
        "            train_dates = temp\n",
        "        else:\n",
        "            train_dates = np.concatenate((train_dates, temp), axis=0)\n",
        "\n",
        "print('Train shapes: ', train_classes_t.shape, train_classes.shape, train_user_data.shape, \n",
        "      train_matrices.shape, train_dates.shape, train_btc_meta_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shapes:  (127177,) (127177,) (127177, 8) (127177, 126, 7) (127177,) (127177, 7, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sfl74a3cgYZl",
        "colab_type": "code",
        "outputId": "16358ee5-fbca-48b8-dd7a-e713dccacca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_classes, test_user_data, test_matrices, test_dates, test_btc_meta_data, test_classes_t = None, None, None, None, None, None\n",
        "base_str = '/content/gdrive/My Drive/Colab Notebooks/data/btc/{}/{}/test/'.format(model_type, mode)\n",
        "\n",
        "for test_file in sort_nicely(os.listdir(base_str)):\n",
        "    if 'btc' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_btc_meta_data is None:\n",
        "            test_btc_meta_data = temp\n",
        "        else:\n",
        "            test_btc_meta_data = np.concatenate((test_btc_meta_data, temp), axis=0)\n",
        "    elif 'tc_up' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_classes_t is None:\n",
        "            test_classes_t = temp\n",
        "        else:\n",
        "            test_classes_t = np.concatenate((test_classes_t, temp), axis=0)\n",
        "    elif 'c_up' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_classes is None:\n",
        "            test_classes = temp\n",
        "        else:\n",
        "            test_classes = np.concatenate((test_classes, temp), axis=0)\n",
        "    elif 'md_up' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_user_data is None:\n",
        "            test_user_data = temp\n",
        "        else:\n",
        "            test_user_data = np.vstack((test_user_data, temp))\n",
        "    elif 'dp_up' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_matrices is None:\n",
        "            test_matrices = temp\n",
        "        else:\n",
        "            test_matrices = np.vstack((test_matrices, temp))\n",
        "    elif 'dt_up' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_dates is None:\n",
        "            test_dates = temp\n",
        "        else:\n",
        "            test_dates = np.concatenate((test_dates, temp), axis=0)\n",
        "            \n",
        "print('Test shapes: ', test_classes_t.shape, test_classes.shape, test_user_data.shape, \n",
        "      test_matrices.shape, test_dates.shape, test_btc_meta_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test shapes:  (35805,) (35805,) (35805, 8) (35805, 126, 7) (35805,) (35805, 7, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wcw9bw0dMXeK",
        "colab_type": "code",
        "outputId": "08155083-685c-48c4-8907-dea48ff36e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# test_btc_meta_data = test_btc_meta_data[:,3:, :]\n",
        "# train_btc_meta_data = train_btc_meta_data[:,3:, :]\n",
        "# test_btc_meta_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21853, 5, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 539
        }
      ]
    },
    {
      "metadata": {
        "id": "5LwUkijVVrQv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Re-assign userIds and normalize data"
      ]
    },
    {
      "metadata": {
        "id": "HGXopB4KnpHH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Re-assign tokens for userIds as this makes normalization easier (no precision errors).\n",
        "unique_users = np.unique(np.concatenate((np.unique(train_user_data[:,-1]), np.unique(test_user_data[:,-1]))))\n",
        "user_ids = {}\n",
        "for i, v in enumerate(unique_users):\n",
        "    user_ids[v] = (i+1)\n",
        "\n",
        "for i, v in enumerate(train_user_data):\n",
        "    train_user_data[i,-1] = user_ids[v[-1]]\n",
        "    \n",
        "for i, v in enumerate(test_user_data):\n",
        "    test_user_data[i,-1] = user_ids[v[-1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRruqobUrmtr",
        "colab_type": "code",
        "outputId": "c518b655-a5ec-42b0-b395-c17304a48b19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_user_data[0,-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4169.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "vr_t-RUxeRRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(train_user_data)\n",
        "train_user_data = scaler.transform(train_user_data)\n",
        "test_user_data = scaler.transform(test_user_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bzicDTE0XAK9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scalers = {}\n",
        "for i in range(train_btc_meta_data.shape[1]):\n",
        "    scalers[i] = StandardScaler()\n",
        "    train_btc_meta_data[:, i, :] = scalers[i].fit_transform(train_btc_meta_data[:, i, :]) \n",
        "\n",
        "for i in range(test_btc_meta_data.shape[1]):\n",
        "    test_btc_meta_data[:, i, :] = scalers[i].transform(test_btc_meta_data[:, i, :]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inR0v7ThNaYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scalers = {}\n",
        "for i in range(train_matrices.shape[1]):\n",
        "    scalers[i] = StandardScaler()\n",
        "    train_matrices[:, i, :] = scalers[i].fit_transform(train_matrices[:, i, :]) \n",
        "\n",
        "for i in range(test_matrices.shape[1]):\n",
        "    test_matrices[:, i, :] = scalers[i].transform(test_matrices[:, i, :]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fnC2pAmOTTmr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Small test to see if flattening matrix and predicting directly with LR would have worked (absolutely sucks xD)"
      ]
    },
    {
      "metadata": {
        "id": "P5KGUSqkM67n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "X_inter = train_matrices[:, :int(train_matrices.shape[1]), :]\n",
        "X_train = X_inter.reshape(X_inter.shape[0], -1)\n",
        "\n",
        "X_train = X_train[:, :int(X_train.shape[1]/4)]\n",
        "y_train = train_classes\n",
        "# train_matrices = None\n",
        "\n",
        "# instantiate the model (using the default parameters)\n",
        "logreg = LogisticRegression(C=0.4, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zJMIpIXQO1tE",
        "colab_type": "code",
        "outputId": "c0ac23d3-8d0a-417e-ab0c-baa3a9ae1d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "cell_type": "code",
      "source": [
        "# fit the model with data\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.4, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "l8gIAPB9S7R0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = logreg.predict_proba(X_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNqj6juFS9nV",
        "colab_type": "code",
        "outputId": "f19cc7a9-d569-4a71-a524-3aebfbf1e386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "FPoIsmECO4YM",
        "colab_type": "code",
        "outputId": "ebf9bbb1-c405-4d2f-a09a-0c0f5be7878b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = logreg.predict(X_train)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_train, y_pred)\n",
        "cnf_matrix\n",
        "\n",
        "%matplotlib inline\n",
        "class_names=[0,1] # name  of classes\n",
        "fig, ax = plt.subplots()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "\n",
        "# create heatmap\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,257.44,'Predicted label')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAFACAYAAAACklMsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f85b8041320>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ZVd7TZREaZJS",
        "colab_type": "code",
        "outputId": "1f6e59a3-898b-420b-8611-81ea1faea73f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "auc = average_precision_score(y_train, y_pred[:,1])\n",
        "auc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4986545768362581"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "NJE1yHs0ajLm",
        "colab_type": "code",
        "outputId": "d75e5550-bec7-41a3-aa06-ebec71a51b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.fixes import signature\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# auc, y, pred = test_model(test_loader, multi_class)\n",
        "# losses, auc, y, pred = validate_model(val_loader, multi_class)\n",
        "# losses, auc, y, pred = train_model(train_loader, multi_class)\n",
        "\n",
        "# preds = [1 if x > 0.5 else 0 for x in pred]\n",
        "# print(np.unique(preds, return_counts=True))\n",
        "# print(np.unique(y, return_counts=True))\n",
        "# print('The accuracy is: {}'.format(accuracy_score(y, preds)))\n",
        "y, pred = y_train, y_pred[:,1]\n",
        "\n",
        "average_precision = average_precision_score(y, pred)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y, pred)\n",
        "\n",
        "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
        "step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "plt.step(recall, precision, color='b', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
        "          average_precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average precision-recall score: 0.50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'2-class Precision-Recall curve: AP=0.50')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f85b80ccd68>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "KLNF-FQXzxJm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Some analysis"
      ]
    },
    {
      "metadata": {
        "id": "U1uAB1p33FOk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## See which test users are not in train"
      ]
    },
    {
      "metadata": {
        "id": "MRuFrHZR3LMN",
        "colab_type": "code",
        "outputId": "5d7262c1-0bea-4fac-cca2-475945689c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "indices = []\n",
        "test_user_ids = np.unique(test_user_data[:,-1])\n",
        "for i in range(len(train_user_data[:,-1])):\n",
        "    if train_user_data[i,-1] not in test_user_ids:\n",
        "        indices.append(i)\n",
        "        \n",
        "len(indices) / len(train_user_data[:,-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.014578107676702548"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "metadata": {
        "id": "9fk8cFNVPKnD",
        "colab_type": "code",
        "outputId": "80e9197c-8e36-4a85-8ec3-eb81e0bc2c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_matrices = np.delete(train_matrices, indices, axis=0)\n",
        "train_classes_t = np.delete(train_classes_t, indices)\n",
        "train_classes = np.delete(train_classes, indices)\n",
        "train_user_data = np.delete(train_user_data, indices, axis=0)\n",
        "train_btc_meta_data = np.delete(train_btc_meta_data, indices, axis=0)\n",
        "train_dates = np.delete(train_dates, indices)\n",
        "train_matrices.shape, train_classes_t.shape, train_user_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((125323, 126, 7), (125323,), (125323, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "WgEdy7VP8Y3K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check distribution of user-data to find important users."
      ]
    },
    {
      "metadata": {
        "id": "4dbvtL_W8XyM",
        "colab_type": "code",
        "outputId": "5bcd9f8b-b3b7-4772-ce81-61848fb84274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "TODO: find important users and filter entire dataset, including test set based on\n",
        "those userIds. So every user that occurs in the test set, should also occur in \n",
        "training set. Else, it's removed.\n",
        "\n",
        "Perhaps that should be my main criteria, not the amount of followers.\n",
        "'''\n",
        "# [['verified', 'followers_count', 'friends_count',\n",
        "#                                'listed_count', 'favourites_count', 'statuses_count', 'unix_months',\n",
        "#                                'twitter_user_id']]\n",
        "\n",
        "\n",
        "# Histogram\n",
        "heights,bins = np.histogram(copy_train_user_data[:, 1],bins=50)\n",
        "\n",
        "# Normalize\n",
        "heights = heights/float(sum(heights))\n",
        "binMids=bins[:-1]+np.diff(bins)/2.\n",
        "plt.plot(binMids,heights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff5a8be5588>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 321
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff5a92af518>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6lSx9HHg_bGC",
        "colab_type": "code",
        "outputId": "b8b75b85-ee5b-4576-c6ff-02ac5c12260f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "perc = 10\n",
        "indices = np.argwhere(train_user_data[:, 1] <= np.percentile(train_user_data[:, 1], perc)).flatten()\n",
        "\n",
        "train_matrices = np.delete(train_matrices, indices, axis=0)\n",
        "train_classes_t = np.delete(train_classes_t, indices)\n",
        "train_classes = np.delete(train_classes, indices)\n",
        "train_user_data = np.delete(train_user_data, indices, axis=0)\n",
        "train_btc_meta_data = np.delete(train_btc_meta_data, indices, axis=0)\n",
        "train_dates = np.delete(train_dates, indices)\n",
        "train_matrices.shape, train_classes_t.shape, train_user_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((112362, 250, 7), (112362,), (112362, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "yyV9pSuIO0EY",
        "colab_type": "code",
        "outputId": "b34f09c2-b665-439f-8c6d-db3172f82fa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.unique(train_classes_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  0,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "metadata": {
        "id": "DP2onjgIH9Rc",
        "colab_type": "code",
        "outputId": "b26edecb-9101-41cb-94ba-968601290831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# indices = np.argwhere(test_user_data[:, 1] <= np.percentile(train_user_data[:, 1], perc)).flatten()\n",
        "\n",
        "# test_matrices = np.delete(test_matrices, indices, axis=0)\n",
        "# test_classes_t = np.delete(test_classes_t, indices)\n",
        "# test_user_data = np.delete(test_user_data, indices, axis=0)\n",
        "# test_btc_meta_data = np.delete(test_btc_meta_data, indices, axis=0)\n",
        "# test_dates = np.delete(test_dates, indices)\n",
        "# test_matrices.shape, test_classes_t.shape, test_user_data.shape,test_dates.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((15753, 500, 7), (15753,), (15753, 8), (15753,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "QQzJHt1t04Q0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compare occurences users training versus test set"
      ]
    },
    {
      "metadata": {
        "id": "f2FS5aGha6dP",
        "colab_type": "code",
        "outputId": "2daa0ca3-7d21-4016-888f-3e8d2a51aaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33800
        }
      },
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(train_user_data[:,-1], return_counts=True)\n",
        "cnt_train = dict(zip(unique, counts))\n",
        "unique, counts = np.unique(test_user_data[:,-1], return_counts=True)\n",
        "cnt_test = dict(zip(unique, counts))\n",
        "\n",
        "cnt_train, cnt_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({-1.8449034827976734: 8,\n",
              "  -1.8379357715339544: 1,\n",
              "  -1.8191229511219125: 44,\n",
              "  -1.8121552398581933: 1,\n",
              "  -1.8079746130999619: 5,\n",
              "  -1.8017036729626146: 7,\n",
              "  -1.7877682504351762: 1,\n",
              "  -1.7856779370560605: 19,\n",
              "  -1.780103768045085: 32,\n",
              "  -1.7794069969187132: 21,\n",
              "  -1.7752263701604816: 17,\n",
              "  -1.7612909476330432: 2,\n",
              "  -1.7592006342539275: 1,\n",
              "  -1.7515361518638364: 13,\n",
              "  -1.7417813560946296: 1,\n",
              "  -1.7403878138418856: 1,\n",
              "  -1.7396910427155137: 4,\n",
              "  -1.7257556201880755: 2,\n",
              "  -1.7243620779353315: 47,\n",
              "  -1.7111234265342652: 7,\n",
              "  -1.6992783173859425: 63,\n",
              "  -1.6902202927431076: 6,\n",
              "  -1.686039665984876: 19,\n",
              "  -1.685342894858504: 20,\n",
              "  -1.6804654969739006: 13,\n",
              "  -1.6790719547211568: 10,\n",
              "  -1.677678412468413: 22,\n",
              "  -1.6721042434574376: 1,\n",
              "  -1.6707107012046938: 4,\n",
              "  -1.6700139300783219: 3,\n",
              "  -1.6651365321937184: 26,\n",
              "  -1.6623494476882308: 1,\n",
              "  -1.6616526765618589: 1,\n",
              "  -1.660955905435487: 67,\n",
              "  -1.660259134309115: 2,\n",
              "  -1.6595623631827432: 19,\n",
              "  -1.650504338539908: 62,\n",
              "  -1.6498075674135362: 10,\n",
              "  -1.644930169528933: 1,\n",
              "  -1.644233398402561: 34,\n",
              "  -1.643536627276189: 3,\n",
              "  -1.6379624582652137: 42,\n",
              "  -1.635872144886098: 5,\n",
              "  -1.6268141202432629: 21,\n",
              "  -1.6198464089795437: 1,\n",
              "  -1.6142722399685685: 19,\n",
              "  -1.6121819265894526: 6,\n",
              "  -1.610091613210337: 17,\n",
              "  -1.608698070957593: 4,\n",
              "  -1.6080012998312212: 8,\n",
              "  -1.601730359693874: 2,\n",
              "  -1.5982465040620142: 6,\n",
              "  -1.5947626484301547: 120,\n",
              "  -1.5870981660400636: 11,\n",
              "  -1.585007852660948: 21,\n",
              "  -1.566891803375278: 1,\n",
              "  -1.5620144054906746: 35,\n",
              "  -1.5536531519742116: 39,\n",
              "  -1.550169296342352: 6,\n",
              "  -1.54947252521598: 2,\n",
              "  -1.5432015850786327: 13,\n",
              "  -1.5397177294467732: 1,\n",
              "  -1.5355371026885416: 12,\n",
              "  -1.5236919935402191: 10,\n",
              "  -1.5174210534028718: 1,\n",
              "  -1.5118468843918964: 1,\n",
              "  -1.5090597998864088: 26,\n",
              "  -1.505575944254549: 17,\n",
              "  -1.4825824970842758: 4,\n",
              "  -1.4784018703260442: 16,\n",
              "  -1.4763115569469285: 15,\n",
              "  -1.4756147858205566: 19,\n",
              "  -1.472827701315069: 5,\n",
              "  -1.4714341590623252: 6,\n",
              "  -1.4679503034304655: 5,\n",
              "  -1.4672535323040936: 16,\n",
              "  -1.4658599900513498: 17,\n",
              "  -1.46237613441949: 4,\n",
              "  -1.4616793632931182: 4,\n",
              "  -1.455408423155771: 3,\n",
              "  -1.4519245675239114: 3,\n",
              "  -1.4498342541447957: 17,\n",
              "  -1.4240537224690346: 17,\n",
              "  -1.4233569513426627: 2,\n",
              "  -1.421266637963547: 1,\n",
              "  -1.4198730957108032: 2,\n",
              "  -1.4163892400789435: 20,\n",
              "  -1.4156924689525716: 1,\n",
              "  -1.410815071067968: 3,\n",
              "  -1.4094215288152243: 19,\n",
              "  -1.4073312154361086: 2,\n",
              "  -1.4038473598042491: 19,\n",
              "  -1.4010602752987613: 13,\n",
              "  -1.3982731907932737: 54,\n",
              "  -1.39687964854053: 1,\n",
              "  -1.3759765147493723: 7,\n",
              "  -1.3683120323592812: 4,\n",
              "  -1.3627378633483058: 20,\n",
              "  -1.3606475499691901: 8,\n",
              "  -1.3599507788428182: 25,\n",
              "  -1.3578604654637023: 2,\n",
              "  -1.3508927541999831: 9,\n",
              "  -1.3494992119472393: 2,\n",
              "  -1.3397444161780325: 1,\n",
              "  -1.3285960781560817: 2,\n",
              "  -1.3251122225242222: 2,\n",
              "  -1.3209315957659906: 1,\n",
              "  -1.3167509690077592: 9,\n",
              "  -1.3139638845022714: 7,\n",
              "  -1.3076929443649243: 20,\n",
              "  -1.302118775353949: 1,\n",
              "  -1.2972413774693454: 11,\n",
              "  -1.2916672084583702: 5,\n",
              "  -1.2819124126891632: 6,\n",
              "  -1.2805188704364194: 6,\n",
              "  -1.2742479302990721: 23,\n",
              "  -1.2707640746672126: 93,\n",
              "  -1.2651899056562372: 4,\n",
              "  -1.2540415676342864: 4,\n",
              "  -1.249860940876055: 15,\n",
              "  -1.2491641697496831: 12,\n",
              "  -1.2442867718650796: 1,\n",
              "  -1.233835204969501: 1,\n",
              "  -1.233138433843129: 11,\n",
              "  -1.2296545782112693: 4,\n",
              "  -1.217112697936575: 4,\n",
              "  -1.215022384557459: 1,\n",
              "  -1.2101449866728557: 40,\n",
              "  -1.2080546732937398: 32,\n",
              "  -1.207357902167368: 4,\n",
              "  -1.2059643599146241: 5,\n",
              "  -1.2003901909036487: 3,\n",
              "  -1.198996648650905: 12,\n",
              "  -1.1976031063981611: 8,\n",
              "  -1.190635395134442: 3,\n",
              "  -1.1773967437333754: 17,\n",
              "  -1.1760032014806316: 37,\n",
              "  -1.1753064303542597: 7,\n",
              "  -1.1746096592278878: 3,\n",
              "  -1.1690354902169124: 7,\n",
              "  -1.165551634585053: 2,\n",
              "  -1.1627645500795651: 18,\n",
              "  -1.1509194409312427: 56,\n",
              "  -1.1411646451620356: 20,\n",
              "  -1.1397711029092918: 39,\n",
              "  -1.1279259937609694: 35,\n",
              "  -1.1272292226345975: 16,\n",
              "  -1.1258356803818534: 2,\n",
              "  -1.1237453670027377: 6,\n",
              "  -1.1146873423599029: 3,\n",
              "  -1.1091131733489275: 12,\n",
              "  -1.1077196310961837: 4,\n",
              "  -1.105629317717068: 78,\n",
              "  -1.1000551487060926: 1,\n",
              "  -1.0993583775797207: 1,\n",
              "  -1.0930874374423734: 4,\n",
              "  -1.0916938951896296: 1,\n",
              "  -1.087513268431398: 4,\n",
              "  -1.086816497305026: 2,\n",
              "  -1.0861197261786542: 126,\n",
              "  -1.0854229550522823: 5,\n",
              "  -1.0847261839259104: 6,\n",
              "  -1.0749713881567036: 11,\n",
              "  -1.0728810747775877: 11,\n",
              "  -1.070790761398472: 7,\n",
              "  -1.0687004480193563: 7,\n",
              "  -1.061732736755637: 1,\n",
              "  -1.0603391945028933: 2,\n",
              "  -1.0561585677446617: 9,\n",
              "  -1.0491908564809425: 2,\n",
              "  -1.044313458596339: 4,\n",
              "  -1.0394360607117357: 23,\n",
              "  -1.018532926920578: 7,\n",
              "  -1.0143523001623465: 2,\n",
              "  -1.0129587579096027: 36,\n",
              "  -1.0115652156568589: 5,\n",
              "  -1.010868444530487: 1,\n",
              "  -1.0080813600249994: 2,\n",
              "  -1.0011136487612802: 9,\n",
              "  -0.9997201065085363: 6,\n",
              "  -0.9990233353821644: 31,\n",
              "  -0.9969330220030486: 1,\n",
              "  -0.9955394797503048: 16,\n",
              "  -0.9941459374975609: 4,\n",
              "  -0.9913588529920733: 9,\n",
              "  -0.9906620818657014: 16,\n",
              "  -0.9885717684865856: 4,\n",
              "  -0.9816040572228664: 4,\n",
              "  -0.9774234304646349: 15,\n",
              "  -0.9683654058218: 3,\n",
              "  -0.9641847790635684: 2,\n",
              "  -0.9627912368108246: 14,\n",
              "  -0.9572170677998493: 6,\n",
              "  -0.9516428987888739: 73,\n",
              "  -0.950946127662502: 3,\n",
              "  -0.9460687297778986: 2,\n",
              "  -0.9446751875251547: 90,\n",
              "  -0.9418881030196671: 6,\n",
              "  -0.9384042473878075: 5,\n",
              "  -0.9356171628823198: 1,\n",
              "  -0.9342236206295759: 2,\n",
              "  -0.9307397649977164: 11,\n",
              "  -0.9237720537339972: 7,\n",
              "  -0.9230752826076252: 3,\n",
              "  -0.9223785114812533: 7,\n",
              "  -0.9195914269757657: 3,\n",
              "  -0.9175011135966499: 10,\n",
              "  -0.9126237157120465: 4,\n",
              "  -0.9105334023329307: 45,\n",
              "  -0.907746317827443: 9,\n",
              "  -0.9021721488164677: 17,\n",
              "  -0.9007786065637239: 54,\n",
              "  -0.8952044375527485: 1,\n",
              "  -0.891023810794517: 1,\n",
              "  -0.8896302685417732: 3,\n",
              "  -0.8889334974154012: 2,\n",
              "  -0.8854496417835416: 1,\n",
              "  -0.8784819305198225: 10,\n",
              "  -0.8777851593934505: 7,\n",
              "  -0.8763916171407067: 2,\n",
              "  -0.8743013037615909: 3,\n",
              "  -0.8673335924978718: 15,\n",
              "  -0.8638497368660122: 11,\n",
              "  -0.8631529657396402: 43,\n",
              "  -0.8520046277176895: 7,\n",
              "  -0.8513078565913176: 104,\n",
              "  -0.8450369164539704: 2,\n",
              "  -0.8443401453275984: 26,\n",
              "  -0.8436433742012265: 1,\n",
              "  -0.8429466030748546: 1,\n",
              "  -0.8359788918111354: 1,\n",
              "  -0.8269208671683005: 10,\n",
              "  -0.822043469283697: 1,\n",
              "  -0.8199531559045813: 37,\n",
              "  -0.8192563847782094: 1,\n",
              "  -0.8150757580199779: 1,\n",
              "  -0.813682215767234: 1,\n",
              "  -0.8108951312617464: 13,\n",
              "  -0.8046241911243991: 2,\n",
              "  -0.8039274199980272: 4,\n",
              "  -0.8025338777452833: 26,\n",
              "  -0.7990500221134237: 55,\n",
              "  -0.7913855397233326: 39,\n",
              "  -0.7906887685969607: 20,\n",
              "  -0.7899919974705888: 5,\n",
              "  -0.7892952263442169: 3,\n",
              "  -0.7816307439541258: 12,\n",
              "  -0.7774501171958943: 2,\n",
              "  -0.7760565749431504: 5,\n",
              "  -0.7753598038167785: 1,\n",
              "  -0.7607276101629682: 19,\n",
              "  -0.7551534411519929: 16,\n",
              "  -0.7523663566465052: 3,\n",
              "  -0.7412180186245545: 37,\n",
              "  -0.7377341629926949: 12,\n",
              "  -0.7356438496135791: 19,\n",
              "  -0.7349470784872072: 5,\n",
              "  -0.7328567651080915: 25,\n",
              "  -0.7272825960971161: 6,\n",
              "  -0.7251922827180004: 4,\n",
              "  -0.7244955115916284: 8,\n",
              "  -0.7189213425806531: 3,\n",
              "  -0.7175278003279093: 55,\n",
              "  -0.7147407158224216: 66,\n",
              "  -0.7133471735696777: 1,\n",
              "  -0.7119536313169339: 18,\n",
              "  -0.7091665468114462: 5,\n",
              "  -0.7015020644213551: 2,\n",
              "  -0.6994117510422394: 7,\n",
              "  -0.6959278954103798: 14,\n",
              "  -0.6924440397785202: 10,\n",
              "  -0.6833860151356852: 2,\n",
              "  -0.6826892440093133: 2,\n",
              "  -0.6805989306301976: 6,\n",
              "  -0.677115074998338: 17,\n",
              "  -0.6736312193664784: 1,\n",
              "  -0.6701473637346188: 7,\n",
              "  -0.668057050355503: 36,\n",
              "  -0.6617861102181558: 2,\n",
              "  -0.6610893390917838: 5,\n",
              "  -0.6541216278280647: 2,\n",
              "  -0.6534248567016927: 5,\n",
              "  -0.6527280855753208: 3,\n",
              "  -0.6415797475533701: 12,\n",
              "  -0.6401862053006263: 14,\n",
              "  -0.6325217229105352: 25,\n",
              "  -0.6304314095314194: 2,\n",
              "  -0.6297346384050475: 18,\n",
              "  -0.6199798426358406: 8,\n",
              "  -0.6192830715094687: 7,\n",
              "  -0.6178895292567249: 10,\n",
              "  -0.6144056736248653: 24,\n",
              "  -0.6137089024984934: 2,\n",
              "  -0.6032573356029146: 3,\n",
              "  -0.6018637933501708: 6,\n",
              "  -0.6011670222237988: 10,\n",
              "  -0.6004702510974269: 40,\n",
              "  -0.5921089975809639: 146,\n",
              "  -0.5900186842018481: 1,\n",
              "  -0.5893219130754762: 1,\n",
              "  -0.5851412863172447: 4,\n",
              "  -0.583050972938129: 14,\n",
              "  -0.5809606595590132: 9,\n",
              "  -0.5788703461798974: 2,\n",
              "  -0.5760832616744098: 41,\n",
              "  -0.5719026349161783: 37,\n",
              "  -0.5684187792843187: 15,\n",
              "  -0.5628446102733433: 55,\n",
              "  -0.5551801278832522: 9,\n",
              "  -0.5544833567568803: 145,\n",
              "  -0.5530898145041365: 9,\n",
              "  -0.5509995011250207: 6,\n",
              "  -0.5433350187349296: 44,\n",
              "  -0.5412447053558138: 1,\n",
              "  -0.53985116310307: 12,\n",
              "  -0.5370640785975823: 58,\n",
              "  -0.5335802229657227: 10,\n",
              "  -0.5307931384602351: 6,\n",
              "  -0.5293995962074912: 5,\n",
              "  -0.5182512581855405: 10,\n",
              "  -0.5119803180481933: 10,\n",
              "  -0.5084964624163337: 174,\n",
              "  -0.4994384377734988: 3,\n",
              "  -0.4987416666471268: 1,\n",
              "  -0.4980448955207549: 1,\n",
              "  -0.4848062441196885: 9,\n",
              "  -0.4813223884878289: 2,\n",
              "  -0.47156759271862203: 1,\n",
              "  -0.46529665258127473: 9,\n",
              "  -0.45763217019118363: 7,\n",
              "  -0.45275477230658023: 14,\n",
              "  -0.4485741455483487: 62,\n",
              "  -0.4471806032956049: 32,\n",
              "  -0.44648383216923293: 68,\n",
              "  -0.43881934977914183: 20,\n",
              "  -0.43463872302091033: 8,\n",
              "  -0.4339419518945384: 3,\n",
              "  -0.43185163851542263: 10,\n",
              "  -0.43115486738905073: 20,\n",
              "  -0.42070330049347193: 29,\n",
              "  -0.41582590260886854: 1,\n",
              "  -0.4151291314824966: 51,\n",
              "  -0.40816142021877744: 3,\n",
              "  -0.4074646490924055: 13,\n",
              "  -0.40467756458691784: 1,\n",
              "  -0.39004537093310754: 36,\n",
              "  -0.38586474417487604: 5,\n",
              "  -0.37959380403752874: 46,\n",
              "  -0.3768067195320411: 1,\n",
              "  -0.37471640615292534: 33,\n",
              "  -0.3740196350265534: 5,\n",
              "  -0.3733228639001815: 31,\n",
              "  -0.36914223714195: 14,\n",
              "  -0.36705192376283424: 12,\n",
              "  -0.36356806813097464: 3,\n",
              "  -0.3586906702463712: 4,\n",
              "  -0.34893587447716434: 3,\n",
              "  -0.34823910335079244: 46,\n",
              "  -0.3468455610980486: 55,\n",
              "  -0.34127139208707324: 32,\n",
              "  -0.33778753645521364: 3,\n",
              "  -0.335000451949726: 4,\n",
              "  -0.3280327406860068: 3,\n",
              "  -0.32663919843326295: 4,\n",
              "  -0.3203682582959157: 1,\n",
              "  -0.3134005470321965: 2,\n",
              "  -0.3050392935157335: 22,\n",
              "  -0.3015554378838739: 13,\n",
              "  -0.29737481112564235: 1,\n",
              "  -0.29667803999927045: 4,\n",
              "  -0.29598126887289855: 1,\n",
              "  -0.29319418436741085: 29,\n",
              "  -0.2911038709882951: 80,\n",
              "  -0.28483293085094785: 63,\n",
              "  -0.2841361597245759: 13,\n",
              "  -0.283439388598204: 8,\n",
              "  -0.2764716773344848: 7,\n",
              "  -0.2757749062081129: 5,\n",
              "  -0.275078135081741: 1,\n",
              "  -0.27438136395536905: 1,\n",
              "  -0.26741365269164985: 11,\n",
              "  -0.26253625480704645: 27,\n",
              "  -0.257658856922443: 9,\n",
              "  -0.25487177241695536: 2,\n",
              "  -0.2541750012905834: 3,\n",
              "  -0.2499943745323519: 3,\n",
              "  -0.24720729002686423: 1,\n",
              "  -0.23536218087854163: 12,\n",
              "  -0.23396863862579778: 3,\n",
              "  -0.23257509637305396: 14,\n",
              "  -0.23048478299393818: 14,\n",
              "  -0.22909124074119436: 3,\n",
              "  -0.22072998722473133: 4,\n",
              "  -0.2200332160983594: 12,\n",
              "  -0.2158525893401279: 1,\n",
              "  -0.21306550483464023: 94,\n",
              "  -0.21027842032915256: 9,\n",
              "  -0.20749133582366489: 94,\n",
              "  -0.20191716681268954: 19,\n",
              "  -0.20052362455994568: 9,\n",
              "  -0.19913008230720186: 14,\n",
              "  -0.19564622667534226: 1,\n",
              "  -0.19285914216985459: 2,\n",
              "  -0.19076882879073884: 2,\n",
              "  -0.1900720576643669: 24,\n",
              "  -0.1824075752742758: 1,\n",
              "  -0.18031726189516006: 7,\n",
              "  -0.17543986401055664: 17,\n",
              "  -0.1712592372523251: 16,\n",
              "  -0.16498829711497784: 5,\n",
              "  -0.16220121260949016: 21,\n",
              "  -0.16150444148311827: 6,\n",
              "  -0.16080767035674634: 1,\n",
              "  -0.15802058585125867: 3,\n",
              "  -0.13920776543921687: 2,\n",
              "  -0.13572390980735727: 1,\n",
              "  -0.13502713868098534: 3,\n",
              "  -0.1329368253018696: 20,\n",
              "  -0.13084651192275384: 23,\n",
              "  -0.12945296967001: 5,\n",
              "  -0.11691108939531547: 2,\n",
              "  -0.11133692038434012: 4,\n",
              "  -0.10854983587885245: 2,\n",
              "  -0.10785306475248053: 30,\n",
              "  -0.1029756668678771: 7,\n",
              "  -0.10227889574150519: 5,\n",
              "  -0.09670472673052984: 2,\n",
              "  -0.08904024434043872: 2,\n",
              "  -0.08834347321406681: 51,\n",
              "  -0.08555638870857914: 21,\n",
              "  -0.08485961758220723: 2,\n",
              "  -0.08276930420309146: 3,\n",
              "  -0.07580159293937228: 33,\n",
              "  -0.06744033942290925: 60,\n",
              "  -0.05977585703281815: 3,\n",
              "  -0.05698877252733048: 17,\n",
              "  -0.05629200140095856: 3,\n",
              "  -0.05559523027458664: 8,\n",
              "  -0.04514366337900786: 25,\n",
              "  -0.04375012112626402: 8,\n",
              "  -0.027724385219709897: 5,\n",
              "  -0.020059902829618795: 2,\n",
              "  -0.01169864931315577: 18,\n",
              "  -0.0047309380494365855: 1,\n",
              "  -0.003337395796692749: 3,\n",
              "  -0.0005503112912050746: 2,\n",
              "  0.00014645983516684387: 10,\n",
              "  0.0022367732142825995: 25,\n",
              "  0.009204484478001785: 1,\n",
              "  0.01199156898348946: 31,\n",
              "  0.012688340109861378: 4,\n",
              "  0.015475424615349051: 8,\n",
              "  0.023836678131812075: 3,\n",
              "  0.025230220384555913: 3,\n",
              "  0.03010761826915934: 8,\n",
              "  0.0321979316482751: 4,\n",
              "  0.03289470277464702: 7,\n",
              "  0.03846887178562236: 3,\n",
              "  0.04404304079659771: 16,\n",
              "  0.05449460769217649: 20,\n",
              "  0.056584921071292246: 9,\n",
              "  0.06494617458775527: 25,\n",
              "  0.07470097035696213: 7,\n",
              "  0.0774880548624498: 29,\n",
              "  0.08166868162068132: 2,\n",
              "  0.08584930837891283: 4,\n",
              "  0.08933316401077242: 15,\n",
              "  0.09002993513714433: 48,\n",
              "  0.09490733302174777: 25,\n",
              "  0.10187504428546695: 13,\n",
              "  0.1039653576645827: 5,\n",
              "  0.10675244217007038: 3,\n",
              "  0.10953952667555805: 14,\n",
              "  0.11162984005467382: 3,\n",
              "  0.11581046681290533: 14,\n",
              "  0.11790078019202108: 1,\n",
              "  0.118597551318393: 4,\n",
              "  0.12695880483485603: 20,\n",
              "  0.12835234708759985: 31,\n",
              "  0.13253297384583138: 3,\n",
              "  0.13462328722494712: 31,\n",
              "  0.13880391398317862: 2,\n",
              "  0.14646839637326975: 11,\n",
              "  0.15064902313150125: 1,\n",
              "  0.15134579425787317: 1,\n",
              "  0.1520425653842451: 3,\n",
              "  0.15552642101610467: 6,\n",
              "  0.16249413227982387: 1,\n",
              "  0.1638876745325677: 20,\n",
              "  0.16458444565893962: 11,\n",
              "  0.16528121678531155: 37,\n",
              "  0.16597798791168344: 149,\n",
              "  0.16667475903805537: 2,\n",
              "  0.1673715301644273: 22,\n",
              "  0.16806830129079922: 5,\n",
              "  0.17015861466991497: 4,\n",
              "  0.17224892804903072: 28,\n",
              "  0.17712632593363414: 39,\n",
              "  0.17921663931274992: 49,\n",
              "  0.18897143508195677: 53,\n",
              "  0.19524237521930404: 8,\n",
              "  0.2049971709885109: 15,\n",
              "  0.20639071324125474: 3,\n",
              "  0.20917779774674242: 87,\n",
              "  0.2119648822522301: 25,\n",
              "  0.21684228013683352: 2,\n",
              "  0.21753905126320544: 44,\n",
              "  0.21893259351594926: 28,\n",
              "  0.2196293646423212: 4,\n",
              "  0.22171967802143694: 8,\n",
              "  0.22241644914780886: 8,\n",
              "  0.22590030477966846: 72,\n",
              "  0.22799061815878421: 1,\n",
              "  0.2286873892851561: 21,\n",
              "  0.22938416041152804: 24,\n",
              "  0.2307777026642719: 4,\n",
              "  0.23286801604338764: 14,\n",
              "  0.23426155829613146: 13,\n",
              "  0.24122926955985066: 34,\n",
              "  0.24262281181259449: 20,\n",
              "  0.24401635406533834: 23,\n",
              "  0.24819698082356983: 2,\n",
              "  0.2530743787081733: 20,\n",
              "  0.25586146321366093: 13,\n",
              "  0.2565582343400329: 1,\n",
              "  0.2572550054664048: 6,\n",
              "  0.25864854771914864: 3,\n",
              "  0.2607388610982644: 2,\n",
              "  0.26213240335100824: 1,\n",
              "  0.2684033434883555: 13,\n",
              "  0.27119042799384313: 7,\n",
              "  0.2718871991202151: 9,\n",
              "  0.2795516815103062: 4,\n",
              "  0.28094522376305003: 18,\n",
              "  0.28233876601579383: 3,\n",
              "  0.28582262164765343: 1,\n",
              "  0.2913967906586288: 17,\n",
              "  0.29279033291137263: 6,\n",
              "  0.3053322131860672: 21,\n",
              "  0.3081192976915548: 55,\n",
              "  0.3122999244497863: 10,\n",
              "  0.31439023782890213: 9,\n",
              "  0.3164805512080179: 9,\n",
              "  0.3192676357135055: 2,\n",
              "  0.3206611779662494: 10,\n",
              "  0.3213579490926213: 1,\n",
              "  0.32205472021899323: 5,\n",
              "  0.3262353469772247: 2,\n",
              "  0.3297192026090843: 13,\n",
              "  0.3338998293673158: 11,\n",
              "  0.3352933716200597: 52,\n",
              "  0.3401707695046631: 8,\n",
              "  0.3443513962628946: 5,\n",
              "  0.3450481673892665: 4,\n",
              "  0.3471384807683823: 29,\n",
              "  0.3506223364002419: 1,\n",
              "  0.3554997342848453: 2,\n",
              "  0.3617706744221926: 1,\n",
              "  0.3631642166749364: 1,\n",
              "  0.3652545300540522: 14,\n",
              "  0.37013192793865557: 27,\n",
              "  0.37291901244414327: 7,\n",
              "  0.3826738082133501: 3,\n",
              "  0.3861576638452097: 11,\n",
              "  0.3896415194770693: 14,\n",
              "  0.39103506172981317: 2,\n",
              "  0.3966092307407885: 13,\n",
              "  0.39939631524627617: 3,\n",
              "  0.40288017087813577: 25,\n",
              "  0.4049704842572515: 1,\n",
              "  0.40706079763636727: 1,\n",
              "  0.40984788214185497: 7,\n",
              "  0.4196026779110618: 31,\n",
              "  0.42238976241654946: 30,\n",
              "  0.4307510159330125: 14,\n",
              "  0.4342348715648721: 1,\n",
              "  0.4356284138176159: 47,\n",
              "  0.43632518494398786: 10,\n",
              "  0.44050581170221936: 13,\n",
              "  0.4460799807131947: 7,\n",
              "  0.4495638363450543: 7,\n",
              "  0.4572283187351454: 47,\n",
              "  0.46001540324063306: 22,\n",
              "  0.4648928011252365: 7,\n",
              "  0.46628634337798036: 11,\n",
              "  0.46977019900983996: 3,\n",
              "  0.47464759689444336: 1,\n",
              "  0.4830088504109064: 6,\n",
              "  0.48509916379002216: 12,\n",
              "  0.486492706042766: 10,\n",
              "  0.49276364618011326: 34,\n",
              "  0.4941571884328571: 13,\n",
              "  0.494853959559229: 2,\n",
              "  0.49624750181197286: 33,\n",
              "  0.4983378151910886: 1,\n",
              "  0.5053055264548078: 7,\n",
              "  0.5060022975811796: 3,\n",
              "  0.511576466592155: 39,\n",
              "  0.5143635510976428: 3,\n",
              "  0.5192409489822462: 26,\n",
              "  0.52063449123499: 2,\n",
              "  0.5220280334877339: 3,\n",
              "  0.5241183468668495: 15,\n",
              "  0.5262086602459654: 29,\n",
              "  0.5317828292569406: 31,\n",
              "  0.5366602271415442: 5,\n",
              "  0.5401440827734038: 5,\n",
              "  0.5478085651634949: 36,\n",
              "  0.5512924207953545: 1,\n",
              "  0.5575633609327016: 8,\n",
              "  0.5582601320590735: 80,\n",
              "  0.5603504454381893: 58,\n",
              "  0.5652278433227927: 29,\n",
              "  0.5687116989546523: 44,\n",
              "  0.5694084700810242: 14,\n",
              "  0.5749826390919996: 2,\n",
              "  0.581253579229347: 4,\n",
              "  0.5826471214820907: 38,\n",
              "  0.5847374348612066: 225,\n",
              "  0.5861309771139503: 4,\n",
              "  0.5910083749985537: 16,\n",
              "  0.5917051461249256: 51,\n",
              "  0.5958857728831572: 5,\n",
              "  0.5979760862622729: 1,\n",
              "  0.5993696285150167: 17,\n",
              "  0.6007631707677606: 1,\n",
              "  0.6077308820314798: 3,\n",
              "  0.6126082799160832: 111,\n",
              "  0.614698593295199: 9,\n",
              "  0.615395364421571: 6,\n",
              "  0.6237566179380339: 12,\n",
              "  0.6272404735698935: 2,\n",
              "  0.6279372446962654: 69,\n",
              "  0.6293307869490092: 1,\n",
              "  0.6321178714544969: 7,\n",
              "  0.6342081848336127: 3,\n",
              "  0.6349049559599846: 28,\n",
              "  0.6362984982127284: 1,\n",
              "  0.6425694383500757: 7,\n",
              "  0.6453565228555634: 16,\n",
              "  0.648143607361051: 4,\n",
              "  0.6544145474983983: 3,\n",
              "  0.6592919453830017: 1,\n",
              "  0.6606854876357456: 6,\n",
              "  0.665562885520349: 30,\n",
              "  0.6662596566467209: 1,\n",
              "  0.6676531988994648: 10,\n",
              "  0.6690467411522086: 8,\n",
              "  0.6732273679104401: 14,\n",
              "  0.674620910163184: 1,\n",
              "  0.6760144524159278: 11,\n",
              "  0.6767112235422997: 28,\n",
              "  0.6781047657950436: 15,\n",
              "  0.6788015369214155: 1,\n",
              "  0.6885563326906223: 2,\n",
              "  0.6906466460697381: 2,\n",
              "  0.6920401883224819: 1,\n",
              "  0.6934337305752258: 19,\n",
              "  0.6941305017015977: 15,\n",
              "  0.6962208150807134: 1,\n",
              "  0.6976143573334573: 16,\n",
              "  0.6990078995862011: 3,\n",
              "  0.7004014418389449: 6,\n",
              "  0.7010982129653169: 28,\n",
              "  0.7094594664817799: 72,\n",
              "  0.7122465509872676: 94,\n",
              "  0.717123948871871: 14,\n",
              "  0.7199110333773587: 1,\n",
              "  0.7324529136520532: 8,\n",
              "  0.7352399981575409: 15,\n",
              "  0.7394206249157724: 2,\n",
              "  0.7408141671685162: 3,\n",
              "  0.7415109382948881: 5,\n",
              "  0.7436012516740039: 1,\n",
              "  0.7442980228003758: 3,\n",
              "  0.7449947939267477: 34,\n",
              "  0.7491754206849792: 3,\n",
              "  0.7519625051904669: 25,\n",
              "  0.7540528185695827: 6,\n",
              "  0.7561431319486984: 1,\n",
              "  0.7568399030750704: 9,\n",
              "  0.7617173009596738: 4,\n",
              "  0.7638076143387895: 16,\n",
              "  0.7652011565915334: 108,\n",
              "  0.7700785544761368: 9,\n",
              "  0.7791365791189718: 81,\n",
              "  0.7826204347508313: 1,\n",
              "  0.7868010615090629: 8,\n",
              "  0.7881946037618067: 8,\n",
              "  0.7888913748881786: 3,\n",
              "  0.794465543899154: 17,\n",
              "  0.7965558572782697: 29,\n",
              "  0.7993429417837574: 51,\n",
              "  0.802826797415617: 2,\n",
              "  0.8125815931848238: 3,\n",
              "  0.8139751354375677: 3,\n",
              "  0.8188525333221711: 8,\n",
              "  0.8209428467012869: 21,\n",
              "  0.8216396178276588: 22,\n",
              "  0.8265170157122622: 27,\n",
              "  0.8272137868386341: 1,\n",
              "  0.8320911847232376: 4,\n",
              "  0.8327879558496095: 49,\n",
              "  0.8334847269759814: 43,\n",
              "  0.8418459804924444: 1,\n",
              "  0.845329836124304: 13,\n",
              "  0.8481169206297917: 2,\n",
              "  0.8488136917561636: 78,\n",
              "  0.8516007762616513: 5,\n",
              "  0.8529943185143951: 55,\n",
              "  0.854387860767139: 17,\n",
              "  0.8564781741462547: 30,\n",
              "  0.862749114283602: 75,\n",
              "  0.8676265121682054: 22,\n",
              "  0.8683232832945773: 3,\n",
              "  0.8704135966736931: 7,\n",
              "  0.8718071389264369: 66,\n",
              "  0.8829554769483876: 28,\n",
              "  0.8843490192011315: 20,\n",
              "  0.8857425614538753: 9,\n",
              "  0.8864393325802472: 9,\n",
              "  0.8906199593384787: 1,\n",
              "  0.8927102727175945: 4,\n",
              "  0.8941038149703383: 1,\n",
              "  0.8982844417285698: 53,\n",
              "  0.9003747551076856: 63,\n",
              "  0.9017682973604294: 10,\n",
              "  0.9031618396131733: 6,\n",
              "  0.9094327797505205: 1,\n",
              "  0.9108263220032644: 5,\n",
              "  0.914310177635124: 28,\n",
              "  0.9205811177724712: 4,\n",
              "  0.922671431151587: 93,\n",
              "  0.9289423712889343: 21,\n",
              "  0.9296391424153062: 5,\n",
              "  0.9338197691735377: 1,\n",
              "  0.9373036248053973: 3,\n",
              "  0.9380003959317692: 19,\n",
              "  0.9463616494482322: 47,\n",
              "  0.9491487339537199: 3,\n",
              "  0.9547229029646952: 69,\n",
              "  0.9561164452174391: 20,\n",
              "  0.956813216343811: 10,\n",
              "  0.9582067585965548: 10,\n",
              "  0.9623873853547863: 4,\n",
              "  0.9644776987339021: 136,\n",
              "  0.9665680121130179: 1,\n",
              "  0.9672647832393898: 3,\n",
              "  0.9707486388712494: 10,\n",
              "  0.982593748019572: 3,\n",
              "  0.9853808325250597: 10,\n",
              "  0.9902582304096631: 22,\n",
              "  0.9951356282942665: 22,\n",
              "  0.9965291705470104: 70,\n",
              "  0.9972259416733823: 40,\n",
              "  1.00001302617887: 2,\n",
              "  1.000709797305242: 19,\n",
              "  1.0021033395579857: 3,\n",
              "  1.0028001106843576: 1,\n",
              "  1.0055871951898454: 5,\n",
              "  1.008374279695333: 2,\n",
              "  1.009071050821705: 59,\n",
              "  1.0118581353271925: 9,\n",
              "  1.0132516775799365: 9,\n",
              "  1.0153419909590522: 8,\n",
              "  1.016735533211796: 3,\n",
              "  1.0195226177172836: 13,\n",
              "  1.0223097022227714: 8,\n",
              "  1.0306709557392344: 1,\n",
              "  1.0313677268656063: 3,\n",
              "  1.0348515824974658: 2,\n",
              "  1.041122522634813: 87,\n",
              "  1.0425160648875569: 4,\n",
              "  1.0487870050249042: 8,\n",
              "  1.049483776151276: 17,\n",
              "  1.050180547277648: 5,\n",
              "  1.0578450296677393: 10,\n",
              "  1.059238571920483: 16,\n",
              "  1.0655095120578304: 2,\n",
              "  1.067599825436946: 2,\n",
              "  1.0773546212061529: 14,\n",
              "  1.0822320190907564: 28,\n",
              "  1.084322332469872: 3,\n",
              "  1.085019103596244: 15,\n",
              "  1.0885029592281037: 6,\n",
              "  1.0912900437335913: 3,\n",
              "  1.0968642127445667: 2,\n",
              "  1.0982577549973105: 1,\n",
              "  1.103135152881914: 35,\n",
              "  1.1066190085137735: 2,\n",
              "  1.111496406398377: 51,\n",
              "  1.1121931775247489: 1,\n",
              "  1.11985765991484: 30,\n",
              "  1.1212512021675838: 8,\n",
              "  1.1219479732939557: 7,\n",
              "  1.127522142304931: 1,\n",
              "  1.128218913431303: 4,\n",
              "  1.136580166947766: 27,\n",
              "  1.1400640225796255: 4,\n",
              "  1.1463349627169728: 4,\n",
              "  1.1484252760960887: 7,\n",
              "  1.1519091317279482: 1,\n",
              "  1.1588768429916674: 8,\n",
              "  1.1595736141180393: 28,\n",
              "  1.160967156370783: 4,\n",
              "  1.1623606986235269: 16,\n",
              "  1.1637542408762709: 15,\n",
              "  1.170025181013618: 4,\n",
              "  1.1721154943927339: 3,\n",
              "  1.1762961211509653: 1,\n",
              "  1.1783864345300812: 8,\n",
              "  1.179083205656453: 2,\n",
              "  1.179779976782825: 26,\n",
              "  1.1804767479091969: 5,\n",
              "  1.1832638324146845: 12,\n",
              "  1.2034701950794702: 4,\n",
              "  1.2055605084585859: 1,\n",
              "  1.2083475929640737: 3,\n",
              "  1.2111346774695613: 11,\n",
              "  1.212528219722305: 2,\n",
              "  1.2174056176069086: 2,\n",
              "  1.2201927021123962: 2,\n",
              "  1.2222830154915119: 2,\n",
              "  1.2264636422497435: 29,\n",
              "  1.2285539556288592: 8,\n",
              "  1.2327345823870908: 22,\n",
              "  1.2334313535134627: 42,\n",
              "  1.2348248957662065: 1,\n",
              "  1.2445796915354133: 2,\n",
              "  1.248063547167273: 10,\n",
              "  1.2501538605463887: 3,\n",
              "  1.2508506316727606: 15,\n",
              "  1.2515474027991325: 1,\n",
              "  1.2522441739255044: 1,\n",
              "  1.2543344873046203: 12,\n",
              "  1.255728029557364: 5,\n",
              "  1.256424800683736: 10,\n",
              "  1.2578183429364798: 29,\n",
              "  1.2661795964529428: 1,\n",
              "  1.2668763675793147: 4,\n",
              "  1.2710569943375463: 7,\n",
              "  1.27245053659029: 18,\n",
              "  1.273147307716662: 8,\n",
              "  1.2759343922221498: 16,\n",
              "  1.280811790106753: 31,\n",
              "  1.281508561233125: 14,\n",
              "  1.2856891879913566: 21,\n",
              "  1.29056658587596: 3,\n",
              "  1.291263357002332: 1,\n",
              "  1.2940504415078196: 1,\n",
              "  1.2982310682660512: 1,\n",
              "  1.3017149238979107: 8,\n",
              "  1.3114697196671175: 2,\n",
              "  1.3170438886780929: 146,\n",
              "  1.3191342020572085: 45,\n",
              "  1.3198309731835807: 61,\n",
              "  1.3212245154363245: 16,\n",
              "  1.3219212865626964: 1,\n",
              "  1.3309793112055313: 2,\n",
              "  1.3337663957110188: 1,\n",
              "  1.3379470224692505: 8,\n",
              "  1.340734106974738: 85,\n",
              "  1.3456115048593416: 3,\n",
              "  1.3470050471120854: 1,\n",
              "  1.3574566140076643: 40,\n",
              "  1.358850156260408: 17,\n",
              "  1.35954692738678: 40,\n",
              "  1.3602436985131519: 2,\n",
              "  1.3651210963977554: 34,\n",
              "  1.3686049520296149: 22,\n",
              "  1.3693017231559867: 7,\n",
              "  1.3727855787878465: 2,\n",
              "  1.376269434419706: 42,\n",
              "  1.3804500611779376: 5,\n",
              "  1.3811468323043095: 4,\n",
              "  1.383933916809797: 3,\n",
              "  1.384630687936169: 11,\n",
              "  1.392991941452632: 75,\n",
              "  1.3943854837053757: 25,\n",
              "  1.3985661104636073: 8,\n",
              "  1.3992628815899792: 24,\n",
              "  1.3999596527163511: 3,\n",
              "  1.4034435083482109: 8,\n",
              "  1.4048370506009547: 21,\n",
              "  1.4083209062328141: 24,\n",
              "  1.409017677359186: 5,\n",
              "  1.4180757020020212: 11,\n",
              "  1.418772473128393: 14,\n",
              "  1.4257401843921123: 8,\n",
              "  1.427830497771228: 47,\n",
              "  1.4292240400239717: 20,\n",
              "  1.4299208111503436: 29,\n",
              "  1.4334046667822034: 4,\n",
              "  1.4341014379085752: 6,\n",
              "  1.4389788357931785: 28,\n",
              "  1.4410691491722945: 5,\n",
              "  1.4501271738151293: 8,\n",
              "  1.4550045716997329: 15,\n",
              "  1.4591851984579642: 51,\n",
              "  1.4633658252161958: 41,\n",
              "  1.4668496808480553: 135,\n",
              "  1.4696367653535432: 2,\n",
              "  1.4738173921117745: 2,\n",
              "  1.4800883322491218: 4,\n",
              "  1.4807851033754937: 17,\n",
              "  1.4821786456282375: 5,\n",
              "  1.4842689590073532: 13,\n",
              "  1.4926302125238164: 1,\n",
              "  1.495417297029304: 10,\n",
              "  1.4968108392820478: 10,\n",
              "  1.4989011526611635: 3,\n",
              "  1.4995979237875356: 2,\n",
              "  1.5086559484303705: 6,\n",
              "  1.5156236596940897: 6,\n",
              "  1.520501057578693: 1,\n",
              "  1.521894599831437: 3,\n",
              "  1.522591370957809: 4,\n",
              "  1.5239849132105527: 7,\n",
              "  1.5253784554632965: 1,\n",
              "  1.5267719977160403: 3,\n",
              "  1.5281655399687841: 1,\n",
              "  1.5330429378533876: 6,\n",
              "  1.5351332512325033: 4,\n",
              "  1.5365267934852471: 3,\n",
              "  1.5400106491171068: 5,\n",
              "  1.5414041913698506: 4,\n",
              "  1.5455848181280822: 4,\n",
              "  1.555339613897289: 5,\n",
              "  1.5581266984027766: 4,\n",
              "  1.5664879519192396: 2,\n",
              "  1.5692750364247274: 3,\n",
              "  1.5699718075510993: 4,\n",
              "  1.5706685786774712: 2,\n",
              "  1.572062120930215: 5,\n",
              "  1.574152434309331: 7,\n",
              "  1.580423374446678: 1,\n",
              "  1.5832104589521658: 48,\n",
              "  1.5866943145840253: 4,\n",
              "  1.588784627963141: 24,\n",
              "  1.5929652547213726: 8,\n",
              "  1.5936620258477445: 2,\n",
              "  1.5971458814796042: 2,\n",
              "  1.5999329659850918: 1,\n",
              "  1.6062039061224391: 9,\n",
              "  1.607597448375183: 4,\n",
              "  1.6117780751334145: 6,\n",
              "  1.6138683885125302: 10,\n",
              "  1.6166554730180178: 54,\n",
              "  1.6173522441443897: 13,\n",
              "  1.6180490152707616: 7,\n",
              "  1.6201393286498775: 5,\n",
              "  1.6250167265344808: 2,\n",
              "  1.6271070399135967: 3,\n",
              "  1.6285005821663405: 16,\n",
              "  1.6305908955454562: 10,\n",
              "  1.6312876666718281: 75,\n",
              "  1.6361650645564316: 5,\n",
              "  1.6375586068091754: 1,\n",
              "  1.6382553779355473: 1,\n",
              "  1.6431327758201508: 6,\n",
              "  1.6473134025783822: 8,\n",
              "  1.6598552828530768: 4,\n",
              "  1.675184247633259: 2,\n",
              "  1.6765777898860028: 1,\n",
              "  1.6772745610123747: 33,\n",
              "  1.6786681032651185: 8,\n",
              "  1.6807584166442344: 14,\n",
              "  1.6863325856552098: 23,\n",
              "  1.6870293567815817: 1,\n",
              "  1.692603525792557: 2,\n",
              "  1.6933002969189288: 2,\n",
              "  1.6953906102980447: 1,\n",
              "  1.70096477930902: 1,\n",
              "  1.701661550435392: 2,\n",
              "  1.7030550926881358: 1,\n",
              "  1.7072357194463672: 6,\n",
              "  1.707932490572739: 1,\n",
              "  1.7169905152155742: 23,\n",
              "  1.718384057468318: 2,\n",
              "  1.7211711419738056: 13,\n",
              "  1.7218679131001775: 2,\n",
              "  1.726745310984781: 2,\n",
              "  1.727442082111153: 24,\n",
              "  1.7281388532375248: 1,\n",
              "  1.7323194799957564: 5,\n",
              "  1.7455581313968227: 2,\n",
              "  1.7469516736495667: 45,\n",
              "  1.7504355292814262: 22,\n",
              "  1.7560096982924016: 1,\n",
              "  1.760190325050633: 10,\n",
              "  1.7671580363143522: 21,\n",
              "  1.76994512081984: 3,\n",
              "  1.7762160609571873: 3,\n",
              "  1.777609603209931: 6,\n",
              "  1.7838805433472784: 3,\n",
              "  1.7922417968637414: 21,\n",
              "  1.795028881369229: 6,\n",
              "  1.7992095081274606: 2,\n",
              "  1.80269336375932: 4,\n",
              "  1.803390134885692: 3,\n",
              "  ...},\n",
              " {-1.8449034827976734: 1,\n",
              "  -1.8393293137866982: 2,\n",
              "  -1.8379357715339544: 1,\n",
              "  -1.8358454581548385: 1,\n",
              "  -1.8351486870284666: 1,\n",
              "  -1.8337551447757228: 1,\n",
              "  -1.8295745180174912: 1,\n",
              "  -1.8288777468911193: 1,\n",
              "  -1.8212132645010282: 1,\n",
              "  -1.8205164933746563: 2,\n",
              "  -1.8191229511219125: 1,\n",
              "  -1.8163358666164249: 1,\n",
              "  -1.813548782110937: 1,\n",
              "  -1.8121552398581933: 3,\n",
              "  -1.8114584687318214: 1,\n",
              "  -1.8079746130999619: 2,\n",
              "  -1.8072778419735898: 1,\n",
              "  -1.8044907574681022: 4,\n",
              "  -1.8017036729626146: 3,\n",
              "  -1.7996133595834987: 3,\n",
              "  -1.796826275078011: 1,\n",
              "  -1.7933424194461516: 5,\n",
              "  -1.7926456483197795: 1,\n",
              "  -1.7912521060670357: 2,\n",
              "  -1.78916179268792: 1,\n",
              "  -1.7877682504351762: 1,\n",
              "  -1.7870714793088043: 1,\n",
              "  -1.7856779370560605: 1,\n",
              "  -1.780103768045085: 6,\n",
              "  -1.7794069969187132: 5,\n",
              "  -1.7766199124132254: 1,\n",
              "  -1.7752263701604816: 4,\n",
              "  -1.7738328279077378: 3,\n",
              "  -1.7654715743912748: 1,\n",
              "  -1.7612909476330432: 1,\n",
              "  -1.7592006342539275: 3,\n",
              "  -1.755716778622068: 1,\n",
              "  -1.7515361518638364: 1,\n",
              "  -1.7438716694737453: 2,\n",
              "  -1.7424781272210015: 2,\n",
              "  -1.7417813560946296: 3,\n",
              "  -1.7403878138418856: 1,\n",
              "  -1.7396910427155137: 1,\n",
              "  -1.7362071870836542: 1,\n",
              "  -1.7355104159572823: 2,\n",
              "  -1.729239475819935: 2,\n",
              "  -1.7278459335671912: 1,\n",
              "  -1.7257556201880755: 1,\n",
              "  -1.7243620779353315: 3,\n",
              "  -1.7222717645562158: 2,\n",
              "  -1.7187879089243563: 1,\n",
              "  -1.7132137399133809: 2,\n",
              "  -1.7111234265342652: 4,\n",
              "  -1.710426655407893: 1,\n",
              "  -1.7076395709024055: 1,\n",
              "  -1.7062460286496617: 5,\n",
              "  -1.7048524863969179: 1,\n",
              "  -1.702762173017802: 2,\n",
              "  -1.6999750885123144: 8,\n",
              "  -1.6992783173859425: 4,\n",
              "  -1.696491232880455: 2,\n",
              "  -1.6916138349958514: 1,\n",
              "  -1.6902202927431076: 2,\n",
              "  -1.686039665984876: 4,\n",
              "  -1.685342894858504: 2,\n",
              "  -1.6818590392266446: 1,\n",
              "  -1.6804654969739006: 1,\n",
              "  -1.6790719547211568: 1,\n",
              "  -1.677678412468413: 6,\n",
              "  -1.6748913279629254: 2,\n",
              "  -1.6741945568365535: 1,\n",
              "  -1.6721042434574376: 1,\n",
              "  -1.6707107012046938: 2,\n",
              "  -1.6700139300783219: 2,\n",
              "  -1.6651365321937184: 2,\n",
              "  -1.6637429899409746: 1,\n",
              "  -1.6630462188146027: 1,\n",
              "  -1.6623494476882308: 1,\n",
              "  -1.6616526765618589: 3,\n",
              "  -1.660955905435487: 5,\n",
              "  -1.660259134309115: 3,\n",
              "  -1.6595623631827432: 1,\n",
              "  -1.6574720498036273: 1,\n",
              "  -1.651897880792652: 1,\n",
              "  -1.650504338539908: 1,\n",
              "  -1.6498075674135362: 3,\n",
              "  -1.644930169528933: 4,\n",
              "  -1.644233398402561: 4,\n",
              "  -1.643536627276189: 1,\n",
              "  -1.642143085023445: 3,\n",
              "  -1.6414463138970732: 1,\n",
              "  -1.6379624582652137: 4,\n",
              "  -1.635872144886098: 3,\n",
              "  -1.634478602633354: 1,\n",
              "  -1.6323882892542383: 2,\n",
              "  -1.6268141202432629: 3,\n",
              "  -1.626117349116891: 1,\n",
              "  -1.6233302646114034: 1,\n",
              "  -1.6198464089795437: 2,\n",
              "  -1.6142722399685685: 1,\n",
              "  -1.6121819265894526: 9,\n",
              "  -1.6114851554630807: 1,\n",
              "  -1.610091613210337: 7,\n",
              "  -1.608698070957593: 1,\n",
              "  -1.6080012998312212: 6,\n",
              "  -1.601730359693874: 3,\n",
              "  -1.5982465040620142: 1,\n",
              "  -1.5975497329356423: 1,\n",
              "  -1.5947626484301547: 6,\n",
              "  -1.5912787927982952: 1,\n",
              "  -1.5870981660400636: 1,\n",
              "  -1.5857046237873198: 1,\n",
              "  -1.585007852660948: 8,\n",
              "  -1.58222076815546: 1,\n",
              "  -1.5794336836499725: 2,\n",
              "  -1.566891803375278: 1,\n",
              "  -1.5648014899961622: 1,\n",
              "  -1.5627111766170465: 1,\n",
              "  -1.5620144054906746: 5,\n",
              "  -1.5550466942269554: 5,\n",
              "  -1.5543499231005835: 4,\n",
              "  -1.5536531519742116: 13,\n",
              "  -1.550169296342352: 2,\n",
              "  -1.54947252521598: 2,\n",
              "  -1.5466854407104924: 4,\n",
              "  -1.5438983562050046: 1,\n",
              "  -1.5432015850786327: 5,\n",
              "  -1.5397177294467732: 1,\n",
              "  -1.5355371026885416: 1,\n",
              "  -1.5285693914248224: 1,\n",
              "  -1.5264790780457067: 3,\n",
              "  -1.5257823069193348: 4,\n",
              "  -1.5236919935402191: 1,\n",
              "  -1.5202081379083594: 1,\n",
              "  -1.5181178245292437: 3,\n",
              "  -1.5174210534028718: 1,\n",
              "  -1.514633968897384: 1,\n",
              "  -1.5132404266446402: 3,\n",
              "  -1.5125436555182683: 1,\n",
              "  -1.5118468843918964: 5,\n",
              "  -1.5097565710127807: 1,\n",
              "  -1.5090597998864088: 3,\n",
              "  -1.505575944254549: 1,\n",
              "  -1.5048791731281772: 1,\n",
              "  -1.5041824020018053: 3,\n",
              "  -1.5013953174963177: 1,\n",
              "  -1.4916405217271107: 1,\n",
              "  -1.490246979474367: 1,\n",
              "  -1.4832792682106477: 2,\n",
              "  -1.4825824970842758: 2,\n",
              "  -1.4797954125787882: 1,\n",
              "  -1.4784018703260442: 2,\n",
              "  -1.4770083280733004: 1,\n",
              "  -1.4763115569469285: 4,\n",
              "  -1.4756147858205566: 1,\n",
              "  -1.4742212435678128: 1,\n",
              "  -1.473524472441441: 2,\n",
              "  -1.472827701315069: 1,\n",
              "  -1.4714341590623252: 1,\n",
              "  -1.470737387935953: 3,\n",
              "  -1.4693438456832093: 1,\n",
              "  -1.4679503034304655: 1,\n",
              "  -1.4672535323040936: 4,\n",
              "  -1.4658599900513498: 7,\n",
              "  -1.464466447798606: 1,\n",
              "  -1.46237613441949: 6,\n",
              "  -1.4616793632931182: 1,\n",
              "  -1.4581955076612587: 1,\n",
              "  -1.4574987365348868: 1,\n",
              "  -1.456801965408515: 1,\n",
              "  -1.4561051942821428: 2,\n",
              "  -1.455408423155771: 6,\n",
              "  -1.4519245675239114: 2,\n",
              "  -1.4498342541447957: 2,\n",
              "  -1.4484407118920517: 1,\n",
              "  -1.447047169639308: 1,\n",
              "  -1.446350398512936: 1,\n",
              "  -1.445653627386564: 3,\n",
              "  -1.4442600851338203: 4,\n",
              "  -1.4414730006283327: 2,\n",
              "  -1.4331117471118695: 1,\n",
              "  -1.4268408069745224: 1,\n",
              "  -1.4261440358481503: 1,\n",
              "  -1.4240537224690346: 12,\n",
              "  -1.4233569513426627: 1,\n",
              "  -1.421266637963547: 1,\n",
              "  -1.4198730957108032: 2,\n",
              "  -1.4177827823316873: 3,\n",
              "  -1.4163892400789435: 1,\n",
              "  -1.4156924689525716: 1,\n",
              "  -1.4149956978261997: 1,\n",
              "  -1.410815071067968: 1,\n",
              "  -1.4101182999415962: 1,\n",
              "  -1.4094215288152243: 6,\n",
              "  -1.4073312154361086: 4,\n",
              "  -1.405240902056993: 1,\n",
              "  -1.4038473598042491: 1,\n",
              "  -1.403150588677877: 1,\n",
              "  -1.4010602752987613: 10,\n",
              "  -1.3989699619196456: 1,\n",
              "  -1.3982731907932737: 21,\n",
              "  -1.39687964854053: 2,\n",
              "  -1.3878216238976948: 1,\n",
              "  -1.3850345393922072: 3,\n",
              "  -1.3808539126339756: 1,\n",
              "  -1.3794603703812318: 1,\n",
              "  -1.3759765147493723: 3,\n",
              "  -1.369705574612025: 1,\n",
              "  -1.3690088034856531: 2,\n",
              "  -1.3683120323592812: 6,\n",
              "  -1.3648281767274215: 2,\n",
              "  -1.3641314056010496: 5,\n",
              "  -1.3634346344746777: 1,\n",
              "  -1.3627378633483058: 2,\n",
              "  -1.361344321095562: 1,\n",
              "  -1.3606475499691901: 1,\n",
              "  -1.3599507788428182: 8,\n",
              "  -1.3578604654637023: 1,\n",
              "  -1.352983067579099: 3,\n",
              "  -1.3515895253263552: 1,\n",
              "  -1.3508927541999831: 4,\n",
              "  -1.3494992119472393: 2,\n",
              "  -1.3397444161780325: 2,\n",
              "  -1.3285960781560817: 1,\n",
              "  -1.3251122225242222: 1,\n",
              "  -1.3237186802714784: 1,\n",
              "  -1.3209315957659906: 1,\n",
              "  -1.3167509690077592: 1,\n",
              "  -1.3160541978813873: 5,\n",
              "  -1.3139638845022714: 2,\n",
              "  -1.3132671133758995: 22,\n",
              "  -1.310480028870412: 2,\n",
              "  -1.3090864866176681: 1,\n",
              "  -1.3076929443649243: 18,\n",
              "  -1.3056026309858084: 1,\n",
              "  -1.3042090887330646: 2,\n",
              "  -1.302118775353949: 2,\n",
              "  -1.2993316908484613: 1,\n",
              "  -1.2972413774693454: 1,\n",
              "  -1.2965446063429735: 2,\n",
              "  -1.2951510640902297: 2,\n",
              "  -1.2923639795847421: 1,\n",
              "  -1.2916672084583702: 4,\n",
              "  -1.2909704373319981: 1,\n",
              "  -1.283305954941907: 3,\n",
              "  -1.2826091838155351: 2,\n",
              "  -1.2819124126891632: 3,\n",
              "  -1.2805188704364194: 5,\n",
              "  -1.274944701425444: 1,\n",
              "  -1.2742479302990721: 6,\n",
              "  -1.2721576169199564: 1,\n",
              "  -1.2707640746672126: 10,\n",
              "  -1.2651899056562372: 6,\n",
              "  -1.2624028211507496: 3,\n",
              "  -1.2540415676342864: 1,\n",
              "  -1.249860940876055: 4,\n",
              "  -1.2491641697496831: 5,\n",
              "  -1.2484673986233112: 1,\n",
              "  -1.2442867718650796: 5,\n",
              "  -1.241499687359592: 1,\n",
              "  -1.2401061451068482: 2,\n",
              "  -1.2373190606013604: 1,\n",
              "  -1.233835204969501: 1,\n",
              "  -1.233138433843129: 5,\n",
              "  -1.2296545782112693: 2,\n",
              "  -1.2282610359585255: 2,\n",
              "  -1.217112697936575: 2,\n",
              "  -1.215022384557459: 2,\n",
              "  -1.2136288423047152: 5,\n",
              "  -1.2122353000519714: 1,\n",
              "  -1.2101449866728557: 7,\n",
              "  -1.2094482155464839: 2,\n",
              "  -1.2080546732937398: 5,\n",
              "  -1.207357902167368: 3,\n",
              "  -1.2059643599146241: 2,\n",
              "  -1.2003901909036487: 3,\n",
              "  -1.1996934197772768: 3,\n",
              "  -1.198996648650905: 5,\n",
              "  -1.1976031063981611: 4,\n",
              "  -1.1969063352717892: 1,\n",
              "  -1.1948160218926736: 1,\n",
              "  -1.1920289373871857: 2,\n",
              "  -1.190635395134442: 1,\n",
              "  -1.1878483106289544: 1,\n",
              "  -1.1871515395025825: 1,\n",
              "  -1.1773967437333754: 4,\n",
              "  -1.1760032014806316: 6,\n",
              "  -1.1753064303542597: 1,\n",
              "  -1.1746096592278878: 6,\n",
              "  -1.173216116975144: 5,\n",
              "  -1.1725193458487722: 7,\n",
              "  -1.1711258035960281: 2,\n",
              "  -1.1697322613432843: 1,\n",
              "  -1.1690354902169124: 4,\n",
              "  -1.165551634585053: 4,\n",
              "  -1.1627645500795651: 16,\n",
              "  -1.15719038106859: 1,\n",
              "  -1.1509194409312427: 1,\n",
              "  -1.1460420430466391: 1,\n",
              "  -1.1439517296675235: 1,\n",
              "  -1.1432549585411516: 1,\n",
              "  -1.1411646451620356: 2,\n",
              "  -1.1397711029092918: 15,\n",
              "  -1.13907433178292: 4,\n",
              "  -1.1362872472774324: 2,\n",
              "  -1.1348937050246886: 1,\n",
              "  -1.1314098493928288: 1,\n",
              "  -1.1279259937609694: 11,\n",
              "  -1.1272292226345975: 9,\n",
              "  -1.1258356803818534: 1,\n",
              "  -1.1237453670027377: 1,\n",
              "  -1.121655053623622: 2,\n",
              "  -1.1181711979917623: 2,\n",
              "  -1.1146873423599029: 4,\n",
              "  -1.113990571233531: 1,\n",
              "  -1.1091131733489275: 6,\n",
              "  -1.1077196310961837: 1,\n",
              "  -1.105629317717068: 5,\n",
              "  -1.103539004337952: 1,\n",
              "  -1.1000551487060926: 5,\n",
              "  -1.0993583775797207: 1,\n",
              "  -1.0930874374423734: 1,\n",
              "  -1.0923906663160015: 2,\n",
              "  -1.0916938951896296: 1,\n",
              "  -1.0889068106841417: 1,\n",
              "  -1.0882100395577698: 2,\n",
              "  -1.087513268431398: 1,\n",
              "  -1.086816497305026: 5,\n",
              "  -1.0861197261786542: 13,\n",
              "  -1.0854229550522823: 1,\n",
              "  -1.0847261839259104: 5,\n",
              "  -1.0749713881567036: 3,\n",
              "  -1.0735778459039595: 6,\n",
              "  -1.0728810747775877: 1,\n",
              "  -1.070790761398472: 1,\n",
              "  -1.0687004480193563: 8,\n",
              "  -1.061732736755637: 2,\n",
              "  -1.0610359656292652: 1,\n",
              "  -1.0603391945028933: 2,\n",
              "  -1.0582488811237774: 2,\n",
              "  -1.0568553388710336: 1,\n",
              "  -1.0561585677446617: 3,\n",
              "  -1.0505843987336863: 1,\n",
              "  -1.0491908564809425: 1,\n",
              "  -1.044313458596339: 7,\n",
              "  -1.0394360607117357: 1,\n",
              "  -1.0345586628271322: 1,\n",
              "  -1.0324683494480165: 1,\n",
              "  -1.0303780360689008: 2,\n",
              "  -1.0261974093106692: 2,\n",
              "  -1.0248038670579254: 5,\n",
              "  -1.0206232402996938: 1,\n",
              "  -1.0199264691733219: 1,\n",
              "  -1.01922969804695: 1,\n",
              "  -1.018532926920578: 2,\n",
              "  -1.0157458424150905: 1,\n",
              "  -1.0143523001623465: 2,\n",
              "  -1.0129587579096027: 5,\n",
              "  -1.0115652156568589: 1,\n",
              "  -1.010868444530487: 1,\n",
              "  -1.0080813600249994: 2,\n",
              "  -1.0073845888986275: 1,\n",
              "  -1.0066878177722554: 1,\n",
              "  -1.0011136487612802: 4,\n",
              "  -0.9997201065085363: 2,\n",
              "  -0.9990233353821644: 1,\n",
              "  -0.9983265642557925: 1,\n",
              "  -0.9976297931294205: 2,\n",
              "  -0.9969330220030486: 2,\n",
              "  -0.9955394797503048: 2,\n",
              "  -0.9941459374975609: 2,\n",
              "  -0.9913588529920733: 2,\n",
              "  -0.9906620818657014: 4,\n",
              "  -0.9892685396129575: 1,\n",
              "  -0.9885717684865856: 1,\n",
              "  -0.9878749973602137: 1,\n",
              "  -0.9816040572228664: 3,\n",
              "  -0.9788169727173787: 5,\n",
              "  -0.9781202015910068: 1,\n",
              "  -0.9774234304646349: 2,\n",
              "  -0.9753331170855191: 16,\n",
              "  -0.9746363459591472: 2,\n",
              "  -0.9739395748327753: 1,\n",
              "  -0.9725460325800315: 1,\n",
              "  -0.9711524903272876: 1,\n",
              "  -0.9704557192009157: 1,\n",
              "  -0.9683654058218: 1,\n",
              "  -0.9655783213163123: 13,\n",
              "  -0.9641847790635684: 3,\n",
              "  -0.9627912368108246: 3,\n",
              "  -0.9572170677998493: 6,\n",
              "  -0.9516428987888739: 11,\n",
              "  -0.950946127662502: 1,\n",
              "  -0.9474622720306424: 1,\n",
              "  -0.9460687297778986: 1,\n",
              "  -0.9446751875251547: 6,\n",
              "  -0.9418881030196671: 1,\n",
              "  -0.9384042473878075: 11,\n",
              "  -0.9356171628823198: 1,\n",
              "  -0.9342236206295759: 2,\n",
              "  -0.9314365361240883: 1,\n",
              "  -0.9307397649977164: 3,\n",
              "  -0.9293462227449725: 3,\n",
              "  -0.9265591382394848: 5,\n",
              "  -0.9237720537339972: 6,\n",
              "  -0.9230752826076252: 1,\n",
              "  -0.9223785114812533: 1,\n",
              "  -0.9209849692285095: 1,\n",
              "  -0.9195914269757657: 4,\n",
              "  -0.9175011135966499: 1,\n",
              "  -0.9161075713439061: 1,\n",
              "  -0.9154108002175341: 1,\n",
              "  -0.9126237157120465: 4,\n",
              "  -0.9105334023329307: 1,\n",
              "  -0.907746317827443: 1,\n",
              "  -0.9035656910692115: 1,\n",
              "  -0.9021721488164677: 4,\n",
              "  -0.9007786065637239: 3,\n",
              "  -0.8952044375527485: 2,\n",
              "  -0.8938108953000047: 4,\n",
              "  -0.891023810794517: 1,\n",
              "  -0.8896302685417732: 2,\n",
              "  -0.8889334974154012: 1,\n",
              "  -0.8868431840362855: 1,\n",
              "  -0.8854496417835416: 1,\n",
              "  -0.8840560995307978: 2,\n",
              "  -0.8805722438989382: 1,\n",
              "  -0.8784819305198225: 1,\n",
              "  -0.8777851593934505: 2,\n",
              "  -0.8763916171407067: 1,\n",
              "  -0.8743013037615909: 3,\n",
              "  -0.8729077615088471: 1,\n",
              "  -0.8722109903824752: 1,\n",
              "  -0.8701206770033594: 1,\n",
              "  -0.8673335924978718: 5,\n",
              "  -0.8666368213714998: 1,\n",
              "  -0.865243279118756: 1,\n",
              "  -0.8638497368660122: 4,\n",
              "  -0.8631529657396402: 14,\n",
              "  -0.8575787967286649: 1,\n",
              "  -0.8527013988440615: 3,\n",
              "  -0.8520046277176895: 1,\n",
              "  -0.8513078565913176: 7,\n",
              "  -0.8492175432122019: 2,\n",
              "  -0.84852077208583: 1,\n",
              "  -0.8450369164539704: 5,\n",
              "  -0.8443401453275984: 3,\n",
              "  -0.8436433742012265: 1,\n",
              "  -0.8429466030748546: 1,\n",
              "  -0.8422498319484827: 1,\n",
              "  -0.8359788918111354: 4,\n",
              "  -0.8317982650529039: 1,\n",
              "  -0.8304047228001601: 1,\n",
              "  -0.8297079516737882: 12,\n",
              "  -0.8269208671683005: 1,\n",
              "  -0.8241337826628128: 4,\n",
              "  -0.822740240410069: 1,\n",
              "  -0.822043469283697: 1,\n",
              "  -0.8199531559045813: 3,\n",
              "  -0.8192563847782094: 1,\n",
              "  -0.8150757580199779: 3,\n",
              "  -0.813682215767234: 2,\n",
              "  -0.8115919023881183: 1,\n",
              "  -0.8108951312617464: 5,\n",
              "  -0.8101983601353744: 2,\n",
              "  -0.8095015890090025: 2,\n",
              "  -0.8046241911243991: 3,\n",
              "  -0.8039274199980272: 3,\n",
              "  -0.8032306488716552: 1,\n",
              "  -0.8025338777452833: 3,\n",
              "  -0.8004435643661676: 2,\n",
              "  -0.7997467932397957: 1,\n",
              "  -0.7990500221134237: 2,\n",
              "  -0.7962629376079361: 1,\n",
              "  -0.7927790819760765: 1,\n",
              "  -0.7913855397233326: 12,\n",
              "  -0.7906887685969607: 1,\n",
              "  -0.7899919974705888: 3,\n",
              "  -0.7892952263442169: 2,\n",
              "  -0.7872049129651011: 1,\n",
              "  -0.7816307439541258: 8,\n",
              "  -0.7774501171958943: 1,\n",
              "  -0.7760565749431504: 2,\n",
              "  -0.7753598038167785: 2,\n",
              "  -0.7746630326904066: 1,\n",
              "  -0.7614243812893401: 20,\n",
              "  -0.7607276101629682: 3,\n",
              "  -0.7572437545311086: 2,\n",
              "  -0.7565469834047367: 2,\n",
              "  -0.7551534411519929: 8,\n",
              "  -0.7523663566465052: 1,\n",
              "  -0.7509728143937613: 4,\n",
              "  -0.7502760432673894: 1,\n",
              "  -0.7495792721410175: 1,\n",
              "  -0.7488825010146456: 1,\n",
              "  -0.7467921876355298: 3,\n",
              "  -0.7419147897509264: 1,\n",
              "  -0.7412180186245545: 22,\n",
              "  -0.7391277052454387: 1,\n",
              "  -0.7377341629926949: 24,\n",
              "  -0.7363406207399511: 1,\n",
              "  -0.7356438496135791: 2,\n",
              "  -0.7349470784872072: 15,\n",
              "  -0.7328567651080915: 4,\n",
              "  -0.7272825960971161: 1,\n",
              "  -0.7265858249707442: 2,\n",
              "  -0.7251922827180004: 6,\n",
              "  -0.7244955115916284: 5,\n",
              "  -0.7237987404652565: 1,\n",
              "  -0.7231019693388846: 1,\n",
              "  -0.7224051982125127: 1,\n",
              "  -0.7189213425806531: 1,\n",
              "  -0.7175278003279093: 12,\n",
              "  -0.7147407158224216: 5,\n",
              "  -0.7133471735696777: 1,\n",
              "  -0.7119536313169339: 7,\n",
              "  -0.7091665468114462: 1,\n",
              "  -0.7056826911795866: 1,\n",
              "  -0.7015020644213551: 2,\n",
              "  -0.7008052932949832: 1,\n",
              "  -0.6994117510422394: 9,\n",
              "  -0.6980182087894955: 1,\n",
              "  -0.6973214376631236: 1,\n",
              "  -0.6959278954103798: 7,\n",
              "  -0.6924440397785202: 1,\n",
              "  -0.6917472686521483: 1,\n",
              "  -0.6875666418939168: 1,\n",
              "  -0.6861730996411729: 1,\n",
              "  -0.6840827862620572: 1,\n",
              "  -0.6833860151356852: 1,\n",
              "  -0.6826892440093133: 1,\n",
              "  -0.6805989306301976: 2,\n",
              "  -0.6778118461247099: 1,\n",
              "  -0.677115074998338: 2,\n",
              "  -0.6757215327455941: 1,\n",
              "  -0.6736312193664784: 1,\n",
              "  -0.6715409059873626: 2,\n",
              "  -0.6701473637346188: 2,\n",
              "  -0.6694505926082469: 1,\n",
              "  -0.668057050355503: 4,\n",
              "  -0.6638764235972715: 1,\n",
              "  -0.6624828813445277: 1,\n",
              "  -0.6617861102181558: 1,\n",
              "  -0.6610893390917838: 4,\n",
              "  -0.65969579683904: 5,\n",
              "  -0.6589990257126681: 8,\n",
              "  -0.6541216278280647: 2,\n",
              "  -0.6534248567016927: 2,\n",
              "  -0.6527280855753208: 8,\n",
              "  -0.642973289806114: 1,\n",
              "  -0.6415797475533701: 5,\n",
              "  -0.6401862053006263: 2,\n",
              "  -0.6387926630478825: 1,\n",
              "  -0.6325217229105352: 3,\n",
              "  -0.6304314095314194: 1,\n",
              "  -0.6297346384050475: 3,\n",
              "  -0.6290378672786756: 2,\n",
              "  -0.6276443250259318: 1,\n",
              "  -0.6199798426358406: 10,\n",
              "  -0.6192830715094687: 3,\n",
              "  -0.6178895292567249: 3,\n",
              "  -0.6144056736248653: 1,\n",
              "  -0.6137089024984934: 8,\n",
              "  -0.6123153602457495: 2,\n",
              "  -0.6032573356029146: 1,\n",
              "  -0.6018637933501708: 2,\n",
              "  -0.6011670222237988: 6,\n",
              "  -0.6004702510974269: 6,\n",
              "  -0.599773479971055: 1,\n",
              "  -0.5935025398337077: 1,\n",
              "  -0.5921089975809639: 5,\n",
              "  -0.5900186842018481: 1,\n",
              "  -0.5893219130754762: 1,\n",
              "  -0.5886251419491043: 1,\n",
              "  -0.5879283708227324: 1,\n",
              "  -0.5851412863172447: 4,\n",
              "  -0.5844445151908728: 1,\n",
              "  -0.583050972938129: 13,\n",
              "  -0.5809606595590132: 3,\n",
              "  -0.5795671173062694: 2,\n",
              "  -0.5788703461798974: 2,\n",
              "  -0.5767800328007817: 1,\n",
              "  -0.5760832616744098: 12,\n",
              "  -0.5753864905480379: 2,\n",
              "  -0.5719026349161783: 2,\n",
              "  -0.5684187792843187: 10,\n",
              "  -0.565631694778831: 2,\n",
              "  -0.5635413813997152: 1,\n",
              "  -0.5628446102733433: 29,\n",
              "  -0.5621478391469714: 1,\n",
              "  -0.5614510680205995: 1,\n",
              "  -0.5600575257678556: 1,\n",
              "  -0.5579672123887399: 2,\n",
              "  -0.5551801278832522: 5,\n",
              "  -0.5544833567568803: 24,\n",
              "  -0.5530898145041365: 8,\n",
              "  -0.5509995011250207: 1,\n",
              "  -0.5496059588722769: 6,\n",
              "  -0.548212416619533: 2,\n",
              "  -0.5433350187349296: 9,\n",
              "  -0.5412447053558138: 1,\n",
              "  -0.53985116310307: 10,\n",
              "  -0.5370640785975823: 1,\n",
              "  -0.5335802229657227: 4,\n",
              "  -0.5321866807129789: 2,\n",
              "  -0.5307931384602351: 1,\n",
              "  -0.5293995962074912: 4,\n",
              "  -0.5287028250811193: 1,\n",
              "  -0.5273092828283755: 3,\n",
              "  -0.5210383426910282: 1,\n",
              "  -0.5182512581855405: 4,\n",
              "  -0.5161609448064248: 2,\n",
              "  -0.5133738603009371: 3,\n",
              "  -0.5119803180481933: 1,\n",
              "  -0.5084964624163337: 26,\n",
              "  -0.5064061490372179: 1,\n",
              "  -0.5029222934053583: 1,\n",
              "  -0.4994384377734988: 1,\n",
              "  -0.4987416666471268: 1,\n",
              "  -0.4980448955207549: 1,\n",
              "  -0.4952578110152672: 1,\n",
              "  -0.4896836420042919: 3,\n",
              "  -0.4848062441196885: 8,\n",
              "  -0.4841094729933165: 1,\n",
              "  -0.4834127018669446: 1,\n",
              "  -0.48271593074057273: 1,\n",
              "  -0.4813223884878289: 2,\n",
              "  -0.47923207510871313: 3,\n",
              "  -0.4736579060977378: 1,\n",
              "  -0.4729611349713658: 1,\n",
              "  -0.47156759271862203: 1,\n",
              "  -0.46529665258127473: 1,\n",
              "  -0.463206339202159: 1,\n",
              "  -0.4590257124439275: 2,\n",
              "  -0.45763217019118363: 3,\n",
              "  -0.45693539906481173: 2,\n",
              "  -0.45275477230658023: 4,\n",
              "  -0.4513612300538364: 2,\n",
              "  -0.4485741455483487: 3,\n",
              "  -0.4478773744219768: 1,\n",
              "  -0.4471806032956049: 3,\n",
              "  -0.44648383216923293: 1,\n",
              "  -0.4436967476637453: 1,\n",
              "  -0.44299997653737333: 4,\n",
              "  -0.44230320541100143: 1,\n",
              "  -0.44160643428462953: 1,\n",
              "  -0.43951612090551373: 2,\n",
              "  -0.43881934977914183: 1,\n",
              "  -0.43463872302091033: 12,\n",
              "  -0.4339419518945384: 2,\n",
              "  -0.43185163851542263: 1,\n",
              "  -0.43115486738905073: 2,\n",
              "  -0.4283677828835631: 12,\n",
              "  -0.42697424063081924: 1,\n",
              "  -0.4248839272517035: 1,\n",
              "  -0.42070330049347193: 4,\n",
              "  -0.4172194448616124: 14,\n",
              "  -0.41582590260886854: 4,\n",
              "  -0.4151291314824966: 13,\n",
              "  -0.4137355892297528: 11,\n",
              "  -0.40816142021877744: 3,\n",
              "  -0.4074646490924055: 1,\n",
              "  -0.4060711068396617: 1,\n",
              "  -0.40467756458691784: 1,\n",
              "  -0.3991033955759425: 1,\n",
              "  -0.39840662444957053: 1,\n",
              "  -0.3956195399440829: 6,\n",
              "  -0.394922768817711: 3,\n",
              "  -0.39004537093310754: 1,\n",
              "  -0.3886518286803637: 1,\n",
              "  -0.38586474417487604: 2,\n",
              "  -0.3851679730485041: 1,\n",
              "  -0.3844712019221322: 3,\n",
              "  -0.38307765966938834: 1,\n",
              "  -0.37959380403752874: 5,\n",
              "  -0.37820026178478494: 2,\n",
              "  -0.3768067195320411: 1,\n",
              "  -0.37541317727929724: 1,\n",
              "  -0.37471640615292534: 3,\n",
              "  -0.3740196350265534: 12,\n",
              "  -0.3733228639001815: 2,\n",
              "  -0.3726260927738096: 1,\n",
              "  -0.37123255052106574: 1,\n",
              "  -0.37053577939469384: 3,\n",
              "  -0.36914223714195: 5,\n",
              "  -0.36705192376283424: 9,\n",
              "  -0.3649616103837185: 1,\n",
              "  -0.36356806813097464: 1,\n",
              "  -0.3586906702463712: 1,\n",
              "  -0.34893587447716434: 1,\n",
              "  -0.34823910335079244: 3,\n",
              "  -0.3468455610980486: 3,\n",
              "  -0.34545201884530474: 2,\n",
              "  -0.34127139208707324: 2,\n",
              "  -0.3391810787079575: 1,\n",
              "  -0.33778753645521364: 3,\n",
              "  -0.335000451949726: 1,\n",
              "  -0.33360690969698215: 1,\n",
              "  -0.3280327406860068: 4,\n",
              "  -0.32663919843326295: 5,\n",
              "  -0.3203682582959157: 1,\n",
              "  -0.317581173790428: 3,\n",
              "  -0.3168844026640561: 1,\n",
              "  -0.3134005470321965: 1,\n",
              "  -0.31131023365308075: 2,\n",
              "  -0.30712960689484925: 2,\n",
              "  -0.3050392935157335: 2,\n",
              "  -0.3015554378838739: 9,\n",
              "  -0.29737481112564235: 2,\n",
              "  -0.29667803999927045: 4,\n",
              "  -0.29598126887289855: 4,\n",
              "  -0.2952844977465266: 1,\n",
              "  -0.29319418436741085: 1,\n",
              "  -0.29249741324103895: 2,\n",
              "  -0.2911038709882951: 7,\n",
              "  -0.28901355760917935: 2,\n",
              "  -0.28483293085094785: 30,\n",
              "  -0.2841361597245759: 5,\n",
              "  -0.283439388598204: 1,\n",
              "  -0.28065230409271635: 1,\n",
              "  -0.2792587618399725: 1,\n",
              "  -0.27716844846085675: 2,\n",
              "  -0.2764716773344848: 3,\n",
              "  -0.2757749062081129: 3,\n",
              "  -0.275078135081741: 1,\n",
              "  -0.27438136395536905: 3,\n",
              "  -0.27368459282899715: 1,\n",
              "  -0.26741365269164985: 4,\n",
              "  -0.2646265681861622: 1,\n",
              "  -0.2639297970597903: 1,\n",
              "  -0.26253625480704645: 27,\n",
              "  -0.2611427125543026: 1,\n",
              "  -0.25974917030155875: 2,\n",
              "  -0.257658856922443: 7,\n",
              "  -0.25487177241695536: 1,\n",
              "  -0.2541750012905834: 2,\n",
              "  -0.2534782301642115: 1,\n",
              "  -0.2499943745323519: 15,\n",
              "  -0.24720729002686423: 1,\n",
              "  -0.24372343439500466: 1,\n",
              "  -0.2388460365104012: 3,\n",
              "  -0.23536218087854163: 3,\n",
              "  -0.23396863862579778: 6,\n",
              "  -0.23257509637305396: 1,\n",
              "  -0.23048478299393818: 2,\n",
              "  -0.22909124074119436: 2,\n",
              "  -0.22072998722473133: 1,\n",
              "  -0.2200332160983594: 9,\n",
              "  -0.2158525893401279: 1,\n",
              "  -0.21306550483464023: 12,\n",
              "  -0.21027842032915256: 5,\n",
              "  -0.20749133582366489: 8,\n",
              "  -0.20191716681268954: 2,\n",
              "  -0.20052362455994568: 1,\n",
              "  -0.19913008230720186: 2,\n",
              "  -0.1970397689280861: 1,\n",
              "  -0.19564622667534226: 1,\n",
              "  -0.19494945554897034: 1,\n",
              "  -0.19285914216985459: 1,\n",
              "  -0.19076882879073884: 4,\n",
              "  -0.1900720576643669: 12,\n",
              "  -0.1824075752742758: 1,\n",
              "  -0.18031726189516006: 4,\n",
              "  -0.17962049076878814: 1,\n",
              "  -0.17822694851604431: 1,\n",
              "  -0.17543986401055664: 4,\n",
              "  -0.17474309288418471: 2,\n",
              "  -0.17195600837869704: 3,\n",
              "  -0.1712592372523251: 3,\n",
              "  -0.1705624661259532: 1,\n",
              "  -0.16916892387320936: 1,\n",
              "  -0.16498829711497784: 4,\n",
              "  -0.16429152598860594: 2,\n",
              "  -0.1628979837358621: 5,\n",
              "  -0.16220121260949016: 4,\n",
              "  -0.16150444148311827: 2,\n",
              "  -0.16080767035674634: 1,\n",
              "  -0.1594141281040025: 4,\n",
              "  -0.15802058585125867: 5,\n",
              "  -0.14896256120842372: 4,\n",
              "  -0.13920776543921687: 1,\n",
              "  -0.13781422318647302: 1,\n",
              "  -0.13572390980735727: 2,\n",
              "  -0.13502713868098534: 1,\n",
              "  -0.13363359642824152: 8,\n",
              "  -0.1329368253018696: 4,\n",
              "  -0.13154328304912577: 1,\n",
              "  -0.13084651192275384: 5,\n",
              "  -0.12945296967001: 2,\n",
              "  -0.1217884872799189: 1,\n",
              "  -0.11691108939531547: 1,\n",
              "  -0.11551754714257163: 1,\n",
              "  -0.11133692038434012: 3,\n",
              "  -0.10994337813159628: 17,\n",
              "  -0.10924660700522437: 1,\n",
              "  -0.10854983587885245: 4,\n",
              "  -0.10785306475248053: 2,\n",
              "  -0.10506598024699286: 1,\n",
              "  -0.10436920912062093: 5,\n",
              "  -0.10367243799424902: 1,\n",
              "  -0.1029756668678771: 1,\n",
              "  -0.10227889574150519: 1,\n",
              "  -0.10158212461513326: 1,\n",
              "  -0.10088535348876135: 1,\n",
              "  -0.09670472673052984: 1,\n",
              "  -0.09600795560415791: 1,\n",
              "  -0.09113055771955449: 1,\n",
              "  -0.08904024434043872: 2,\n",
              "  -0.08834347321406681: 13,\n",
              "  -0.08694993096132297: 1,\n",
              "  -0.08555638870857914: 13,\n",
              "  -0.08485961758220723: 1,\n",
              "  -0.08276930420309146: 1,\n",
              "  -0.07858867744485995: 2,\n",
              "  -0.07580159293937228: 3,\n",
              "  -0.06813711054928118: 1,\n",
              "  -0.06744033942290925: 1,\n",
              "  -0.06047262815919007: 1,\n",
              "  -0.05977585703281815: 2,\n",
              "  -0.05698877252733048: 4,\n",
              "  -0.05629200140095856: 3,\n",
              "  -0.05559523027458664: 4,\n",
              "  -0.052808145769098966: 1,\n",
              "  -0.05141460351635513: 1,\n",
              "  -0.04514366337900786: 29,\n",
              "  -0.04444689225263594: 2,\n",
              "  -0.04375012112626402: 3,\n",
              "  -0.04026626549440443: 4,\n",
              "  -0.036782409862544836: 1,\n",
              "  -0.03329855423068524: 1,\n",
              "  -0.029117927472453734: 13,\n",
              "  -0.027724385219709897: 1,\n",
              "  -0.020059902829618795: 1,\n",
              "  -0.01239542043952769: 3,\n",
              "  -0.01169864931315577: 2,\n",
              "  -0.00751802255492426: 2,\n",
              "  -0.0047309380494365855: 2,\n",
              "  -0.003337395796692749: 1,\n",
              "  -0.0012470824175769932: 3,\n",
              "  -0.0005503112912050746: 1,\n",
              "  0.00014645983516684387: 4,\n",
              "  0.0022367732142825995: 3,\n",
              "  0.0036303154670264365: 2,\n",
              "  0.0071141710988860294: 1,\n",
              "  0.009204484478001785: 2,\n",
              "  0.01199156898348946: 4,\n",
              "  0.012688340109861378: 5,\n",
              "  0.015475424615349051: 2,\n",
              "  0.017565737994464807: 1,\n",
              "  0.02174636475269632: 1,\n",
              "  0.023836678131812075: 1,\n",
              "  0.025230220384555913: 7,\n",
              "  0.02732053376367167: 9,\n",
              "  0.02941084714278742: 1,\n",
              "  0.03010761826915934: 1,\n",
              "  0.03080438939553126: 1,\n",
              "  0.0321979316482751: 2,\n",
              "  0.03289470277464702: 3,\n",
              "  0.034985016153762774: 2,\n",
              "  0.03846887178562236: 1,\n",
              "  0.0398624140383662: 3,\n",
              "  0.04404304079659771: 1,\n",
              "  0.04543658304934155: 1,\n",
              "  0.046133354175713466: 1,\n",
              "  0.05310106543943265: 1,\n",
              "  0.05449460769217649: 6,\n",
              "  0.056584921071292246: 3,\n",
              "  0.062159090082267596: 1,\n",
              "  0.06355263233501143: 4,\n",
              "  0.06494617458775527: 1,\n",
              "  0.07052034359873062: 1,\n",
              "  0.07470097035696213: 4,\n",
              "  0.0774880548624498: 7,\n",
              "  0.08166868162068132: 3,\n",
              "  0.08584930837891283: 6,\n",
              "  0.08933316401077242: 4,\n",
              "  0.09002993513714433: 4,\n",
              "  0.09490733302174777: 3,\n",
              "  0.10187504428546695: 5,\n",
              "  0.1039653576645827: 2,\n",
              "  0.10675244217007038: 2,\n",
              "  0.10814598442281421: 2,\n",
              "  0.10953952667555805: 11,\n",
              "  0.11162984005467382: 10,\n",
              "  0.11581046681290533: 7,\n",
              "  0.11790078019202108: 1,\n",
              "  0.118597551318393: 5,\n",
              "  0.12277817807662451: 2,\n",
              "  0.12347494920299643: 2,\n",
              "  0.12556526258211217: 2,\n",
              "  0.1262620337084841: 1,\n",
              "  0.12695880483485603: 1,\n",
              "  0.12835234708759985: 7,\n",
              "  0.13253297384583138: 3,\n",
              "  0.13462328722494712: 2,\n",
              "  0.1374103717304348: 1,\n",
              "  0.13880391398317862: 2,\n",
              "  0.14228776961503822: 5,\n",
              "  0.14646839637326975: 1,\n",
              "  0.14995225200512932: 4,\n",
              "  0.15064902313150125: 1,\n",
              "  0.15134579425787317: 1,\n",
              "  0.1520425653842451: 5,\n",
              "  0.15552642101610467: 6,\n",
              "  0.15761673439522045: 2,\n",
              "  0.15831350552159235: 1,\n",
              "  0.15901027664796427: 1,\n",
              "  0.16179736115345195: 2,\n",
              "  0.16249413227982387: 1,\n",
              "  0.1638876745325677: 2,\n",
              "  0.16458444565893962: 3,\n",
              "  0.16528121678531155: 1,\n",
              "  0.16597798791168344: 5,\n",
              "  0.16667475903805537: 1,\n",
              "  0.1673715301644273: 2,\n",
              "  0.16806830129079922: 6,\n",
              "  0.17015861466991497: 1,\n",
              "  0.17224892804903072: 5,\n",
              "  0.17573278368089032: 1,\n",
              "  0.17712632593363414: 4,\n",
              "  0.17921663931274992: 8,\n",
              "  0.18130695269186567: 1,\n",
              "  0.1820037238182376: 1,\n",
              "  0.18827466395558484: 1,\n",
              "  0.18897143508195677: 10,\n",
              "  0.19175851958744444: 4,\n",
              "  0.19524237521930404: 2,\n",
              "  0.19942300197753554: 1,\n",
              "  0.2015133153566513: 3,\n",
              "  0.2049971709885109: 9,\n",
              "  0.20639071324125474: 3,\n",
              "  0.20917779774674242: 3,\n",
              "  0.2119648822522301: 3,\n",
              "  0.21684228013683352: 1,\n",
              "  0.21753905126320544: 4,\n",
              "  0.21893259351594926: 5,\n",
              "  0.2196293646423212: 1,\n",
              "  0.221022906895065: 1,\n",
              "  0.22171967802143694: 1,\n",
              "  0.22241644914780886: 1,\n",
              "  0.2245067625269246: 1,\n",
              "  0.22590030477966846: 2,\n",
              "  0.2272938470324123: 4,\n",
              "  0.22799061815878421: 2,\n",
              "  0.2286873892851561: 23,\n",
              "  0.22938416041152804: 12,\n",
              "  0.2307777026642719: 1,\n",
              "  0.23286801604338764: 1,\n",
              "  0.23426155829613146: 8,\n",
              "  0.23635187167524724: 1,\n",
              "  0.24122926955985066: 8,\n",
              "  0.24262281181259449: 1,\n",
              "  0.24401635406533834: 3,\n",
              "  0.24819698082356983: 2,\n",
              "  0.2530743787081733: 8,\n",
              "  0.25586146321366093: 15,\n",
              "  0.2565582343400329: 1,\n",
              "  0.2572550054664048: 3,\n",
              "  0.2579517765927767: 1,\n",
              "  0.25864854771914864: 2,\n",
              "  0.25934531884552053: 1,\n",
              "  0.2607388610982644: 5,\n",
              "  0.26213240335100824: 1,\n",
              "  0.2649194878564959: 1,\n",
              "  0.2684033434883555: 3,\n",
              "  0.27119042799384313: 3,\n",
              "  0.2718871991202151: 12,\n",
              "  0.27397751249933083: 1,\n",
              "  0.2767645970048185: 2,\n",
              "  0.2795516815103062: 2,\n",
              "  0.28094522376305003: 13,\n",
              "  0.28233876601579383: 3,\n",
              "  0.2830355371421658: 1,\n",
              "  0.28442907939490963: 2,\n",
              "  0.28512585052128153: 1,\n",
              "  0.28582262164765343: 2,\n",
              "  0.2865193927740254: 1,\n",
              "  0.2913967906586288: 4,\n",
              "  0.29279033291137263: 29,\n",
              "  0.2941838751641165: 5,\n",
              "  0.2948806462904884: 1,\n",
              "  0.29557741741686033: 1,\n",
              "  0.3025451286805795: 1,\n",
              "  0.3053322131860672: 10,\n",
              "  0.3081192976915548: 5,\n",
              "  0.31090638219704253: 1,\n",
              "  0.3122999244497863: 1,\n",
              "  0.31439023782890213: 2,\n",
              "  0.31508700895527403: 3,\n",
              "  0.3157837800816459: 1,\n",
              "  0.3164805512080179: 1,\n",
              "  0.3192676357135055: 1,\n",
              "  0.3199644068398775: 9,\n",
              "  0.3206611779662494: 1,\n",
              "  0.3213579490926213: 5,\n",
              "  0.32205472021899323: 2,\n",
              "  0.3262353469772247: 3,\n",
              "  0.3269321181035966: 2,\n",
              "  0.3297192026090843: 1,\n",
              "  0.3304159737354562: 2,\n",
              "  ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yT6F4YVbBwvO"
      },
      "cell_type": "markdown",
      "source": [
        "## See which train users are not in test"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "1cfa3f74-063f-473f-fe0f-0f21669fdbac",
        "id": "WirW9CPHBwvP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "indices = []\n",
        "train_user_ids = np.unique(train_user_data[:,-1])\n",
        "for i in range(len(test_user_data[:,-1])):\n",
        "    if test_user_data[i,-1] not in train_user_ids:\n",
        "        indices.append(i)\n",
        "        \n",
        "len(indices) / len(test_user_data[:,-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.7929060187124703e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "c6738e7b-71d1-4a77-fcca-7ec6eb4cfcce",
        "id": "Wr8leuOvBwvT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_matrices = np.delete(test_matrices, indices, axis=0)\n",
        "test_classes_t = np.delete(test_classes_t, indices)\n",
        "test_classes = np.delete(test_classes, indices)\n",
        "test_user_data = np.delete(test_user_data, indices, axis=0)\n",
        "test_btc_meta_data = np.delete(test_btc_meta_data, indices, axis=0)\n",
        "test_dates = np.delete(test_dates, indices)\n",
        "test_matrices.shape, test_classes_t.shape, test_user_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35804, 126, 7), (35804,), (35804, 8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ka0gbU_-BwvY"
      },
      "cell_type": "markdown",
      "source": [
        "### Compare occurences users training versus test set"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "9b2ceebd-4d7d-44fb-b009-291ac20fe5c0",
        "id": "Rvf8QhFCBwvZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34051
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "NOTE: Amount of occurences is SUPER low, so I'm pretty sure we can't map any \n",
        "relevant information for a user.\n",
        "'''\n",
        "\n",
        "unique, counts = np.unique(train_user_data[:,-1], return_counts=True)\n",
        "cnt_train = dict(zip(unique, counts))\n",
        "unique, counts = np.unique(test_user_data[:,-1], return_counts=True)\n",
        "cnt_test = dict(zip(unique, counts))\n",
        "\n",
        "list_users_occ = [k for k,v in cnt_train.items() if v > 10]\n",
        "list_users_occ, cnt_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([8.39137366786943e-05,\n",
              "  0.0009230511034656374,\n",
              "  0.0013426197868591089,\n",
              "  0.001678274733573886,\n",
              "  0.0028530670470756064,\n",
              "  0.0034404632038264665,\n",
              "  0.0037761181505412437,\n",
              "  0.004363514307292104,\n",
              "  0.004699169254006881,\n",
              "  0.005034824200721658,\n",
              "  0.005118737937400353,\n",
              "  0.00545439288411513,\n",
              "  0.005873961567508601,\n",
              "  0.005957875304187296,\n",
              "  0.00604178904086599,\n",
              "  0.0062096165142233786,\n",
              "  0.006964840144331627,\n",
              "  0.008223546194512042,\n",
              "  0.009230511034656374,\n",
              "  0.009314424771335067,\n",
              "  0.009398338508013762,\n",
              "  0.009817907191407233,\n",
              "  0.01015356213812201,\n",
              "  0.010405303348158094,\n",
              "  0.010740958294872871,\n",
              "  0.010824872031551564,\n",
              "  0.01090878576823026,\n",
              "  0.010992699504908953,\n",
              "  0.011412268188302425,\n",
              "  0.011496181924981119,\n",
              "  0.012167491818410673,\n",
              "  0.012587060501804146,\n",
              "  0.01367793907862717,\n",
              "  0.014181421498699337,\n",
              "  0.014600990182092809,\n",
              "  0.015188386338843668,\n",
              "  0.015272300075522363,\n",
              "  0.016279264915666693,\n",
              "  0.016447092389024084,\n",
              "  0.01653100612570278,\n",
              "  0.01661491986238147,\n",
              "  0.016950574809096248,\n",
              "  0.017118402282453638,\n",
              "  0.017454057229168415,\n",
              "  0.01753797096584711,\n",
              "  0.017621884702525802,\n",
              "  0.018209280859276665,\n",
              "  0.018293194595955357,\n",
              "  0.018461022069312747,\n",
              "  0.018796677016027524,\n",
              "  0.0191323319627423,\n",
              "  0.019216245699420997,\n",
              "  0.01930015943609969,\n",
              "  0.01946798690945708,\n",
              "  0.01988755559285055,\n",
              "  0.020055383066207938,\n",
              "  0.020391038012922715,\n",
              "  0.020642779222958797,\n",
              "  0.020894520432994883,\n",
              "  0.020978434169673574,\n",
              "  0.02106234790635227,\n",
              "  0.021733657799781824,\n",
              "  0.022153226483175296,\n",
              "  0.02223714021985399,\n",
              "  0.022321053956532683,\n",
              "  0.02257279516656877,\n",
              "  0.022992363849962238,\n",
              "  0.023244105059998323,\n",
              "  0.0235797600067131,\n",
              "  0.023747587480070487,\n",
              "  0.023831501216749183,\n",
              "  0.023915414953427878,\n",
              "  0.024083242426785265,\n",
              "  0.024586724846857432,\n",
              "  0.024838466056893514,\n",
              "  0.025090207266929596,\n",
              "  0.025258034740286987,\n",
              "  0.025425862213644373,\n",
              "  0.025677603423680456,\n",
              "  0.025845430897037846,\n",
              "  0.026600654527146095,\n",
              "  0.026684568263824787,\n",
              "  0.026852395737182178,\n",
              "  0.027020223210539564,\n",
              "  0.02727196442057565,\n",
              "  0.028362842997398673,\n",
              "  0.02844675673407737,\n",
              "  0.028614584207434755,\n",
              "  0.02869849794411345,\n",
              "  0.02886632541747084,\n",
              "  0.029034152890828228,\n",
              "  0.029369807837543005,\n",
              "  0.029537635310900395,\n",
              "  0.029621549047579087,\n",
              "  0.029705462784257782,\n",
              "  0.029957203994293864,\n",
              "  0.030964168834438196,\n",
              "  0.03104808257111689,\n",
              "  0.031131996307795586,\n",
              "  0.03121591004447428,\n",
              "  0.03155156499118906,\n",
              "  0.03171939246454645,\n",
              "  0.03180330620122514,\n",
              "  0.032390702357976,\n",
              "  0.03247461609465469,\n",
              "  0.03255852983133339,\n",
              "  0.03281027104136947,\n",
              "  0.03289418477804817,\n",
              "  0.03322983972476294,\n",
              "  0.0338172358815138,\n",
              "  0.034068977091549886,\n",
              "  0.034236804564907276,\n",
              "  0.03507594193169422,\n",
              "  0.03591507929848116,\n",
              "  0.03608290677183855,\n",
              "  0.03641856171855333,\n",
              "  0.03658638919191071,\n",
              "  0.037089871611982884,\n",
              "  0.03717378534866157,\n",
              "  0.03784509524209113,\n",
              "  0.0382646639254846,\n",
              "  0.03843249139884199,\n",
              "  0.03927162876562893,\n",
              "  0.03952336997566502,\n",
              "  0.04002685239573718,\n",
              "  0.040530334815809345,\n",
              "  0.04061424855248804,\n",
              "  0.041033817235881516,\n",
              "  0.04111773097256021,\n",
              "  0.0412016447092389,\n",
              "  0.04136947218259629,\n",
              "  0.041872954602668454,\n",
              "  0.042376437022740625,\n",
              "  0.04254426449609801,\n",
              "  0.0427120919694554,\n",
              "  0.043131660652848874,\n",
              "  0.04388688428295712,\n",
              "  0.04405471175631451,\n",
              "  0.0441386254929932,\n",
              "  0.04439036670302929,\n",
              "  0.04506167659645884,\n",
              "  0.04514559033313754,\n",
              "  0.04531341780649492,\n",
              "  0.045397331543173616,\n",
              "  0.04548124527985231,\n",
              "  0.046320382646639256,\n",
              "  0.04648821011999665,\n",
              "  0.046572123856675335,\n",
              "  0.04665603759335403,\n",
              "  0.04724343375010489,\n",
              "  0.047327347486783584,\n",
              "  0.04757908869681967,\n",
              "  0.04774691617017706,\n",
              "  0.047830829906855755,\n",
              "  0.047914743643534444,\n",
              "  0.048418226063606615,\n",
              "  0.04866996727364269,\n",
              "  0.04875388101032139,\n",
              "  0.04959301837710833,\n",
              "  0.049760845850465724,\n",
              "  0.04992867332382311,\n",
              "  0.0500965007971805,\n",
              "  0.05043215574389528,\n",
              "  0.05059998321725266,\n",
              "  0.05068389695393136,\n",
              "  0.05076781069061005,\n",
              "  0.05093563816396744,\n",
              "  0.05110346563732483,\n",
              "  0.051271293110682216,\n",
              "  0.05244608542418394,\n",
              "  0.052613912897541325,\n",
              "  0.05286565410757741,\n",
              "  0.053704791474364355,\n",
              "  0.05420827389443652,\n",
              "  0.05437610136779391,\n",
              "  0.0545439288411513,\n",
              "  0.05471175631450868,\n",
              "  0.05546697994461693,\n",
              "  0.05664177225811865,\n",
              "  0.05680959973147604,\n",
              "  0.05706134094151213,\n",
              "  0.05714525467819082,\n",
              "  0.05798439204497776,\n",
              "  0.05857178820172862,\n",
              "  0.05873961567508601,\n",
              "  0.059578753041872955,\n",
              "  0.05966266677855165,\n",
              "  0.05983049425190904,\n",
              "  0.059998321725266424,\n",
              "  0.06058571788201728,\n",
              "  0.06083745909205337,\n",
              "  0.060921372828732064,\n",
              "  0.06176051019551901,\n",
              "  0.061844423932197704,\n",
              "  0.06201225140555509,\n",
              "  0.06218007887891248,\n",
              "  0.06234790635226987,\n",
              "  0.06276747503566334,\n",
              "  0.06293530250902073,\n",
              "  0.06310312998237812,\n",
              "  0.0633548711924142,\n",
              "  0.06369052613912897,\n",
              "  0.06377443987580766,\n",
              "  0.06469749097927331,\n",
              "  0.0648653184526307,\n",
              "  0.06494923218930938,\n",
              "  0.06528488713602416,\n",
              "  0.06545271460938155,\n",
              "  0.06562054208273894,\n",
              "  0.06578836955609633,\n",
              "  0.06604011076613242,\n",
              "  0.06654359318620458,\n",
              "  0.06721490307963414,\n",
              "  0.06746664428967022,\n",
              "  0.0677183854997063,\n",
              "  0.06830578165645716,\n",
              "  0.06864143660317194,\n",
              "  0.06880926407652933,\n",
              "  0.0692288327599228,\n",
              "  0.06931274649660149,\n",
              "  0.06948057396995888,\n",
              "  0.06956448770663758,\n",
              "  0.06990014265335236,\n",
              "  0.06998405639003105,\n",
              "  0.07006797012670975,\n",
              "  0.07023579760006714,\n",
              "  0.07099102123017538,\n",
              "  0.07115884870353277,\n",
              "  0.07124276244021147,\n",
              "  0.07157841738692625,\n",
              "  0.07166233112360493,\n",
              "  0.0721658135436771,\n",
              "  0.0722497272803558,\n",
              "  0.07275320970042796,\n",
              "  0.07292103717378536,\n",
              "  0.07308886464714273,\n",
              "  0.07317277838382143,\n",
              "  0.07334060585717882,\n",
              "  0.0735923470672149,\n",
              "  0.0736762608038936,\n",
              "  0.07384408827725099,\n",
              "  0.07409582948728707,\n",
              "  0.07426365696064446,\n",
              "  0.07443148443400184,\n",
              "  0.07451539817068054,\n",
              "  0.07476713938071662,\n",
              "  0.07485105311739532,\n",
              "  0.07493496685407401,\n",
              "  0.07527062180078879,\n",
              "  0.07552236301082488,\n",
              "  0.07577410422086095,\n",
              "  0.07619367290425443,\n",
              "  0.0765293278509692,\n",
              "  0.0766132415876479,\n",
              "  0.0766971553243266,\n",
              "  0.07711672400772006,\n",
              "  0.07720063774439875,\n",
              "  0.07736846521775614,\n",
              "  0.07753629269111353,\n",
              "  0.07762020642779223,\n",
              "  0.07770412016447092,\n",
              "  0.07778803390114962,\n",
              "  0.0780397751111857,\n",
              "  0.07837543005790047,\n",
              "  0.07871108500461525,\n",
              "  0.0796341361080809,\n",
              "  0.08013761852815306,\n",
              "  0.08022153226483175,\n",
              "  0.08047327347486784,\n",
              "  0.08064110094822523,\n",
              "  0.08097675589494001,\n",
              "  0.08122849710497608,\n",
              "  0.08131241084165478,\n",
              "  0.08164806578836956,\n",
              "  0.08189980699840564,\n",
              "  0.08198372073508434,\n",
              "  0.08215154820844173,\n",
              "  0.08265503062851388,\n",
              "  0.08307459931190736,\n",
              "  0.08324242678526475,\n",
              "  0.08424939162540908,\n",
              "  0.08433330536208777,\n",
              "  0.0853402702022321,\n",
              "  0.0854241839389108,\n",
              "  0.08559201141226819,\n",
              "  0.08575983888562558,\n",
              "  0.08601158009566166,\n",
              "  0.08626332130569775,\n",
              "  0.08659897625241252,\n",
              "  0.08752202735587816,\n",
              "  0.0882772509859864,\n",
              "  0.08911638835277334,\n",
              "  0.09045900813963245,\n",
              "  0.09079466308634723,\n",
              "  0.09087857682302593,\n",
              "  0.0912981455064194,\n",
              "  0.09154988671645549,\n",
              "  0.09171771418981287,\n",
              "  0.09180162792649156,\n",
              "  0.09222119660988504,\n",
              "  0.09255685155659982,\n",
              "  0.09272467902995721,\n",
              "  0.0929764202399933,\n",
              "  0.09322816145002936,\n",
              "  0.09331207518670806,\n",
              "  0.09339598892338676,\n",
              "  0.09373164387010154,\n",
              "  0.09389947134345893,\n",
              "  0.0942351262901737,\n",
              "  0.0943190400268524,\n",
              "  0.09515817739363934,\n",
              "  0.09574557355039019,\n",
              "  0.09591340102374758,\n",
              "  0.09616514223378367,\n",
              "  0.09633296970714106,\n",
              "  0.09650079718049845,\n",
              "  0.09700427960057062,\n",
              "  0.097172107073928,\n",
              "  0.09725602081060669,\n",
              "  0.09784341696735756,\n",
              "  0.09834689938742973,\n",
              "  0.09843081312410841,\n",
              "  0.09943777796425275,\n",
              "  0.09968951917428884,\n",
              "  0.09994126038432491,\n",
              "  0.1000251741210036,\n",
              "  0.10061257027775447,\n",
              "  0.10103213896114795,\n",
              "  0.10128388017118402,\n",
              "  0.10203910380129227,\n",
              "  0.10220693127464966,\n",
              "  0.10237475874800706,\n",
              "  0.10287824116807921,\n",
              "  0.10296215490475791,\n",
              "  0.10329780985147269,\n",
              "  0.10338172358815138,\n",
              "  0.10371737853486616,\n",
              "  0.1045565159016531,\n",
              "  0.1046404296383318,\n",
              "  0.10564739447847613,\n",
              "  0.10589913568851221,\n",
              "  0.1060669631618696,\n",
              "  0.10699001426533523,\n",
              "  0.10799697910547956,\n",
              "  0.10833263405219434,\n",
              "  0.10841654778887304,\n",
              "  0.10866828899890912,\n",
              "  0.10875220273558782,\n",
              "  0.1090039439456239,\n",
              "  0.1090878576823026,\n",
              "  0.10925568515565998,\n",
              "  0.10975916757573215,\n",
              "  0.11001090878576823,\n",
              "  0.11009482252244693,\n",
              "  0.11017873625912562,\n",
              "  0.11026264999580432,\n",
              "  0.11034656373248301,\n",
              "  0.11051439120584039,\n",
              "  0.11076613241587648,\n",
              "  0.11085004615255517,\n",
              "  0.11126961483594865,\n",
              "  0.11177309725602082,\n",
              "  0.1118570109926995,\n",
              "  0.1119409247293782,\n",
              "  0.11210875220273558,\n",
              "  0.11244440714945036,\n",
              "  0.1132835445162373,\n",
              "  0.11387094067298817,\n",
              "  0.11437442309306034,\n",
              "  0.11496181924981119,\n",
              "  0.11512964672316858,\n",
              "  0.11571704287991945,\n",
              "  0.11672400772006378,\n",
              "  0.11680792145674247,\n",
              "  0.11756314508685071,\n",
              "  0.1177309725602081,\n",
              "  0.11806662750692289,\n",
              "  0.11840228245363767,\n",
              "  0.11865402366367374,\n",
              "  0.1193253335571033,\n",
              "  0.11957707476713939,\n",
              "  0.11974490224049678,\n",
              "  0.11999664345053285,\n",
              "  0.12008055718721154,\n",
              "  0.12016447092389024,\n",
              "  0.12058403960728371,\n",
              "  0.12066795334396241,\n",
              "  0.1207518670806411,\n",
              "  0.1209196945539985,\n",
              "  0.12108752202735588,\n",
              "  0.12125534950071326,\n",
              "  0.12150709071074935,\n",
              "  0.12159100444742804,\n",
              "  0.12201057313082152,\n",
              "  0.12217840060417891,\n",
              "  0.12284971049760846,\n",
              "  0.12293362423428715,\n",
              "  0.12335319291768063,\n",
              "  0.12352102039103802,\n",
              "  0.12427624402114626,\n",
              "  0.12444407149450365,\n",
              "  0.12486364017789713,\n",
              "  0.12494755391457582,\n",
              "  0.1250314676512545,\n",
              "  0.1251153813879332,\n",
              "  0.1251992951246119,\n",
              "  0.125451036334648,\n",
              "  0.12553495007132667,\n",
              "  0.1266258286481497,\n",
              "  0.1267097423848284,\n",
              "  0.12696148359486448,\n",
              "  0.1270453973315432,\n",
              "  0.12738105227825794,\n",
              "  0.12771670722497272,\n",
              "  0.12780062096165143,\n",
              "  0.12855584459175967,\n",
              "  0.12872367206511706,\n",
              "  0.12905932701183184,\n",
              "  0.12922715448518923,\n",
              "  0.12989846437861877,\n",
              "  0.1310732566921205,\n",
              "  0.13132499790215657,\n",
              "  0.13140891163883528,\n",
              "  0.13149282537551396,\n",
              "  0.13174456658555006,\n",
              "  0.13191239405890745,\n",
              "  0.13199630779558613,\n",
              "  0.13241587647897962,\n",
              "  0.13258370395233698,\n",
              "  0.13283544516237308,\n",
              "  0.13367458252916004,\n",
              "  0.13375849626583872,\n",
              "  0.13434589242258957,\n",
              "  0.1353528572627339,\n",
              "  0.1355206847360913,\n",
              "  0.13560459847276998,\n",
              "  0.13577242594612737,\n",
              "  0.13644373583955693,\n",
              "  0.136695477049593,\n",
              "  0.1368633045229504,\n",
              "  0.13719895946966518,\n",
              "  0.13736678694302257,\n",
              "  0.13761852815305867,\n",
              "  0.13787026936309474,\n",
              "  0.13795418309977342,\n",
              "  0.13820592430980952,\n",
              "  0.1384576655198456,\n",
              "  0.1385415792565243,\n",
              "  0.1387094067298817,\n",
              "  0.13887723420323908,\n",
              "  0.13904506167659647,\n",
              "  0.13929680288663254,\n",
              "  0.1401359402534195,\n",
              "  0.14047159520013427,\n",
              "  0.14055550893681296,\n",
              "  0.14072333641017035,\n",
              "  0.14080725014684903,\n",
              "  0.14089116388352774,\n",
              "  0.1415624737769573,\n",
              "  0.14164638751363598,\n",
              "  0.14181421498699337,\n",
              "  0.14223378367038683,\n",
              "  0.14231769740706554,\n",
              "  0.14248552488042293,\n",
              "  0.1429890073004951,\n",
              "  0.14382814466728203,\n",
              "  0.1441637996139968,\n",
              "  0.14424771335067552,\n",
              "  0.1443316270873542,\n",
              "  0.1444155408240329,\n",
              "  0.14550641940085593,\n",
              "  0.14617772929428546,\n",
              "  0.14642947050432156,\n",
              "  0.1471846941344298,\n",
              "  0.1472686078711085,\n",
              "  0.14768817655450198,\n",
              "  0.14793991776453805,\n",
              "  0.14844340018461022,\n",
              "  0.14886296886800368,\n",
              "  0.1489468826046824,\n",
              "  0.14928253755139717,\n",
              "  0.14953427876143324,\n",
              "  0.14961819249811195,\n",
              "  0.1503734161282202,\n",
              "  0.15054124360157758,\n",
              "  0.15096081228497105,\n",
              "  0.15104472602164976,\n",
              "  0.15112863975832844,\n",
              "  0.1516321221784006,\n",
              "  0.1519677771251154,\n",
              "  0.15205169086179407,\n",
              "  0.15221951833515146,\n",
              "  0.15247125954518756,\n",
              "  0.15347822438533187,\n",
              "  0.15398170680540404,\n",
              "  0.15406562054208273,\n",
              "  0.15448518922547622,\n",
              "  0.1547369304355123,\n",
              "  0.154820844172191,\n",
              "  0.15490475790886968,\n",
              "  0.15532432659226314,\n",
              "  0.1558278090123353,\n",
              "  0.1564152051690862,\n",
              "  0.15649911890576487,\n",
              "  0.15683477385247965,\n",
              "  0.15725434253587312,\n",
              "  0.15733825627255182,\n",
              "  0.15834522111269614,\n",
              "  0.15859696232273224,\n",
              "  0.1590165310061257,\n",
              "  0.1591004447428044,\n",
              "  0.1591843584794831,\n",
              "  0.1592682722161618,\n",
              "  0.16010740958294872,\n",
              "  0.16019132331962743,\n",
              "  0.1605269782663422,\n",
              "  0.16077871947637828,\n",
              "  0.16111437442309307,\n",
              "  0.16119828815977175,\n",
              "  0.16153394310648653,\n",
              "  0.16170177057984392,\n",
              "  0.16195351178988002,\n",
              "  0.16245699420995216,\n",
              "  0.16270873541998826,\n",
              "  0.16287656289334565,\n",
              "  0.16321221784006043,\n",
              "  0.1632961315767391,\n",
              "  0.1634639590500965,\n",
              "  0.16371570026013257,\n",
              "  0.16413526894352606,\n",
              "  0.16421918268020474,\n",
              "  0.16438701015356214,\n",
              "  0.164974406310313,\n",
              "  0.1662331123604934,\n",
              "  0.16690442225392296,\n",
              "  0.16724007720063774,\n",
              "  0.16732399093731645,\n",
              "  0.16740790467399513,\n",
              "  0.1679113870940673,\n",
              "  0.16849878325081816,\n",
              "  0.16858269698749687,\n",
              "  0.16866661072417555,\n",
              "  0.16883443819753294,\n",
              "  0.16917009314424772,\n",
              "  0.17000923051103467,\n",
              "  0.17009314424771335,\n",
              "  0.17017705798439203,\n",
              "  0.17126793656121508,\n",
              "  0.17143576403457245,\n",
              "  0.17168750524460855,\n",
              "  0.1723588151380381,\n",
              "  0.17261055634807418,\n",
              "  0.17336577997818242,\n",
              "  0.1737014349248972,\n",
              "  0.1738692623982546,\n",
              "  0.17403708987161198,\n",
              "  0.17420491734496937,\n",
              "  0.17479231350172023,\n",
              "  0.17487622723839893,\n",
              "  0.17537970965847108,\n",
              "  0.17613493328857935,\n",
              "  0.17638667449861542,\n",
              "  0.1765545019719728,\n",
              "  0.17680624318200888,\n",
              "  0.17714189812872366,\n",
              "  0.17764538054879583,\n",
              "  0.17772929428547454,\n",
              "  0.17865234538894018,\n",
              "  0.17890408659897625,\n",
              "  0.17907191407233364,\n",
              "  0.1795753964924058,\n",
              "  0.1796593102290845,\n",
              "  0.1799110514391206,\n",
              "  0.18024670638583537,\n",
              "  0.18066627506922883,\n",
              "  0.1810858437526223,\n",
              "  0.1813375849626584,\n",
              "  0.18158932617269447,\n",
              "  0.18167323990937317,\n",
              "  0.18184106738273056,\n",
              "  0.18192498111940925,\n",
              "  0.18200889485608795,\n",
              "  0.1825123772761601,\n",
              "  0.1840228245363766,\n",
              "  0.18469413442980617,\n",
              "  0.18519761684987832,\n",
              "  0.1853654443232357,\n",
              "  0.1855332717965931,\n",
              "  0.1857850130066292,\n",
              "  0.18586892674330788,\n",
              "  0.18628849542670134,\n",
              "  0.18662415037341612,\n",
              "  0.18687589158345222,\n",
              "  0.1869598053201309,\n",
              "  0.18729546026684568,\n",
              "  0.1873793740035244,\n",
              "  0.18746328774020307,\n",
              "  0.18830242510699002,\n",
              "  0.1886380800537048,\n",
              "  0.18880590752706217,\n",
              "  0.18930938994713434,\n",
              "  0.18947721742049173,\n",
              "  0.18964504489384912,\n",
              "  0.1898128723672065,\n",
              "  0.1900646135772426,\n",
              "  0.1901485273139213,\n",
              "  0.19031635478727868,\n",
              "  0.19040026852395736,\n",
              "  0.19073592347067214,\n",
              "  0.19081983720735085,\n",
              "  0.19149114710078038,\n",
              "  0.19174288831081648,\n",
              "  0.19199462952085256,\n",
              "  0.19233028446756734,\n",
              "  0.1931694218343543,\n",
              "  0.19325333557103297,\n",
              "  0.19342116304439036,\n",
              "  0.19358899051774775,\n",
              "  0.1941763866744986,\n",
              "  0.1944281278845347,\n",
              "  0.19451204162121338,\n",
              "  0.19493161030460687,\n",
              "  0.19501552404128555,\n",
              "  0.19509943777796426,\n",
              "  0.1956029201980364,\n",
              "  0.1959385751447512,\n",
              "  0.19627423009146597,\n",
              "  0.1972811949316103,\n",
              "  0.19795250482503987,\n",
              "  0.1988755559285055,\n",
              "  0.19912729713854158,\n",
              "  0.19921121087522028,\n",
              "  0.19937903834857768,\n",
              "  0.19946295208525636,\n",
              "  0.19954686582193504,\n",
              "  0.19996643450532853,\n",
              "  0.2004699169254007,\n",
              "  0.20055383066207938,\n",
              "  0.20072165813543677,\n",
              "  0.20105731308215155,\n",
              "  0.20130905429218762,\n",
              "  0.2018964504489385,\n",
              "  0.20256776034236804,\n",
              "  0.20273558781572543,\n",
              "  0.2030712427624402,\n",
              "  0.20315515649911892,\n",
              "  0.20357472518251238,\n",
              "  0.20374255265586977,\n",
              "  0.20407820760258455,\n",
              "  0.20416212133926323,\n",
              "  0.20424603507594194,\n",
              "  0.20432994881262062,\n",
              "  0.20441386254929933,\n",
              "  0.2047495174960141,\n",
              "  0.2048334312326928,\n",
              "  0.20500125870605018,\n",
              "  0.20516908617940757,\n",
              "  0.2062599647562306,\n",
              "  0.20718301585969623,\n",
              "  0.20726692959637494,\n",
              "  0.2076025845430897,\n",
              "  0.20793823948980447,\n",
              "  0.20827389443651925,\n",
              "  0.20835780817319796,\n",
              "  0.20844172190987664,\n",
              "  0.20860954938323403,\n",
              "  0.2088612905932701,\n",
              "  0.2091130318033062,\n",
              "  0.20919694553998489,\n",
              "  0.20995216917009316,\n",
              "  0.21011999664345055,\n",
              "  0.21020391038012923,\n",
              "  0.21037173785348662,\n",
              "  0.2106234790635227,\n",
              "  0.2108752202735588,\n",
              "  0.21095913401023747,\n",
              "  0.21146261643030964,\n",
              "  0.21163044390366703,\n",
              "  0.2119660988503818,\n",
              "  0.2121339263237392,\n",
              "  0.21221784006041788,\n",
              "  0.21238566753377527,\n",
              "  0.21246958127045398,\n",
              "  0.21322480490056223,\n",
              "  0.2133087186372409,\n",
              "  0.2136443735839557,\n",
              "  0.2137282873206344,\n",
              "  0.21406394226734918,\n",
              "  0.21431568347738525,\n",
              "  0.21439959721406393,\n",
              "  0.2149869933708148,\n",
              "  0.2151548208441722,\n",
              "  0.21523873458085088,\n",
              "  0.21549047579088698,\n",
              "  0.21557438952756566,\n",
              "  0.21591004447428044,\n",
              "  0.21616178568431652,\n",
              "  0.21658135436771,\n",
              "  0.2167491818410674,\n",
              "  0.21691700931442476,\n",
              "  0.21758831920785432,\n",
              "  0.2177561466812117,\n",
              "  0.21784006041789042,\n",
              "  0.21867919778467734,\n",
              "  0.21876311152135605,\n",
              "  0.2191826802047495,\n",
              "  0.2193505076781069,\n",
              "  0.21960224888814298,\n",
              "  0.21968616262482168,\n",
              "  0.21977007636150037,\n",
              "  0.21993790383485776,\n",
              "  0.22044138625492993,\n",
              "  0.22060921372828732,\n",
              "  0.2207770412016447,\n",
              "  0.2209448686750021,\n",
              "  0.22136443735839556,\n",
              "  0.22262314340857597,\n",
              "  0.22304271209196946,\n",
              "  0.22321053956532685,\n",
              "  0.22337836703868424,\n",
              "  0.22346228077536293,\n",
              "  0.223714021985399,\n",
              "  0.2239657631954351,\n",
              "  0.2251405555089368,\n",
              "  0.22581186540236636,\n",
              "  0.22648317529579592,\n",
              "  0.22732231266258288,\n",
              "  0.22740622639926156,\n",
              "  0.22757405387261895,\n",
              "  0.22765796760929763,\n",
              "  0.22790970881933373,\n",
              "  0.2279936225560124,\n",
              "  0.22883275992279936,\n",
              "  0.22900058739615675,\n",
              "  0.2298397247629437,\n",
              "  0.23109843081312412,\n",
              "  0.2311823445498028,\n",
              "  0.2312662582864815,\n",
              "  0.2314340857598389,\n",
              "  0.23176974070655365,\n",
              "  0.23185365444323236,\n",
              "  0.23218930938994714,\n",
              "  0.2326927918100193,\n",
              "  0.23311236049341277,\n",
              "  0.23319627423009145,\n",
              "  0.23336410170344885,\n",
              "  0.23344801544012755,\n",
              "  0.23395149786019973,\n",
              "  0.2342032390702358,\n",
              "  0.23445498028027187,\n",
              "  0.23462280775362926,\n",
              "  0.23470672149030797,\n",
              "  0.23521020391038014,\n",
              "  0.23537803138373753,\n",
              "  0.2354619451204162,\n",
              "  0.23554585885709492,\n",
              "  0.23571368633045228,\n",
              "  0.235797600067131,\n",
              "  0.23588151380380967,\n",
              "  0.23638499622388184,\n",
              "  0.23646890996056055,\n",
              "  0.23655282369723923,\n",
              "  0.2370563061173114,\n",
              "  0.2371402198539901,\n",
              "  0.23739196106402619,\n",
              "  0.23781152974741965,\n",
              "  0.23806327095745575,\n",
              "  0.2385667533775279,\n",
              "  0.2386506671142066,\n",
              "  0.238818494587564,\n",
              "  0.23932197700763616,\n",
              "  0.2399932869010657,\n",
              "  0.2400772006377444,\n",
              "  0.24116807921456743,\n",
              "  0.2416715616346396,\n",
              "  0.241839389107997,\n",
              "  0.2428463539481413,\n",
              "  0.2430980951581774,\n",
              "  0.24343375010489218,\n",
              "  0.24368549131492825,\n",
              "  0.24393723252496433,\n",
              "  0.24410505999832172,\n",
              "  0.2444407149450365,\n",
              "  0.24494419736510867,\n",
              "  0.24519593857514474,\n",
              "  0.24553159352185952,\n",
              "  0.24578333473189562,\n",
              "  0.2458672484685743,\n",
              "  0.245951162205253,\n",
              "  0.2460350759419317,\n",
              "  0.2462868171519678,\n",
              "  0.24670638583536125,\n",
              "  0.24695812704539732,\n",
              "  0.24704204078207603,\n",
              "  0.24712595451875471,\n",
              "  0.24737769572879081,\n",
              "  0.24779726441218428,\n",
              "  0.24796509188554167,\n",
              "  0.24830074683225645,\n",
              "  0.24855248804229252,\n",
              "  0.24880422925232862,\n",
              "  0.2488881429890073,\n",
              "  0.24930771167240076,\n",
              "  0.24964336661911554,\n",
              "  0.24989510782915164,\n",
              "  0.24997902156583032,\n",
              "  0.250062935302509,\n",
              "  0.2501468490391877,\n",
              "  0.25073424519593857,\n",
              "  0.2508181589326173,\n",
              "  0.2514055550893681,\n",
              "  0.25241251992951247,\n",
              "  0.25266426113954854,\n",
              "  0.25274817487622725,\n",
              "  0.2529999160862633,\n",
              "  0.2532516572962994,\n",
              "  0.2533355710329781,\n",
              "  0.2535873122430142,\n",
              "  0.25383905345305025,\n",
              "  0.25392296718972895,\n",
              "  0.2540907946630864,\n",
              "  0.2545103633464798,\n",
              "  0.2545942770831585,\n",
              "  0.25509775950323066,\n",
              "  0.25518167323990937,\n",
              "  0.2553495007132668,\n",
              "  0.256104724343375,\n",
              "  0.2563564655534111,\n",
              "  0.25669212050012585,\n",
              "  0.25677603423680456,\n",
              "  0.25685994797348327,\n",
              "  0.25719560292019805,\n",
              "  0.2572795166568767,\n",
              "  0.2573634303935554,\n",
              "  0.25753125786691283,\n",
              "  0.2576990853402702,\n",
              "  0.2578669128136276,\n",
              "  0.25828648149702105,\n",
              "  0.2589577913904506,\n",
              "  0.2592095326004867,\n",
              "  0.25954518754720146,\n",
              "  0.26030041117730973,\n",
              "  0.26038432491398844,\n",
              "  0.2604682386506671,\n",
              "  0.2605521523873458,\n",
              "  0.2607199798607032,\n",
              "  0.26105563480741795,\n",
              "  0.26122346228077536,\n",
              "  0.2622304271209197,\n",
              "  0.26239825459427707,\n",
              "  0.26306956448770663,\n",
              "  0.2633213056977427,\n",
              "  0.2634891331711001,\n",
              "  0.2638247881178149,\n",
              "  0.26449609801124446,\n",
              "  0.26516740790467397,\n",
              "  0.26550306285138875,\n",
              "  0.26609045900813966,\n",
              "  0.2664261139548544,\n",
              "  0.26676176890156916,\n",
              "  0.2669295963749266,\n",
              "  0.2670135101116053,\n",
              "  0.26709742384828394,\n",
              "  0.26726525132164136,\n",
              "  0.26768482000503485,\n",
              "  0.2677687337417135,\n",
              "  0.2678526474783922,\n",
              "  0.268188302425107,\n",
              "  0.2682722161617857,\n",
              "  0.26835612989846436,\n",
              "  0.2685239573718218,\n",
              "  0.2686078711085005,\n",
              "  0.2691113535285726,\n",
              "  0.26919526726525134,\n",
              "  0.2695309222119661,\n",
              "  0.2696987496853235,\n",
              "  0.2702022321053957,\n",
              "  0.27037005957875304,\n",
              "  0.2711252832088613,\n",
              "  0.2713770244188974,\n",
              "  0.27171267936561216,\n",
              "  0.2717965931022908,\n",
              "  0.2721322480490056,\n",
              "  0.2723839892590417,\n",
              "  0.27288747167911387,\n",
              "  0.2734748678358647,\n",
              "  0.2738105227825795,\n",
              "  0.2738944365192582,\n",
              "  0.274230091465973,\n",
              "  0.27439791893933035,\n",
              "  0.2749853150960812,\n",
              "  0.2751531425694386,\n",
              "  0.27523705630611733,\n",
              "  0.2758244524628682,\n",
              "  0.27590836619954684,\n",
              "  0.2769153310396912,\n",
              "  0.27758664093312074,\n",
              "  0.2784257782999077,\n",
              "  0.2785936057732651,\n",
              "  0.2786775195099438,\n",
              "  0.2790131744566586,\n",
              "  0.27909708819333723,\n",
              "  0.2795166568767307,\n",
              "  0.2796005706134094,\n",
              "  0.28010405303348157,\n",
              "  0.28035579424351764,\n",
              "  0.28060753545355377,\n",
              "  0.2806914491902324,\n",
              "  0.2812788453469833,\n",
              "  0.28161450029369806,\n",
              "  0.2817823277670555,\n",
              "  0.28211798271377025,\n",
              "  0.28287320634387847,\n",
              "  0.28320886129059325,\n",
              "  0.2834606025006294,\n",
              "  0.28354451623730803,\n",
              "  0.28404799865738023,\n",
              "  0.2842158261307376,\n",
              "  0.28446756734077366,\n",
              "  0.2845514810774524,\n",
              "  0.2846353948141311,\n",
              "  0.28497104976084586,\n",
              "  0.2851388772342032,\n",
              "  0.285474532180918,\n",
              "  0.2858101871276328,\n",
              "  0.28614584207434757,\n",
              "  0.2862297558110263,\n",
              "  0.28639758328438364,\n",
              "  0.2867332382310984,\n",
              "  0.2871528069144919,\n",
              "  0.28799194428127883,\n",
              "  0.28815977175463625,\n",
              "  0.2883275992279936,\n",
              "  0.2884115129646723,\n",
              "  0.2886632541747084,\n",
              "  0.28891499538474447,\n",
              "  0.2890828228581019,\n",
              "  0.29076109759167573,\n",
              "  0.29092892506503315,\n",
              "  0.29101283880171186,\n",
              "  0.29126458001174793,\n",
              "  0.29134849374842664,\n",
              "  0.2917680624318201,\n",
              "  0.29210371737853486,\n",
              "  0.29218763111521356,\n",
              "  0.2923554585885709,\n",
              "  0.29252328606192834,\n",
              "  0.2928589410086431,\n",
              "  0.2931106822186792,\n",
              "  0.29386590584878747,\n",
              "  0.294537215742217,\n",
              "  0.2947889569522531,\n",
              "  0.2951246118989679,\n",
              "  0.29537635310900395,\n",
              "  0.2960476630024335,\n",
              "  0.2962154904757909,\n",
              "  0.2963833179491483,\n",
              "  0.2967189728958631,\n",
              "  0.29688680036922044,\n",
              "  0.2971385415792565,\n",
              "  0.2972224553159352,\n",
              "  0.29772593773600736,\n",
              "  0.2978937652093648,\n",
              "  0.29806159268272214,\n",
              "  0.29831333389275827,\n",
              "  0.29848116136611563,\n",
              "  0.2990685575228665,\n",
              "  0.29932029873290256,\n",
              "  0.2995720399429387,\n",
              "  0.29973986741629605,\n",
              "  0.29982378115297476,\n",
              "  0.2999076948896534,\n",
              "  0.3003272635730469,\n",
              "  0.30057900478308297,\n",
              "  0.3006629185197617,\n",
              "  0.3007468322564404,\n",
              "  0.30141814214986995,\n",
              "  0.3015020558865486,\n",
              "  0.3015859696232273,\n",
              "  0.3018377108332634,\n",
              "  0.3022572795166569,\n",
              "  0.30259293446337165,\n",
              "  0.3036838130401947,\n",
              "  0.30376772677687336,\n",
              "  0.30418729546026685,\n",
              "  0.30427120919694556,\n",
              "  0.3044390366703029,\n",
              "  0.30460686414366034,\n",
              "  0.3049425190903751,\n",
              "  0.3051942603004112,\n",
              "  0.30536208777376855,\n",
              "  0.30586557019384075,\n",
              "  0.3059494839305194,\n",
              "  0.3060333976671981,\n",
              "  0.3061173114038768,\n",
              "  0.30620122514055553,\n",
              "  0.3062851388772342,\n",
              "  0.3063690526139129,\n",
              "  0.3064529663505916,\n",
              "  0.30662079382394897,\n",
              "  ...],\n",
              " {8.39137366786943e-05: 21,\n",
              "  0.0001678274733573886: 7,\n",
              "  0.0002517412100360829: 6,\n",
              "  0.0003356549467147772: 3,\n",
              "  0.0005034824200721658: 8,\n",
              "  0.0005873961567508601: 8,\n",
              "  0.0007552236301082487: 9,\n",
              "  0.000839137366786943: 8,\n",
              "  0.0009230511034656374: 19,\n",
              "  0.0010069648401443317: 7,\n",
              "  0.0012587060501804146: 4,\n",
              "  0.0013426197868591089: 15,\n",
              "  0.0014265335235378032: 3,\n",
              "  0.0015104472602164975: 5,\n",
              "  0.0015943609968951918: 2,\n",
              "  0.001678274733573886: 16,\n",
              "  0.0018461022069312747: 1,\n",
              "  0.001930015943609969: 1,\n",
              "  0.0020139296802886633: 8,\n",
              "  0.002181757153646052: 1,\n",
              "  0.0022656708903247462: 3,\n",
              "  0.002433498363682135: 6,\n",
              "  0.002517412100360829: 1,\n",
              "  0.0026013258370395234: 5,\n",
              "  0.0026852395737182178: 6,\n",
              "  0.002769153310396912: 1,\n",
              "  0.0028530670470756064: 31,\n",
              "  0.003020894520432995: 1,\n",
              "  0.0031048082571116893: 2,\n",
              "  0.0031887219937903836: 1,\n",
              "  0.003272635730469078: 2,\n",
              "  0.0034404632038264665: 20,\n",
              "  0.003524376940505161: 6,\n",
              "  0.003608290677183855: 2,\n",
              "  0.0037761181505412437: 30,\n",
              "  0.003860031887219938: 1,\n",
              "  0.003943945623898632: 3,\n",
              "  0.004195686833934715: 7,\n",
              "  0.0042796005706134095: 5,\n",
              "  0.004363514307292104: 27,\n",
              "  0.004447428043970798: 2,\n",
              "  0.004615255517328187: 2,\n",
              "  0.004699169254006881: 25,\n",
              "  0.004783082990685575: 10,\n",
              "  0.00486699672736427: 10,\n",
              "  0.004950910464042964: 5,\n",
              "  0.005034824200721658: 14,\n",
              "  0.005118737937400353: 58,\n",
              "  0.005202651674079047: 1,\n",
              "  0.005286565410757741: 1,\n",
              "  0.00545439288411513: 12,\n",
              "  0.005706134094151213: 1,\n",
              "  0.005790047830829907: 4,\n",
              "  0.005873961567508601: 38,\n",
              "  0.005957875304187296: 64,\n",
              "  0.00604178904086599: 22,\n",
              "  0.006125702777544684: 2,\n",
              "  0.0062096165142233786: 11,\n",
              "  0.006293530250902073: 7,\n",
              "  0.006880926407652933: 5,\n",
              "  0.006964840144331627: 30,\n",
              "  0.007048753881010322: 2,\n",
              "  0.007132667617689016: 1,\n",
              "  0.00721658135436771: 2,\n",
              "  0.0073004950910464045: 6,\n",
              "  0.007384408827725099: 7,\n",
              "  0.007468322564403793: 1,\n",
              "  0.007552236301082487: 2,\n",
              "  0.007636150037761182: 2,\n",
              "  0.00780397751111857: 4,\n",
              "  0.007971804984475958: 7,\n",
              "  0.008055718721154653: 2,\n",
              "  0.008223546194512042: 14,\n",
              "  0.00839137366786943: 1,\n",
              "  0.008475287404548124: 1,\n",
              "  0.008559201141226819: 1,\n",
              "  0.008643114877905513: 6,\n",
              "  0.008894856087941596: 4,\n",
              "  0.00897876982462029: 2,\n",
              "  0.009146597297977678: 6,\n",
              "  0.009230511034656374: 11,\n",
              "  0.009314424771335067: 29,\n",
              "  0.009398338508013762: 11,\n",
              "  0.00956616598137115: 7,\n",
              "  0.009650079718049844: 4,\n",
              "  0.00973399345472854: 9,\n",
              "  0.009817907191407233: 41,\n",
              "  0.009985734664764621: 2,\n",
              "  0.01015356213812201: 44,\n",
              "  0.010321389611479399: 5,\n",
              "  0.010405303348158094: 11,\n",
              "  0.010489217084836787: 7,\n",
              "  0.010573130821515482: 3,\n",
              "  0.010740958294872871: 20,\n",
              "  0.010824872031551564: 32,\n",
              "  0.01090878576823026: 30,\n",
              "  0.010992699504908953: 20,\n",
              "  0.011076613241587648: 1,\n",
              "  0.011160526978266342: 7,\n",
              "  0.011244440714945037: 9,\n",
              "  0.01132835445162373: 3,\n",
              "  0.011412268188302425: 21,\n",
              "  0.011496181924981119: 50,\n",
              "  0.011580095661659814: 1,\n",
              "  0.011664009398338508: 2,\n",
              "  0.011747923135017203: 3,\n",
              "  0.011999664345053285: 1,\n",
              "  0.01208357808173198: 3,\n",
              "  0.012167491818410673: 13,\n",
              "  0.012251405555089369: 9,\n",
              "  0.012419233028446757: 1,\n",
              "  0.01250314676512545: 1,\n",
              "  0.012587060501804146: 39,\n",
              "  0.012754887975161534: 8,\n",
              "  0.012922715448518923: 1,\n",
              "  0.0132583703952337: 1,\n",
              "  0.013426197868591089: 4,\n",
              "  0.013594025341948477: 2,\n",
              "  0.01367793907862717: 64,\n",
              "  0.013761852815305866: 1,\n",
              "  0.013929680288663255: 2,\n",
              "  0.014013594025341948: 1,\n",
              "  0.014097507762020643: 3,\n",
              "  0.014181421498699337: 53,\n",
              "  0.014349248972056725: 2,\n",
              "  0.01443316270873542: 2,\n",
              "  0.014600990182092809: 28,\n",
              "  0.014684903918771502: 1,\n",
              "  0.014768817655450198: 1,\n",
              "  0.014852731392128891: 1,\n",
              "  0.014936645128807586: 7,\n",
              "  0.015188386338843668: 11,\n",
              "  0.015272300075522363: 50,\n",
              "  0.015356213812201057: 3,\n",
              "  0.015524041285558446: 1,\n",
              "  0.015691868758915834: 3,\n",
              "  0.01577578249559453: 1,\n",
              "  0.015859696232273224: 2,\n",
              "  0.015943609968951916: 8,\n",
              "  0.01602752370563061: 2,\n",
              "  0.016111437442309307: 8,\n",
              "  0.016195351178988: 6,\n",
              "  0.016279264915666693: 27,\n",
              "  0.016447092389024084: 11,\n",
              "  0.01653100612570278: 72,\n",
              "  0.01661491986238147: 17,\n",
              "  0.01678274733573886: 6,\n",
              "  0.016866661072417556: 2,\n",
              "  0.016950574809096248: 97,\n",
              "  0.017034488545774943: 3,\n",
              "  0.017118402282453638: 147,\n",
              "  0.017202316019132333: 1,\n",
              "  0.017286229755811025: 2,\n",
              "  0.017454057229168415: 48,\n",
              "  0.01753797096584711: 13,\n",
              "  0.017621884702525802: 55,\n",
              "  0.017789712175883193: 2,\n",
              "  0.017873625912561888: 1,\n",
              "  0.01795753964924058: 2,\n",
              "  0.018041453385919275: 6,\n",
              "  0.01812536712259797: 4,\n",
              "  0.018209280859276665: 13,\n",
              "  0.018293194595955357: 11,\n",
              "  0.018377108332634052: 10,\n",
              "  0.018461022069312747: 48,\n",
              "  0.018544935805991442: 2,\n",
              "  0.018628849542670134: 2,\n",
              "  0.01871276327934883: 2,\n",
              "  0.018796677016027524: 17,\n",
              "  0.01888059075270622: 5,\n",
              "  0.01896450448938491: 2,\n",
              "  0.019048418226063606: 3,\n",
              "  0.0191323319627423: 13,\n",
              "  0.019216245699420997: 13,\n",
              "  0.01930015943609969: 59,\n",
              "  0.01946798690945708: 35,\n",
              "  0.019635814382814466: 9,\n",
              "  0.01971972811949316: 1,\n",
              "  0.01988755559285055: 69,\n",
              "  0.019971469329529243: 3,\n",
              "  0.020055383066207938: 23,\n",
              "  0.020139296802886633: 2,\n",
              "  0.02030712427624402: 2,\n",
              "  0.020391038012922715: 51,\n",
              "  0.02047495174960141: 3,\n",
              "  0.020558865486280106: 5,\n",
              "  0.020642779222958797: 76,\n",
              "  0.020810606696316188: 1,\n",
              "  0.020894520432994883: 14,\n",
              "  0.020978434169673574: 16,\n",
              "  0.02106234790635227: 39,\n",
              "  0.021146261643030965: 1,\n",
              "  0.02123017537970966: 9,\n",
              "  0.021398002853067047: 2,\n",
              "  0.021565830326424437: 6,\n",
              "  0.02164974406310313: 2,\n",
              "  0.021733657799781824: 52,\n",
              "  0.021901485273139214: 5,\n",
              "  0.0220693127464966: 4,\n",
              "  0.022153226483175296: 19,\n",
              "  0.02223714021985399: 11,\n",
              "  0.022321053956532683: 12,\n",
              "  0.022488881429890074: 4,\n",
              "  0.02257279516656877: 14,\n",
              "  0.022740622639926156: 1,\n",
              "  0.02282453637660485: 6,\n",
              "  0.022992363849962238: 14,\n",
              "  0.023076277586640933: 9,\n",
              "  0.023160191323319628: 4,\n",
              "  0.023244105059998323: 118,\n",
              "  0.02341193253335571: 10,\n",
              "  0.0235797600067131: 22,\n",
              "  0.023663673743391792: 4,\n",
              "  0.023747587480070487: 40,\n",
              "  0.023831501216749183: 61,\n",
              "  0.023915414953427878: 11,\n",
              "  0.02399932869010657: 5,\n",
              "  0.024083242426785265: 39,\n",
              "  0.02416715616346396: 3,\n",
              "  0.024251069900142655: 7,\n",
              "  0.024334983636821347: 2,\n",
              "  0.024418897373500042: 1,\n",
              "  0.024502811110178737: 2,\n",
              "  0.024586724846857432: 28,\n",
              "  0.024670638583536124: 5,\n",
              "  0.024838466056893514: 18,\n",
              "  0.0250062935302509: 3,\n",
              "  0.025090207266929596: 21,\n",
              "  0.02517412100360829: 3,\n",
              "  0.025258034740286987: 11,\n",
              "  0.02534194847696568: 1,\n",
              "  0.025425862213644373: 13,\n",
              "  0.02550977595032307: 10,\n",
              "  0.025593689687001764: 3,\n",
              "  0.025677603423680456: 71,\n",
              "  0.025845430897037846: 15,\n",
              "  0.02592934463371654: 1,\n",
              "  0.026097172107073928: 2,\n",
              "  0.026181085843752623: 2,\n",
              "  0.02626499958043132: 1,\n",
              "  0.02634891331711001: 4,\n",
              "  0.026600654527146095: 16,\n",
              "  0.026684568263824787: 13,\n",
              "  0.026768482000503482: 7,\n",
              "  0.026852395737182178: 18,\n",
              "  0.026936309473860873: 9,\n",
              "  0.027020223210539564: 15,\n",
              "  0.02710413694721826: 5,\n",
              "  0.02727196442057565: 19,\n",
              "  0.02735587815725434: 1,\n",
              "  0.027523705630611732: 3,\n",
              "  0.027607619367290427: 8,\n",
              "  0.02769153310396912: 1,\n",
              "  0.027775446840647814: 2,\n",
              "  0.02785936057732651: 1,\n",
              "  0.02811110178736259: 1,\n",
              "  0.02827892926071998: 8,\n",
              "  0.028362842997398673: 35,\n",
              "  0.02844675673407737: 39,\n",
              "  0.028614584207434755: 12,\n",
              "  0.02869849794411345: 26,\n",
              "  0.028782411680792146: 2,\n",
              "  0.02886632541747084: 24,\n",
              "  0.029034152890828228: 16,\n",
              "  0.029201980364185618: 10,\n",
              "  0.02928589410086431: 9,\n",
              "  0.029369807837543005: 15,\n",
              "  0.029537635310900395: 16,\n",
              "  0.029621549047579087: 12,\n",
              "  0.029705462784257782: 31,\n",
              "  0.029789376520936477: 10,\n",
              "  0.029957203994293864: 11,\n",
              "  0.03004111773097256: 4,\n",
              "  0.030125031467651255: 3,\n",
              "  0.03020894520432995: 3,\n",
              "  0.03029285894100864: 2,\n",
              "  0.030544600151044727: 3,\n",
              "  0.03062851388772342: 8,\n",
              "  0.030712427624402114: 10,\n",
              "  0.03079634136108081: 8,\n",
              "  0.030880255097759504: 3,\n",
              "  0.030964168834438196: 83,\n",
              "  0.03104808257111689: 12,\n",
              "  0.031131996307795586: 25,\n",
              "  0.03121591004447428: 21,\n",
              "  0.03138373751783167: 7,\n",
              "  0.03146765125451036: 5,\n",
              "  0.03155156499118906: 15,\n",
              "  0.031635478727867754: 3,\n",
              "  0.03171939246454645: 93,\n",
              "  0.03180330620122514: 14,\n",
              "  0.03188721993790383: 2,\n",
              "  0.03205504741126122: 1,\n",
              "  0.03213896114793992: 1,\n",
              "  0.03222287488461861: 3,\n",
              "  0.032390702357976: 64,\n",
              "  0.03247461609465469: 63,\n",
              "  0.03255852983133339: 62,\n",
              "  0.03272635730469078: 7,\n",
              "  0.03281027104136947: 16,\n",
              "  0.03289418477804817: 16,\n",
              "  0.03306201225140556: 4,\n",
              "  0.03322983972476294: 46,\n",
              "  0.033313753461441636: 6,\n",
              "  0.03339766719812033: 1,\n",
              "  0.03373332214483511: 1,\n",
              "  0.0338172358815138: 12,\n",
              "  0.033901149618192496: 6,\n",
              "  0.03398506335487119: 10,\n",
              "  0.034068977091549886: 26,\n",
              "  0.034236804564907276: 13,\n",
              "  0.03432071830158597: 1,\n",
              "  0.03457245951162205: 1,\n",
              "  0.034656373248300745: 1,\n",
              "  0.03474028698497944: 3,\n",
              "  0.034824200721658136: 1,\n",
              "  0.03490811445833683: 6,\n",
              "  0.034992028195015526: 5,\n",
              "  0.03507594193169422: 20,\n",
              "  0.03515985566837291: 2,\n",
              "  0.0353276831417303: 2,\n",
              "  0.035411596878408995: 3,\n",
              "  0.03549551061508769: 2,\n",
              "  0.035579424351766385: 1,\n",
              "  0.035747251825123776: 2,\n",
              "  0.035831165561802464: 1,\n",
              "  0.03591507929848116: 20,\n",
              "  0.03608290677183855: 35,\n",
              "  0.036166820508517245: 3,\n",
              "  0.03625073424519594: 3,\n",
              "  0.036334647981874635: 1,\n",
              "  0.03641856171855333: 23,\n",
              "  0.03650247545523202: 5,\n",
              "  0.03658638919191071: 40,\n",
              "  0.036922044138625494: 4,\n",
              "  0.03700595787530419: 1,\n",
              "  0.037089871611982884: 122,\n",
              "  0.03717378534866157: 21,\n",
              "  0.03725769908534027: 1,\n",
              "  0.03734161282201896: 8,\n",
              "  0.03742552655869766: 5,\n",
              "  0.03750944029537635: 7,\n",
              "  0.03759335403205505: 1,\n",
              "  0.037677267768733744: 8,\n",
              "  0.03776118150541244: 10,\n",
              "  0.03784509524209113: 16,\n",
              "  0.03792900897876982: 1,\n",
              "  0.03801292271544852: 2,\n",
              "  0.03809683645212721: 10,\n",
              "  0.03818075018880591: 1,\n",
              "  0.0382646639254846: 35,\n",
              "  0.0383485776621633: 1,\n",
              "  0.03843249139884199: 34,\n",
              "  0.03860031887219938: 10,\n",
              "  0.03868423260887807: 6,\n",
              "  0.03901988755559285: 1,\n",
              "  0.03910380129227155: 2,\n",
              "  0.039187715028950236: 6,\n",
              "  0.03927162876562893: 23,\n",
              "  0.039355542502307626: 5,\n",
              "  0.03943945623898632: 2,\n",
              "  0.03952336997566502: 15,\n",
              "  0.03960728371234371: 2,\n",
              "  0.03969119744902241: 3,\n",
              "  0.0397751111857011: 1,\n",
              "  0.039942938659058486: 3,\n",
              "  0.04002685239573718: 30,\n",
              "  0.040110766132415876: 5,\n",
              "  0.040278593605773266: 1,\n",
              "  0.04036250734245196: 2,\n",
              "  0.04044642107913066: 4,\n",
              "  0.040530334815809345: 19,\n",
              "  0.04061424855248804: 52,\n",
              "  0.04078207602584543: 3,\n",
              "  0.040865989762524126: 1,\n",
              "  0.04094990349920282: 1,\n",
              "  0.041033817235881516: 12,\n",
              "  0.04111773097256021: 11,\n",
              "  0.0412016447092389: 11,\n",
              "  0.04136947218259629: 16,\n",
              "  0.04153729965595368: 2,\n",
              "  0.041621213392632375: 1,\n",
              "  0.04170512712931107: 8,\n",
              "  0.041872954602668454: 29,\n",
              "  0.04195686833934715: 7,\n",
              "  0.042040782076025844: 1,\n",
              "  0.042376437022740625: 38,\n",
              "  0.04246035075941932: 1,\n",
              "  0.04254426449609801: 15,\n",
              "  0.0427120919694554: 116,\n",
              "  0.04287991944281279: 5,\n",
              "  0.042963833179491484: 2,\n",
              "  0.04304774691617018: 1,\n",
              "  0.043131660652848874: 13,\n",
              "  0.04321557438952756: 1,\n",
              "  0.04329948812620626: 4,\n",
              "  0.04338340186288495: 4,\n",
              "  0.04355122933624234: 1,\n",
              "  0.043719056809599734: 2,\n",
              "  0.04380297054627843: 2,\n",
              "  0.04388688428295712: 13,\n",
              "  0.04397079801963581: 1,\n",
              "  0.04405471175631451: 18,\n",
              "  0.0441386254929932: 15,\n",
              "  0.0442225392296719: 6,\n",
              "  0.04430645296635059: 8,\n",
              "  0.04439036670302929: 13,\n",
              "  0.04447428043970798: 2,\n",
              "  0.04455819417638667: 8,\n",
              "  0.04464210791306537: 4,\n",
              "  0.04472602164974406: 2,\n",
              "  0.04480993538642276: 10,\n",
              "  0.04489384912310145: 1,\n",
              "  0.04497776285978015: 1,\n",
              "  0.04506167659645884: 79,\n",
              "  0.04514559033313754: 44,\n",
              "  0.045229504069816226: 8,\n",
              "  0.04531341780649492: 60,\n",
              "  0.045397331543173616: 41,\n",
              "  0.04548124527985231: 11,\n",
              "  0.04556515901653101: 2,\n",
              "  0.04581690022656709: 6,\n",
              "  0.04590081396324578: 1,\n",
              "  0.045984727699924476: 2,\n",
              "  0.04623646890996056: 8,\n",
              "  0.046320382646639256: 106,\n",
              "  0.04648821011999665: 24,\n",
              "  0.046572123856675335: 59,\n",
              "  0.04665603759335403: 36,\n",
              "  0.046739951330032725: 1,\n",
              "  0.04682386506671142: 3,\n",
              "  0.04699169254006881: 1,\n",
              "  0.0471595200134262: 3,\n",
              "  0.04724343375010489: 26,\n",
              "  0.047327347486783584: 17,\n",
              "  0.04741126122346228: 10,\n",
              "  0.047495174960140975: 2,\n",
              "  0.04757908869681967: 15,\n",
              "  0.047663002433498365: 2,\n",
              "  0.04774691617017706: 52,\n",
              "  0.047830829906855755: 14,\n",
              "  0.047914743643534444: 44,\n",
              "  0.04799865738021314: 2,\n",
              "  0.04833431232692792: 4,\n",
              "  0.048418226063606615: 23,\n",
              "  0.04866996727364269: 19,\n",
              "  0.04875388101032139: 12,\n",
              "  0.048837794747000084: 5,\n",
              "  0.049005622220357474: 1,\n",
              "  0.049173449693714864: 1,\n",
              "  0.04942519090375094: 4,\n",
              "  0.04959301837710833: 28,\n",
              "  0.049760845850465724: 85,\n",
              "  0.04984475958714442: 4,\n",
              "  0.04992867332382311: 40,\n",
              "  0.0500125870605018: 2,\n",
              "  0.0500965007971805: 29,\n",
              "  0.05018041453385919: 1,\n",
              "  0.05026432827053789: 2,\n",
              "  0.05034824200721658: 5,\n",
              "  0.05043215574389528: 14,\n",
              "  0.05051606948057397: 1,\n",
              "  0.05059998321725266: 52,\n",
              "  0.05068389695393136: 11,\n",
              "  0.05076781069061005: 13,\n",
              "  0.05085172442728875: 1,\n",
              "  0.05093563816396744: 96,\n",
              "  0.05110346563732483: 29,\n",
              "  0.05118737937400353: 9,\n",
              "  0.051271293110682216: 12,\n",
              "  0.05135520684736091: 7,\n",
              "  0.051439120584039606: 1,\n",
              "  0.05169086179407569: 3,\n",
              "  0.05177477553075439: 3,\n",
              "  0.05185868926743308: 1,\n",
              "  0.05194260300411177: 2,\n",
              "  0.05211043047746916: 8,\n",
              "  0.052194344214147856: 3,\n",
              "  0.052362171687505246: 2,\n",
              "  0.05244608542418394: 28,\n",
              "  0.05252999916086264: 6,\n",
              "  0.052613912897541325: 70,\n",
              "  0.05269782663422002: 5,\n",
              "  0.052781740370898715: 2,\n",
              "  0.05286565410757741: 12,\n",
              "  0.052949567844256105: 3,\n",
              "  0.05328522279097088: 4,\n",
              "  0.05345305026432827: 1,\n",
              "  0.05362087773768566: 3,\n",
              "  0.053704791474364355: 65,\n",
              "  0.05378870521104305: 1,\n",
              "  0.053872618947721745: 3,\n",
              "  0.053956532684400434: 2,\n",
              "  0.05404044642107913: 3,\n",
              "  0.05420827389443652: 41,\n",
              "  0.05437610136779391: 29,\n",
              "  0.054460015104472605: 8,\n",
              "  0.0545439288411513: 15,\n",
              "  0.05462784257782999: 3,\n",
              "  0.05471175631450868: 99,\n",
              "  0.05479567005118738: 1,\n",
              "  0.055047411261223464: 1,\n",
              "  0.05513132499790216: 8,\n",
              "  0.055215238734580854: 6,\n",
              "  0.05529915247125954: 5,\n",
              "  0.05538306620793824: 5,\n",
              "  0.05546697994461693: 12,\n",
              "  0.05555089368129563: 3,\n",
              "  0.05563480741797432: 2,\n",
              "  0.05571872115465302: 1,\n",
              "  0.055802634891331714: 8,\n",
              "  0.05588654862801041: 2,\n",
              "  0.0559704623646891: 1,\n",
              "  0.05605437610136779: 1,\n",
              "  0.05613828983804649: 5,\n",
              "  0.05639003104808257: 7,\n",
              "  0.05655785852143996: 5,\n",
              "  0.05664177225811865: 27,\n",
              "  0.05680959973147604: 73,\n",
              "  0.05697742720483343: 1,\n",
              "  0.05706134094151213: 12,\n",
              "  0.05714525467819082: 48,\n",
              "  0.05722916841486951: 1,\n",
              "  0.057313082151548206: 1,\n",
              "  0.057480909624905596: 1,\n",
              "  0.05764873709826299: 3,\n",
              "  0.05773265083494168: 1,\n",
              "  0.05781656457162038: 2,\n",
              "  0.05798439204497776: 55,\n",
              "  0.058068305781656455: 1,\n",
              "  0.05815221951833515: 1,\n",
              "  0.058236133255013846: 1,\n",
              "  0.05832004699169254: 5,\n",
              "  0.058403960728371236: 2,\n",
              "  0.05848787446504993: 4,\n",
              "  0.05857178820172862: 18,\n",
              "  0.05873961567508601: 38,\n",
              "  0.058823529411764705: 2,\n",
              "  0.0589074431484434: 1,\n",
              "  0.058991356885122095: 1,\n",
              "  0.05907527062180079: 7,\n",
              "  0.059578753041872955: 17,\n",
              "  0.05966266677855165: 15,\n",
              "  0.05983049425190904: 74,\n",
              "  0.059998321725266424: 13,\n",
              "  0.06008223546194512: 4,\n",
              "  0.060166149198623814: 5,\n",
              "  0.06025006293530251: 1,\n",
              "  0.060333976671981204: 2,\n",
              "  0.060501804145338595: 5,\n",
              "  0.06058571788201728: 20,\n",
              "  0.06066963161869598: 3,\n",
              "  0.06075354535537467: 2,\n",
              "  0.06083745909205337: 26,\n",
              "  0.060921372828732064: 126,\n",
              "  0.06117311403876815: 2,\n",
              "  0.06125702777544684: 2,\n",
              "  0.06134094151212553: 3,\n",
              "  0.06142485524880423: 5,\n",
              "  0.06150876898548292: 4,\n",
              "  0.06159268272216162: 1,\n",
              "  0.06176051019551901: 58,\n",
              "  0.061844423932197704: 26,\n",
              "  0.06192833766887639: 4,\n",
              "  0.06201225140555509: 16,\n",
              "  0.06209616514223378: 2,\n",
              "  0.06218007887891248: 58,\n",
              "  0.06234790635226987: 61,\n",
              "  0.06243182008894856: 1,\n",
              "  0.06276747503566334: 12,\n",
              "  0.06285138877234203: 1,\n",
              "  0.06293530250902073: 33,\n",
              "  0.06301921624569942: 4,\n",
              "  0.06310312998237812: 27,\n",
              "  0.06318704371905681: 1,\n",
              "  0.06327095745573551: 2,\n",
              "  0.0633548711924142: 28,\n",
              "  0.0635226986657716: 8,\n",
              "  0.06369052613912897: 17,\n",
              "  0.06377443987580766: 120,\n",
              "  0.06402618108584375: 2,\n",
              "  0.06411009482252245: 3,\n",
              "  0.06419400855920114: 1,\n",
              "  0.06427792229587984: 9,\n",
              "  0.06436183603255853: 2,\n",
              "  0.06444574976923723: 10,\n",
              "  0.06452966350591592: 2,\n",
              "  0.06469749097927331: 27,\n",
              "  0.064781404715952: 1,\n",
              "  0.0648653184526307: 22,\n",
              "  0.06494923218930938: 23,\n",
              "  0.06511705966266677: 3,\n",
              "  0.06520097339934547: 2,\n",
              "  0.06528488713602416: 76,\n",
              "  0.06536880087270286: 6,\n",
              "  0.06545271460938155: 12,\n",
              "  0.06562054208273894: 52,\n",
              "  0.06570445581941764: 2,\n",
              "  0.06578836955609633: 24,\n",
              "  0.06595619702945373: 1,\n",
              "  0.06604011076613242: 23,\n",
              "  0.06620793823948981: 5,\n",
              "  0.06645967944952588: 4,\n",
              "  0.06654359318620458: 11,\n",
              "  0.06662750692288327: 4,\n",
              "  0.06671142065956197: 5,\n",
              "  0.06679533439624066: 5,\n",
              "  0.06696316186959805: 3,\n",
              "  0.06704707560627675: 6,\n",
              "  0.06721490307963414: 163,\n",
              "  0.06729881681631283: 1,\n",
              "  0.06746664428967022: 21,\n",
              "  0.06755055802634892: 8,\n",
              "  0.0676344717630276: 1,\n",
              "  0.0677183854997063: 30,\n",
              "  0.06797012670974238: 5,\n",
              "  0.06813795418309977: 1,\n",
              "  0.06830578165645716: 71,\n",
              "  0.06838969539313586: 5,\n",
              "  0.06864143660317194: 19,\n",
              "  0.06872535033985064: 1,\n",
              "  0.06880926407652933: 77,\n",
              "  0.06889317781320803: 2,\n",
              "  0.06897709154988671: 8,\n",
              "  0.0690610052865654: 2,\n",
              "  0.0692288327599228: 15,\n",
              "  0.06931274649660149: 43,\n",
              "  0.06948057396995888: 21,\n",
              "  0.06956448770663758: 100,\n",
              "  0.06964840144331627: 2,\n",
              "  0.06973231517999497: 4,\n",
              "  0.06981622891667366: 1,\n",
              "  0.06990014265335236: 63,\n",
              "  0.06998405639003105: 25,\n",
              "  0.07006797012670975: 23,\n",
              "  0.07023579760006714: 12,\n",
              "  0.07031971133674582: 5,\n",
              "  0.0705714525467819: 4,\n",
              "  0.07099102123017538: 105,\n",
              "  0.07107493496685408: 1,\n",
              "  0.07115884870353277: 60,\n",
              "  0.07124276244021147: 27,\n",
              "  0.07141058991356886: 1,\n",
              "  0.07149450365024755: 2,\n",
              "  0.07157841738692625: 50,\n",
              "  0.07166233112360493: 76,\n",
              "  0.07174624486028362: 8,\n",
              "  0.07183015859696232: 4,\n",
              "  0.07191407233364101: 1,\n",
              "  0.07199798607031971: 3,\n",
              "  0.0721658135436771: 51,\n",
              "  0.0722497272803558: 29,\n",
              "  0.07233364101703449: 1,\n",
              "  0.07241755475371318: 4,\n",
              "  0.07250146849039188: 1,\n",
              "  0.07275320970042796: 21,\n",
              "  0.07283712343710666: 3,\n",
              "  0.07292103717378536: 14,\n",
              "  0.07308886464714273: 28,\n",
              "  0.07317277838382143: 19,\n",
              "  0.07325669212050012: 3,\n",
              "  0.07334060585717882: 25,\n",
              "  0.07342451959385751: 1,\n",
              "  0.07350843333053621: 2,\n",
              "  0.0735923470672149: 52,\n",
              "  0.0736762608038936: 36,\n",
              "  0.07376017454057229: 3,\n",
              "  0.07384408827725099: 30,\n",
              "  0.07392800201392968: 2,\n",
              "  0.07409582948728707: 43,\n",
              "  0.07417974322396577: 7,\n",
              "  0.07426365696064446: 13,\n",
              "  0.07434757069732315: 1,\n",
              "  0.07443148443400184: 38,\n",
              "  0.07451539817068054: 90,\n",
              "  0.07468322564403793: 10,\n",
              "  0.07476713938071662: 17,\n",
              "  0.07485105311739532: 51,\n",
              "  0.07493496685407401: 22,\n",
              "  0.0750188805907527: 5,\n",
              "  0.0751027943274314: 3,\n",
              "  0.0751867080641101: 1,\n",
              "  0.07527062180078879: 67,\n",
              "  0.07535453553746749: 1,\n",
              "  0.07543844927414618: 1,\n",
              "  0.07552236301082488: 161,\n",
              "  0.07560627674750357: 1,\n",
              "  0.07569019048418225: 3,\n",
              "  0.07577410422086095: 23,\n",
              "  0.07594193169421834: 1,\n",
              "  0.07602584543089703: 3,\n",
              "  0.07610975916757573: 4,\n",
              "  0.07619367290425443: 27,\n",
              "  0.07627758664093312: 1,\n",
              "  0.07644541411429051: 3,\n",
              "  0.0765293278509692: 30,\n",
              "  0.0766132415876479: 23,\n",
              "  0.0766971553243266: 19,\n",
              "  0.07678106906100529: 2,\n",
              "  0.07694889653436268: 4,\n",
              "  0.07703281027104136: 5,\n",
              "  0.07711672400772006: 65,\n",
              "  0.07720063774439875: 20,\n",
              "  0.07728455148107745: 6,\n",
              "  0.07736846521775614: 26,\n",
              "  0.07745237895443484: 1,\n",
              "  0.07753629269111353: 64,\n",
              "  0.07762020642779223: 29,\n",
              "  0.07770412016447092: 21,\n",
              "  0.07778803390114962: 12,\n",
              "  0.07787194763782831: 3,\n",
              "  0.0780397751111857: 23,\n",
              "  0.0781236888478644: 3,\n",
              "  0.0782076025845431: 5,\n",
              "  0.07837543005790047: 35,\n",
              "  0.07845934379457917: 5,\n",
              "  0.07854325753125786: 1,\n",
              "  0.07862717126793656: 1,\n",
              "  0.07871108500461525: 32,\n",
              "  0.07887891247797264: 10,\n",
              "  0.07896282621465134: 4,\n",
              "  0.07904673995133003: 6,\n",
              "  0.07913065368800873: 2,\n",
              "  0.07921456742468742: 2,\n",
              "  0.07929848116136612: 5,\n",
              "  0.07938239489804481: 5,\n",
              "  0.0795502223714022: 1,\n",
              "  0.0796341361080809: 61,\n",
              "  0.07971804984475958: 6,\n",
              "  0.07980196358143828: 1,\n",
              "  0.07988587731811697: 1,\n",
              "  0.07996979105479567: 6,\n",
              "  0.08013761852815306: 43,\n",
              "  0.08022153226483175: 23,\n",
              "  0.08038935973818914: 10,\n",
              "  0.08047327347486784: 57,\n",
              "  0.08055718721154653: 4,\n",
              "  0.08064110094822523: 33,\n",
              "  0.08072501468490392: 5,\n",
              "  0.08080892842158262: 9,\n",
              "  0.08089284215826131: 8,\n",
              "  0.08097675589494001: 12,\n",
              "  0.08106066963161869: 4,\n",
              "  0.08114458336829738: 1,\n",
              "  0.08122849710497608: 13,\n",
              "  0.08131241084165478: 20,\n",
              "  0.08139632457833347: 5,\n",
              "  0.08148023831501217: 1,\n",
              "  0.08156415205169086: 1,\n",
              "  0.08164806578836956: 32,\n",
              "  0.08173197952504825: 1,\n",
              "  0.08181589326172695: 1,\n",
              "  0.08189980699840564: 21,\n",
              "  0.08198372073508434: 43,\n",
              "  0.08206763447176303: 8,\n",
              "  0.08215154820844173: 27,\n",
              "  0.08223546194512042: 2,\n",
              "  0.08231937568179912: 6,\n",
              "  0.0824032894184778: 1,\n",
              "  0.0824872031551565: 8,\n",
              "  0.08257111689183519: 1,\n",
              "  0.08265503062851388: 53,\n",
              "  0.08273894436519258: 3,\n",
              "  0.08282285810187127: 1,\n",
              "  0.08290677183854997: 4,\n",
              "  0.08307459931190736: 14,\n",
              "  0.08315851304858606: 1,\n",
              "  0.08324242678526475: 35,\n",
              "  0.08332634052194345: 7,\n",
              "  0.08341025425862214: 2,\n",
              "  0.08349416799530084: 10,\n",
              "  0.08357808173197953: 4,\n",
              "  0.08374590920533691: 1,\n",
              "  0.0838298229420156: 2,\n",
              "  0.0839137366786943: 6,\n",
              "  0.08399765041537299: 9,\n",
              "  0.08408156415205169: 4,\n",
              "  0.08424939162540908: 66,\n",
              "  0.08433330536208777: 15,\n",
              "  0.08441721909876647: 6,\n",
              "  0.08450113283544516: 4,\n",
              "  0.08458504657212386: 7,\n",
              "  0.08466896030880255: 2,\n",
              "  0.08475287404548125: 6,\n",
              "  0.08483678778215994: 2,\n",
              "  0.08492070151883864: 1,\n",
              "  0.08500461525551734: 3,\n",
              "  0.08508852899219602: 1,\n",
              "  0.08517244272887471: 8,\n",
              "  0.0852563564655534: 1,\n",
              "  0.0853402702022321: 16,\n",
              "  0.0854241839389108: 11,\n",
              "  0.08559201141226819: 29,\n",
              "  0.08567592514894688: 1,\n",
              "  0.08575983888562558: 30,\n",
              "  0.08584375262230427: 3,\n",
              "  0.08592766635898297: 10,\n",
              "  0.08601158009566166: 14,\n",
              "  0.08617940756901905: 1,\n",
              "  0.08626332130569775: 21,\n",
              "  0.08634723504237643: 2,\n",
              "  0.08651506251573382: 1,\n",
              "  0.08659897625241252: 21,\n",
              "  0.08668288998909121: 1,\n",
              "  0.0867668037257699: 1,\n",
              "  0.08701854493580599: 1,\n",
              "  0.08718637240916338: 5,\n",
              "  0.08727028614584208: 8,\n",
              "  0.08735419988252077: 2,\n",
              "  0.08752202735587816: 38,\n",
              "  0.08760594109255686: 2,\n",
              "  0.08768985482923554: 1,\n",
              "  0.08777376856591423: 2,\n",
              "  0.08785768230259293: 5,\n",
              "  0.08794159603927162: 2,\n",
              "  0.08802550977595032: 7,\n",
              "  0.08810942351262901: 1,\n",
              "  0.08819333724930771: 6,\n",
              "  0.0882772509859864: 13,\n",
              "  0.0884450784593438: 5,\n",
              "  0.08869681966937988: 2,\n",
              "  0.08878073340605858: 5,\n",
              "  0.08886464714273727: 1,\n",
              "  0.08903247461609465: 1,\n",
              "  0.08911638835277334: 23,\n",
              "  0.08920030208945204: 10,\n",
              "  0.08928421582613073: 1,\n",
              "  0.08936812956280943: 6,\n",
              "  0.08945204329948812: 1,\n",
              "  0.08953595703616682: 1,\n",
              "  0.08961987077284551: 1,\n",
              "  0.08970378450952421: 1,\n",
              "  0.0897876982462029: 4,\n",
              "  0.0898716119828816: 1,\n",
              "  0.0899555257195603: 6,\n",
              "  0.09003943945623899: 6,\n",
              "  0.09012335319291769: 1,\n",
              "  0.09020726692959638: 1,\n",
              "  0.09037509440295376: 4,\n",
              "  0.09045900813963245: 50,\n",
              "  0.09054292187631115: 1,\n",
              "  0.09062683561298984: 8,\n",
              "  0.09071074934966854: 5,\n",
              "  0.09079466308634723: 24,\n",
              "  0.09087857682302593: 32,\n",
              "  0.09096249055970462: 1,\n",
              "  0.09104640429638332: 2,\n",
              "  0.09121423176974071: 1,\n",
              "  0.0912981455064194: 37,\n",
              "  0.0913820592430981: 2,\n",
              "  0.09154988671645549: 12,\n",
              "  0.09163380045313418: 3,\n",
              "  0.09171771418981287: 37,\n",
              "  0.09180162792649156: 47,\n",
              "  0.09188554166317026: 8,\n",
              "  0.09196945539984895: 3,\n",
              "  0.09205336913652765: 7,\n",
              "  0.09222119660988504: 32,\n",
              "  0.09230511034656373: 1,\n",
              "  0.09238902408324243: 3,\n",
              "  0.09255685155659982: 16,\n",
              "  0.09264076529327851: 1,\n",
              "  0.09272467902995721: 43,\n",
              "  0.0929764202399933: 47,\n",
              "  0.09306033397667197: 1,\n",
              "  0.09314424771335067: 3,\n",
              "  0.09322816145002936: 82,\n",
              "  0.09331207518670806: 110,\n",
              "  0.09339598892338676: 32,\n",
              "  0.09347990266006545: 7,\n",
              "  0.09356381639674415: 4,\n",
              "  0.09364773013342284: 4,\n",
              "  0.09373164387010154: 94,\n",
              "  0.09381555760678023: 5,\n",
              "  0.09389947134345893: 20,\n",
              "  0.0942351262901737: 25,\n",
              "  0.0943190400268524: 42,\n",
              "  0.09440295376353108: 5,\n",
              "  0.09448686750020978: 8,\n",
              "  0.09473860871024586: 2,\n",
              "  0.09490643618360325: 1,\n",
              "  0.09499034992028195: 8,\n",
              "  0.09507426365696064: 4,\n",
              "  0.09515817739363934: 33,\n",
              "  0.09524209113031804: 1,\n",
              "  0.09532600486699673: 2,\n",
              "  0.09540991860367543: 3,\n",
              "  0.09557774607703282: 3,\n",
              "  0.09574557355039019: 15,\n",
              "  0.09591340102374758: 27,\n",
              "  0.09599731476042628: 2,\n",
              "  0.09608122849710497: 1,\n",
              "  0.09616514223378367: 27,\n",
              "  0.09624905597046236: 2,\n",
              "  0.09633296970714106: 46,\n",
              "  0.09650079718049845: 33,\n",
              "  0.09658471091717714: 1,\n",
              "  0.09666862465385584: 4,\n",
              "  0.09675253839053453: 1,\n",
              "  0.09683645212721323: 4,\n",
              "  0.09700427960057062: 54,\n",
              "  0.0970881933372493: 3,\n",
              "  0.097172107073928: 17,\n",
              "  0.09725602081060669: 19,\n",
              "  0.09733993454728539: 9,\n",
              "  0.09742384828396408: 2,\n",
              "  0.09750776202064278: 1,\n",
              "  0.09759167575732147: 7,\n",
              "  0.09767558949400017: 1,\n",
              "  0.09784341696735756: 27,\n",
              "  0.09809515817739364: 4,\n",
              "  0.09817907191407234: 9,\n",
              "  0.09834689938742973: 38,\n",
              "  0.09843081312410841: 36,\n",
              "  0.0985147268607871: 1,\n",
              "  0.0985986405974658: 2,\n",
              "  0.09876646807082319: 4,\n",
              "  0.09885038180750189: 1,\n",
              "  0.09901820928085928: 7,\n",
              "  0.09910212301753797: 2,\n",
              "  0.09918603675421667: 2,\n",
              "  0.09926995049089536: 1,\n",
              "  0.09935386422757406: 2,\n",
              "  0.09943777796425275: 71,\n",
              "  0.09952169170093145: 2,\n",
              "  0.09960560543761014: 1,\n",
              "  0.09968951917428884: 43,\n",
              "  0.09977343291096752: 1,\n",
              "  0.09994126038432491: 32,\n",
              "  0.1000251741210036: 12,\n",
              "  0.1001090878576823: 8,\n",
              "  0.100193001594361: 1,\n",
              "  0.10044474280439708: 1,\n",
              "  0.10052865654107578: 2,\n",
              "  0.10061257027775447: 16,\n",
              "  0.10069648401443317: 1,\n",
              "  0.10078039775111186: 2,\n",
              "  0.10086431148779056: 2,\n",
              "  0.10094822522446925: 1,\n",
              "  0.10103213896114795: 16,\n",
              "  0.10111605269782663: 6,\n",
              "  0.10119996643450532: 1,\n",
              "  0.10128388017118402: 11,\n",
              "  0.10145170764454141: 8,\n",
              "  0.1015356213812201: 3,\n",
              "  0.1016195351178988: 9,\n",
              "  0.10178736259125619: 1,\n",
              "  0.10187127632793488: 2,\n",
              "  0.10195519006461358: 1,\n",
              "  0.10203910380129227: 22,\n",
              "  0.10220693127464966: 18,\n",
              "  0.10229084501132836: 4,\n",
              "  0.10237475874800706: 46,\n",
              "  0.10245867248468574: 1,\n",
              "  0.10271041369472182: 1,\n",
              "  0.10279432743140052: 4,\n",
              "  0.10287824116807921: 62,\n",
              "  0.10296215490475791: 12,\n",
              "  0.1030460686414366: 2,\n",
              "  0.1031299823781153: 1,\n",
              "  0.10329780985147269: 23,\n",
              "  0.10338172358815138: 13,\n",
              "  0.10346563732483008: 1,\n",
              "  0.10354955106150877: 6,\n",
              "  0.10371737853486616: 23,\n",
              "  0.10380129227154485: 1,\n",
              "  0.10388520600822354: 8,\n",
              "  0.10413694721825963: 3,\n",
              "  0.10422086095493832: 4,\n",
              "  0.10438868842829571: 1,\n",
              "  0.1044726021649744: 1,\n",
              "  0.1045565159016531: 114,\n",
              "  0.1046404296383318: 20,\n",
              "  0.10480825711168919: 1,\n",
              "  0.10489217084836788: 2,\n",
              "  0.10505999832172527: 2,\n",
              "  0.10522782579508265: 1,\n",
              "  0.10564739447847613: 15,\n",
              "  0.10573130821515482: 7,\n",
              "  0.10581522195183352: 5,\n",
              "  0.10589913568851221: 11,\n",
              "  0.1059830494251909: 2,\n",
              "  0.1060669631618696: 37,\n",
              "  0.1061508768985483: 2,\n",
              "  0.10623479063522699: 4,\n",
              "  0.10640261810858438: 3,\n",
              "  0.10648653184526306: 4,\n",
              "  0.10665435931862045: 1,\n",
              "  0.10673827305529915: 2,\n",
              "  0.10682218679197784: 5,\n",
              "  0.10699001426533523: 11,\n",
              "  0.10707392800201393: 1,\n",
              "  0.10715784173869262: 1,\n",
              "  0.10732566921205001: 1,\n",
              "  0.10740958294872871: 6,\n",
              "  0.1074934966854074: 3,\n",
              "  0.1076613241587648: 1,\n",
              "  0.10774523789544349: 3,\n",
              "  0.10791306536880087: 1,\n",
              "  0.10799697910547956: 49,\n",
              "  ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LHfXJv19PUbK"
      },
      "cell_type": "markdown",
      "source": [
        "# Device config\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Vz5EK_lsPUbK",
        "outputId": "19886030-fdf6-43cb-b769-0ea32128ee7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "R8ZUFjzAPUba"
      },
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding"
      ]
    },
    {
      "metadata": {
        "id": "RD54yNsoJszv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder(categories='auto')\n",
        "enc.fit(train_classes_t.reshape(-1, 1))\n",
        "\n",
        "# 3. Transform\n",
        "train_classes_t = enc.transform(train_classes_t.reshape(-1, 1)).toarray()\n",
        "test_classes_t = enc.transform(test_classes_t.reshape(-1, 1)).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whJ9TGvXu91T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check class imbalance"
      ]
    },
    {
      "metadata": {
        "id": "iWfocSahl6Ax",
        "colab_type": "code",
        "outputId": "1a635a27-a260-4991-d4d8-becde6e9d442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cnt_1, cnt_2, cnt_3 = 0, 0, 0\n",
        "for i in test_classes_t:\n",
        "    j = list(i)\n",
        "    if j == [1, 0, 0]:\n",
        "        cnt_1+=1\n",
        "    elif j == [0, 1, 0]:\n",
        "        cnt_2+=1\n",
        "    elif j == [0, 0, 1]:\n",
        "        cnt_3+=1\n",
        "        \n",
        "cnt_1/ len(test_classes_t), cnt_2/ len(test_classes_t), cnt_3 / len(test_classes_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.48407400343314894, 0.2944243117807871, 0.22150168478606397)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "T-ODbAEflDoB",
        "colab_type": "code",
        "outputId": "867a54e9-fbfb-4267-9e6c-c3f7bcaa58e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cnt_1, cnt_2, cnt_3 = 0, 0, 0\n",
        "for i in train_classes_t:\n",
        "    j = list(i)\n",
        "    if j == [1, 0, 0]:\n",
        "        cnt_1+=1\n",
        "    elif j == [0, 1, 0]:\n",
        "        cnt_2+=1\n",
        "    elif j == [0, 0, 1]:\n",
        "        cnt_3+=1\n",
        "        \n",
        "cnt_1/ len(train_classes_t), cnt_2/ len(train_classes_t), cnt_3 / len(train_classes_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.35532067270886325, 0.38490603314153066, 0.2597732941496061)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "rQkBotSWmAnh",
        "colab_type": "code",
        "outputId": "7457f6c0-6c82-4247-e954-c1995ff7e9a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_classes_t.shape, train_matrices.shape, test_classes_t.shape, test_matrices.shape, "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((80986, 3), (80986, 500, 7), (15729, 3), (15729, 500, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "zEhs--jkfUGa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wA1Jtv8wPUbb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 25\n",
        "multi_class = False\n",
        "num_classes = 3 if multi_class else 1\n",
        "batch_size = 128\n",
        "learning_rate = 0.1\n",
        "size_user_info = 8\n",
        "\n",
        "tensor_1 = torch.from_numpy(train_matrices)\n",
        "tensor_2 = torch.from_numpy(train_classes_t) if multi_class else torch.from_numpy(train_classes)\n",
        "tensor_3 = torch.from_numpy(train_user_data.astype(float))\n",
        "tensor_4 = torch.from_numpy(train_dates.astype(float))\n",
        "tensor_5 = torch.from_numpy(train_btc_meta_data.astype(float))\n",
        "\n",
        "full_dataset = torch.utils.data.TensorDataset(tensor_1, tensor_2, tensor_3, tensor_4, tensor_5)\n",
        "\n",
        "tensor_1 = torch.from_numpy(test_matrices)\n",
        "tensor_2 = torch.from_numpy(test_classes_t) if multi_class else torch.from_numpy(test_classes)\n",
        "tensor_3 = torch.from_numpy(test_user_data.astype(float))\n",
        "tensor_4 = torch.from_numpy(test_dates.astype(float))\n",
        "tensor_5 = torch.from_numpy(test_btc_meta_data.astype(float))\n",
        "\n",
        "test = torch.utils.data.TensorDataset(tensor_1, tensor_2, tensor_3, tensor_4, tensor_5)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          drop_last=True,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ci9PWyjRPUbW"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5ZuzphFRPUbW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/rodgzilla/machine_learning_pytorch_simple_cnn_1d_nlp/blob/master/notebooks/CNN%201D.ipynb\n",
        "# class ConvNet(nn.Module):\n",
        "#     def __init__(self, num_classes):\n",
        "#         super(ConvNet, self).__init__()\n",
        "#         self.conv1 = nn.Conv1d(INPUT_SIZE_CNN, 256, kernel_size=5, stride=1, padding=2)\n",
        "#         self.bn1 = nn.BatchNorm1d(256)\n",
        "#         self.conv2 = nn.Conv1d(256, 128, kernel_size=5, stride=1, padding=1)\n",
        "#         self.conv3 = nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1)\n",
        "#         self.conv4 = nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "#         self.fc1 = nn.Linear(5*64, 128)\n",
        "#         self.bn2 = nn.BatchNorm1d(128)\n",
        "#         self.fc2 = nn.Linear(128, 32)\n",
        "#         self.fc3 = nn.Linear(32 + size_user_info, num_classes) # +  size_user_info\n",
        "#         self.out_act = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, x, user_data, btc_meta_data):\n",
        "#         #print(x.shape, btc_meta_data.shape)\n",
        "#         #x = torch.cat((x, btc_meta_data), 0)\n",
        "#         #print(x.shape)\n",
        "#         x = x.view(batch_size, INPUT_SIZE_CNN, 7).float()\n",
        "#         #print(x.shape)\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         #x = self.bn1(x)\n",
        "#         #print(x.shape)\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         #print(x.shape)\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         #print(x.shape)\n",
        "#         x = x.view(x.shape[0], -1)\n",
        "#         #print(x.shape)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.bn2(x)\n",
        "#         x = self.fc2(x)\n",
        "#         x = torch.cat((x, user_data.float()), 1)\n",
        "#         x = self.fc3(x)\n",
        "#         if not multi_class:\n",
        "#             x = self.out_act(x)\n",
        "#         return x\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # Todo: add batchnorm for all layers?\n",
        "        self.conv1 = nn.Conv1d(7, 256, kernel_size=5, stride=1, padding=2) #+ 7\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(256, 128, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        \n",
        "        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        \n",
        "        self.conv4 = nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        \n",
        "        self.fc1 = nn.Linear(5*64, 128)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.bn6 = nn.BatchNorm1d(32)\n",
        "        self.drop2 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.fc3 = nn.Linear(32 + size_user_info, num_classes) # +  size_user_info\n",
        "        self.bn7 = nn.BatchNorm1d(num_classes)\n",
        "        self.drop3 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.out_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, user_data, btc_meta_data):\n",
        "        x = btc_meta_data#torch.cat((x, btc_meta_data), 1)\n",
        "        x = x.view(batch_size, btc_meta_data.shape[1], 7).float() #+ btc_meta_data.shape[1]\n",
        "        x = self.bn1(F.relu(self.conv1(x)))\n",
        "\n",
        "        x = self.bn2(F.relu(self.conv2(x)))\n",
        "        x = self.bn3(F.relu(self.conv3(x)))\n",
        "        x = self.bn4(F.relu(self.conv4(x)))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        x = self.drop1(self.bn5(F.relu(self.fc1(x))))\n",
        "        x = self.drop2(self.bn6(F.relu(self.fc2(x))))\n",
        "        \n",
        "        x = torch.cat((x, user_data.float()), 1)\n",
        "        x = self.drop3(self.bn7(self.fc3(x)))\n",
        "        \n",
        "        if not multi_class:\n",
        "            x = self.out_act(x)\n",
        "        return x\n",
        "\n",
        "model = ConvNet(num_classes).to(device)\n",
        "\n",
        "#criterion = nn.BCELoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7mFcxQnvbvHp",
        "colab_type": "code",
        "outputId": "61b8c477-963a-4492-ae1c-a38f8ad0540d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "pytorch_total_params"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "263499"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dagH0eMcPUbd"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Train and test final model"
      ]
    },
    {
      "metadata": {
        "id": "937T81WD4c1c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stops the training if validation loss dosen't improve after a given patience.\n",
        "    CREDITS GO TO: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=7, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/data/btc/models/{}/{}/{}_checkpoint.pt'.format(model_type, batch_size, np.round(map_score, 3)))\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8tX7-se4PUbe",
        "outputId": "08141502-e19c-4ba4-f89c-a541f0941fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1192
        }
      },
      "cell_type": "code",
      "source": [
        "# Init model\n",
        "model = ConvNet(num_classes).to(device)\n",
        "\n",
        "if multi_class:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "else:\n",
        "    criterion = nn.BCELoss()    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=1e-2)#S\n",
        "\n",
        "# Set train and validation data loaders.\n",
        "len_train = int(len(full_dataset) * 0.8)\n",
        "len_val = len(full_dataset) - len_train\n",
        "len_train_temp = len_train\n",
        "\n",
        "train, validation = torch.utils.data.dataset.random_split(full_dataset, [len_train, len_val])\n",
        "# len_train = [x for x in range(0, len_train_temp)]\n",
        "# len_val = [x for x in range(len_train_temp, len(full_dataset))]\n",
        "# train = torch.utils.data.dataset.Subset(full_dataset, len_train)\n",
        "# validation = torch.utils.data.dataset.Subset(full_dataset, len_val)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train,\n",
        "                                           batch_size=batch_size,\n",
        "                                           drop_last=True,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=validation,\n",
        "                                          batch_size=batch_size,\n",
        "                                          drop_last=True,\n",
        "                                          shuffle=True)\n",
        "\n",
        "total_step = len(train_loader)\n",
        "\n",
        "train_losses, train_aucs, val_losses, val_aucs = [], [], [], []\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_auc, _, _ = train_model(train_loader, multi_class)\n",
        "    val_loss, val_auc, _, _ = validate_model(val_loader, multi_class)\n",
        "    train_losses.append(train_loss)\n",
        "    train_aucs.append(train_auc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_aucs.append(val_aucs)\n",
        "    print('At end of epoch, average (training) loss: {}, MAP: {} '.format(train_loss, train_auc))\n",
        "    print('At end of epoch, average (validation) loss: {}, MAP: {} '.format(val_loss, val_auc))\n",
        "    early_stopping(val_loss, model)\n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "        print(\"EARLY STOPPAGE AFTER {} EPOCHS\".format(epoch))\n",
        "        break\n",
        "\n",
        "map_score, y, pred = test_model(test_loader, multi_class)\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/data/btc/models/model_{}_{}_{}_{}_{}.ckpt'.format(\n",
        "    num_epochs, batch_size, model_type, auc, time.time()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Step [1/783], Loss: 0.8925\n",
            "Epoch [1/25], Step [262/783], Loss: 0.7990\n",
            "Epoch [1/25], Step [523/783], Loss: 0.7745\n",
            "Epoch [1/25], val MAP/acc: 0.4496794871794872, val loss: 15.205917358398438\n",
            "At end of epoch, average (training) loss: 0.7581698298454285, MAP: 0.4510092415718665 \n",
            "At end of epoch, average (validation) loss: 15.205917358398438, MAP: 0.4496794871794872 \n",
            "Epoch [2/25], Step [1/783], Loss: 16.1901\n",
            "Epoch [2/25], Step [262/783], Loss: 12.5590\n",
            "Epoch [2/25], Step [523/783], Loss: 12.5059\n",
            "Epoch [2/25], val MAP/acc: 0.449599358974359, val loss: 12.422890663146973\n",
            "At end of epoch, average (training) loss: 12.515069007873535, MAP: 0.4523784655512405 \n",
            "At end of epoch, average (validation) loss: 12.422890663146973, MAP: 0.449599358974359 \n",
            "Validation loss decreased (inf --> 12.422891).  Saving model ...\n",
            "Epoch [3/25], Step [1/783], Loss: 13.1679\n",
            "Epoch [3/25], Step [262/783], Loss: 12.5195\n",
            "Epoch [3/25], Step [523/783], Loss: 12.5327\n",
            "Epoch [3/25], val MAP/acc: 0.44979967948717947, val loss: 12.428425788879395\n",
            "At end of epoch, average (training) loss: 12.502662658691406, MAP: 0.45248643039591313 \n",
            "At end of epoch, average (validation) loss: 12.428425788879395, MAP: 0.44979967948717947 \n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [4/25], Step [1/783], Loss: 11.6568\n",
            "Epoch [4/25], Step [262/783], Loss: 12.4964\n",
            "Epoch [4/25], Step [523/783], Loss: 12.5087\n",
            "Epoch [4/25], val MAP/acc: 0.44915865384615383, val loss: 12.410712242126465\n",
            "At end of epoch, average (training) loss: 12.503767013549805, MAP: 0.45252634099616856 \n",
            "At end of epoch, average (validation) loss: 12.410712242126465, MAP: 0.44915865384615383 \n",
            "Validation loss decreased (12.422891 --> 12.410712).  Saving model ...\n",
            "Epoch [5/25], Step [1/783], Loss: 13.8155\n",
            "Epoch [5/25], Step [262/783], Loss: 12.5747\n",
            "Epoch [5/25], Step [523/783], Loss: 12.5005\n",
            "Epoch [5/25], val MAP/acc: 0.44971955128205127, val loss: 12.426210403442383\n",
            "At end of epoch, average (training) loss: 12.502387046813965, MAP: 0.4524764527458493 \n",
            "At end of epoch, average (validation) loss: 12.426210403442383, MAP: 0.44971955128205127 \n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [6/25], Step [1/783], Loss: 11.4410\n",
            "Epoch [6/25], Step [262/783], Loss: 12.4873\n",
            "Epoch [6/25], Step [523/783], Loss: 12.4906\n",
            "Epoch [6/25], val MAP/acc: 0.4498397435897436, val loss: 12.429532051086426\n",
            "At end of epoch, average (training) loss: 12.504318237304688, MAP: 0.4525462962962963 \n",
            "At end of epoch, average (validation) loss: 12.429532051086426, MAP: 0.4498397435897436 \n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [7/25], Step [1/783], Loss: 12.0886\n",
            "Epoch [7/25], Step [262/783], Loss: 12.6406\n",
            "Epoch [7/25], Step [523/783], Loss: 12.5116\n",
            "Epoch [7/25], val MAP/acc: 0.4497596153846154, val loss: 12.42731761932373\n",
            "At end of epoch, average (training) loss: 12.503767013549805, MAP: 0.45252634099616856 \n",
            "At end of epoch, average (validation) loss: 12.42731761932373, MAP: 0.4497596153846154 \n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch [8/25], Step [1/783], Loss: 11.8727\n",
            "Epoch [8/25], Step [262/783], Loss: 12.4124\n",
            "Epoch [8/25], Step [523/783], Loss: 12.4869\n",
            "Epoch [8/25], val MAP/acc: 0.4498798076923077, val loss: 12.430639266967773\n",
            "At end of epoch, average (training) loss: 12.503212928771973, MAP: 0.45250638569604085 \n",
            "At end of epoch, average (validation) loss: 12.430639266967773, MAP: 0.4498798076923077 \n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch [9/25], Step [1/783], Loss: 14.6790\n",
            "Epoch [9/25], Step [262/783], Loss: 12.6793\n",
            "Epoch [9/25], Step [523/783], Loss: 12.5736\n",
            "Epoch [9/25], val MAP/acc: 0.4502403846153846, val loss: 12.44060230255127\n",
            "At end of epoch, average (training) loss: 12.50348949432373, MAP: 0.45251636334610473 \n",
            "At end of epoch, average (validation) loss: 12.44060230255127, MAP: 0.4502403846153846 \n",
            "EarlyStopping counter: 5 out of 5\n",
            "EARLY STOPPAGE AFTER 8 EPOCHS\n",
            "Test MAP: 0.4258512544802867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9D8J_1xtAPNG",
        "colab_type": "code",
        "outputId": "848886f4-d11f-4f6a-bfb7-e6ace1d00b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.fixes import signature\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "auc, y, pred = test_model(test_loader, multi_class)\n",
        "#losses, auc, y, pred = validate_model(val_loader, multi_class)\n",
        "#losses, auc, y, pred = train_model(train_loader, multi_class)\n",
        "\n",
        "preds = [1 if x > 0.5 else 0 for x in pred]\n",
        "print(np.unique(preds, return_counts=Tue))\n",
        "print(np.unique(y, return_counts=True))\n",
        "print('The accuracy is: {}'.format(accuracy_score(y, preds)))\n",
        "\n",
        "average_precision = average_precision_score(y, pred)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y, pred)\n",
        "\n",
        "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
        "step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "plt.step(recall, precision, color='b', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
        "          average_precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test MAP: 0.4258512544802867\n",
            "(array([0]), array([35712]))\n",
            "(array([0, 1]), array([20504, 15208]))\n",
            "The accuracy is: 0.5741487455197133\n",
            "Average precision-recall score: 0.43\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '2-class Precision-Recall curve: AP=0.43')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wrYOE3JJ5H6P",
        "colab_type": "code",
        "outputId": "e558214e-4c7e-46e5-ed3f-c70094ed6477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "# Make plot.\n",
        "plt.plot(train_losses, color='skyblue')\n",
        "plt.plot(val_losses, color='green')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSEFS7kIIkYKk3IUQIgVJuQshRAr6/yowYNm4yTepAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f62323411d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MPW-UzYQ7iag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(train_aucs, color='skyblue')\n",
        "plt.plot(val_aucs, color='green')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wyZbq40RvagH",
        "colab_type": "code",
        "outputId": "6393028e-40e4-4693-8bd7-bb9d7f1de208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "auc, y, pred = test_model(test_loader, multi_class)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test MAP: 0.40002634756017913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j_Sg1xu6LxqK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Optional) Load model"
      ]
    },
    {
      "metadata": {
        "id": "RJkcal8DLyIF",
        "colab_type": "code",
        "outputId": "865dac28-f978-4a26-e3b2-b65314cfb5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "cell_type": "code",
      "source": [
        "base_str = '/content/gdrive/My Drive/Colab Notebooks/data/btc'\n",
        "model = model.load_state_dict(torch.load('{}/model_{}_{}_{}.ckpt'.format(\n",
        "    base_str, num_epochs, batch_size, 'cluster')))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-73e416ad7dd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbase_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/My Drive/Colab Notebooks/data/btc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = model.load_state_dict(torch.load('{}/model_{}_{}_{}.ckpt'.format(\n\u001b[0m\u001b[1;32m      3\u001b[0m     base_str, num_epochs, batch_size, 'cluster')))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "C-QvA-Tif6Ea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Methods"
      ]
    },
    {
      "metadata": {
        "id": "cUXpr9gDb2jG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
        "\n",
        "def validate_model(val_loader, multi_class):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = []\n",
        "        y = []\n",
        "        losses = []\n",
        "        for images, labels, user_data, dates, btc_meta_data in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            user_data = user_data.to(device)\n",
        "            btc_meta_data = btc_meta_data.to(device)\n",
        "            outputs = model(images, user_data, btc_meta_data)\n",
        "            if multi_class:\n",
        "                loss = criterion(outputs, torch.max(labels, 1)[1])\n",
        "                pred.extend(torch.max(outputs, 1)[1].data.cpu().numpy())\n",
        "                y.extend(torch.max(labels, 1)[1].data.cpu().numpy())\n",
        "            else:\n",
        "                loss = criterion(outputs, labels.float())\n",
        "                pred.extend(outputs.data.cpu().numpy())\n",
        "                y.extend(labels.data.cpu().numpy())\n",
        "            losses.append(loss.data.cpu().numpy())\n",
        "            \n",
        "        losses = np.mean(losses)\n",
        "      \n",
        "        if multi_class:\n",
        "            auc = accuracy_score(y, pred)\n",
        "        else:\n",
        "            auc = average_precision_score(y, pred)\n",
        "\n",
        "        print('Epoch [{}/{}], val MAP/acc: {}, val loss: {}'\n",
        "              .format(epoch + 1, num_epochs, auc, losses))\n",
        "        return losses, auc, y, pred\n",
        "\n",
        "def train_model(train_loader, multi_class):\n",
        "    losses = []\n",
        "    pred = []\n",
        "    y = []\n",
        "    for i, (images, labels, user_data, dates, btc_meta_data) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        user_data = user_data.to(device)\n",
        "        btc_meta_data = btc_meta_data.to(device)\n",
        "        outputs = model(images, user_data, btc_meta_data)\n",
        "        \n",
        "        if multi_class:\n",
        "            loss = criterion(outputs, torch.max(labels, 1)[1])\n",
        "            pred.extend(torch.max(outputs, 1)[1].data.cpu().numpy())\n",
        "            y.extend(torch.max(labels, 1)[1].data.cpu().numpy())\n",
        "        else:\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            pred.extend(outputs.data.cpu().numpy())\n",
        "            y.extend(labels.data.cpu().numpy())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.data.cpu().numpy())\n",
        "        \n",
        "        if i % int(len(train_loader) / 3)  == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, np.mean(losses)))\n",
        "\n",
        "    if multi_class:\n",
        "        auc = accuracy_score(y, pred)\n",
        "    else:\n",
        "        auc = average_precision_score(y, pred)\n",
        "\n",
        "    return np.mean(losses), auc, y, pred\n",
        "\n",
        "def test_model(test_loader, multi_class):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = []\n",
        "        y = []\n",
        "        for images, labels, user_data, dates, btc_meta_data in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            user_data = user_data.to(device)\n",
        "            btc_meta_data = btc_meta_data.to(device)\n",
        "            outputs = model(images, user_data, btc_meta_data)\n",
        "            if multi_class:\n",
        "                pred.extend(torch.max(outputs, 1)[1].data.cpu().numpy())\n",
        "                y.extend(torch.max(labels, 1)[1].data.cpu().numpy())\n",
        "            else:\n",
        "                pred.extend(outputs.data.cpu().numpy())\n",
        "                y.extend(labels.data.cpu().numpy())\n",
        "            \n",
        "      \n",
        "        if multi_class:\n",
        "            auc = accuracy_score(y, pred)\n",
        "        else:\n",
        "            auc = average_precision_score(y, pred)\n",
        "\n",
        "        print('Test MAP: {}'.format(auc))\n",
        "        return auc, y, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "caOTV5E5aLf5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# K-Fold CV - SUPER MEGA DEATH LOOP."
      ]
    },
    {
      "metadata": {
        "id": "osMPi4-LI45G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Methods"
      ]
    },
    {
      "metadata": {
        "id": "AFfQuQf0kWzT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Early stoppage class"
      ]
    },
    {
      "metadata": {
        "id": "cKNuXdg29ToW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stops the training if validation loss dosen't improve after a given patience.\n",
        "    CREDITS GO TO: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=7, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "                            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        \n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "        \n",
        "        \n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')\n",
        "        #torch.save(model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/data/btc/models/{}/{}/{}_checkpoint.pt'.format(model_type, batch_size, np.round(map_score, 3)))\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DDUupONTK1nL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Aggregate test"
      ]
    },
    {
      "metadata": {
        "id": "4UbwefSfCiKY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def conf_matrix(combination, batch_size, conf_matrix, score_type):    \n",
        "    class_names=[0,1] # name  of classes\n",
        "    fig, ax = plt.subplots()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # create heatmap\n",
        "    sns.heatmap(pd.DataFrame(conf_matrix), annot=True, fmt='g')\n",
        "    ax.xaxis.set_label_position(\"top\")\n",
        "#     plt.tight_layout()\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig('/content/gdrive/My Drive/Colab Notebooks/data/btc/result_conf_matrix/{}_{}_{}_{}_{}_{}'.format(combination[0], combination[1], int(combination[2]), int(combination[3]), batch_size, score_type))\n",
        "    plt.close()\n",
        "\n",
        "def mean_pred_test(combination, train_df, test_df):\n",
        "#     for i in range(1, 99):\n",
        "#         threshold = i / 100\n",
        "#         cnt_MV = 0\n",
        "#         cnt_max = 0\n",
        "#         dates = train_df['date'].unique()\n",
        "#         score = 0\n",
        "\n",
        "#         for date in dates:\n",
        "#             temp = train_df[train_df['date'] == date]\n",
        "#             y_true = temp['true'].unique()[0]\n",
        "#             MV = temp['pred_max'].value_counts().index[0]\n",
        "\n",
        "#             max_out = temp['output'].mean()\n",
        "#             max_all = 1 if max_out >= threshold else 0\n",
        "\n",
        "#             if y_true == max_all:\n",
        "#                 cnt_max += 1\n",
        "#         new_score = cnt_max / len(dates)\n",
        "#         if new_score > score:\n",
        "#             best_thresh = threshold\n",
        "#             score = new_score\n",
        "\n",
        "#     cnt_max = 0\n",
        "#     pairs = []\n",
        "    dates = test_df['date'].unique()\n",
        "    y_test = []\n",
        "    y_pred = []\n",
        "    for date in dates:\n",
        "        temp = test_df[test_df['date'] == date]\n",
        "        y_true = temp['true'].unique()[0]\n",
        "\n",
        "        max_out = temp['output'].mean()\n",
        "        #print(max_out)\n",
        "        max_all = 1.0 if max_out > 0.5 else 0.0\n",
        "        y_pred.append(max_all)\n",
        "        y_test.append(y_true)\n",
        "    #print(y_test, y_pred)\n",
        "    #conf_matrix(combination, y_test, y_pred)\n",
        "    return metrics.confusion_matrix(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred)\n",
        "\n",
        "def dataset_LR(train_df, test_df):\n",
        "    # Create dataset first.\n",
        "    train_users = train_df['user'].unique()\n",
        "    test_users = test_df['user'].unique()\n",
        "\n",
        "    users = np.concatenate((train_users, test_users))\n",
        "    user_ids_lr = {}\n",
        "    for i, v in enumerate(users):\n",
        "        user_ids_lr[v] = i\n",
        "\n",
        "    # Loop over dates, check if user has prediction\n",
        "    X_train = None\n",
        "    y_train = []\n",
        "    cnt = 0\n",
        "    for date in train_df['date'].unique():\n",
        "        cnt += 1\n",
        "        temp = train_df[train_df['date'] == date]\n",
        "        arr = np.zeros(len(users))\n",
        "        for row in temp.itertuples():\n",
        "            usr = row[5]\n",
        "            pred = row[2]\n",
        "            lbl = row[4]\n",
        "            arr[user_ids_lr[usr]] = pred\n",
        "        y_train.append([lbl])\n",
        "        if X_train is None:\n",
        "            X_train = arr\n",
        "        else:\n",
        "            X_train = np.vstack((X_train, arr))\n",
        "    \n",
        "    y_train = np.array(y_train)  \n",
        "\n",
        "    # Loop over dates, check if user has prediction\n",
        "    X_test = None\n",
        "    y_test = []\n",
        "\n",
        "    for date in test_df['date'].unique():\n",
        "        cnt += 1\n",
        "        temp = test_df[test_df['date'] == date]\n",
        "        arr = np.zeros(len(users))\n",
        "        for row in temp.itertuples():\n",
        "            usr = row[5]\n",
        "            pred = row[2]\n",
        "            lbl = row[4]\n",
        "            try:\n",
        "                arr[user_ids_lr[usr]] = pred\n",
        "            except:\n",
        "                #print('User {} not found'.format(usr))\n",
        "                cnt_n += 1\n",
        "        y_test.append([lbl])\n",
        "        if X_test is None:\n",
        "            X_test = arr\n",
        "        else:\n",
        "            X_test = np.vstack((X_test, arr))\n",
        "\n",
        "    y_test = np.array(y_test)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "    \n",
        "    \n",
        "def LR(combination, train_df, test_df):    \n",
        "    X_train, y_train, X_test, y_test = dataset_LR(train_df, test_df)\n",
        "    # instantiate the model (using the default parameters)\n",
        "    logreg = LogisticRegression()\n",
        "\n",
        "    # fit the model with data\n",
        "    logreg.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = logreg.predict(X_test)\n",
        "    y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
        "    #print(y_pred_prob, y_test)\n",
        "    #conf_matrix(combination, y_test, y_pred)\n",
        "    return metrics.confusion_matrix(y_test, y_pred), precision_score(y_test, y_pred), recall_score(y_test, y_pred)\n",
        "\n",
        "def aggregate_evaluation_test(combination):\n",
        "    ## Create datasets.\n",
        "    train_lr = None\n",
        "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    with torch.no_grad():\n",
        "        for images, labels, user_data, dates, btc_meta_data in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            user_data = user_data.to(device)\n",
        "            btc_meta_data = btc_meta_data.to(device)\n",
        "            outputs = model(images, user_data, btc_meta_data, batch_size, combination[2], combination[3])\n",
        "            predicted = torch.max(outputs, 1)[1].data.cpu().numpy()\n",
        "\n",
        "            if multi_class:\n",
        "                labels = torch.max(labels, 1)[1].data.cpu().numpy()\n",
        "                temp = np.dstack((labels, user_data.data.cpu().numpy()[:,-1],\n",
        "                               dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "                temp = np.hstack((outputs, temp))\n",
        "\n",
        "            else:\n",
        "                outputs = np.array([x[0] for x in outputs.data.cpu().numpy()])\n",
        "                temp = np.dstack((outputs, labels, \n",
        "                                   user_data.data.cpu().numpy()[:,-1],\n",
        "                                   dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "            if train_lr is None:\n",
        "                train_lr = temp\n",
        "            else:\n",
        "                train_lr = np.concatenate((train_lr, temp), axis=0)\n",
        "                \n",
        "                \n",
        "    test_lr = None\n",
        "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    with torch.no_grad():\n",
        "        for images, labels, user_data, dates, btc_meta_data in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            user_data = user_data.to(device)\n",
        "            btc_meta_data = btc_meta_data.to(device)\n",
        "            outputs = model(images, user_data, btc_meta_data, batch_size, combination[2], combination[3])\n",
        "            predicted = torch.max(outputs, 1)[1].data.cpu().numpy()\n",
        "\n",
        "            if multi_class:\n",
        "                labels = torch.max(labels, 1)[1].data.cpu().numpy()\n",
        "                temp = np.dstack((labels, user_data.data.cpu().numpy()[:,-1],\n",
        "                               dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "                temp = np.hstack((outputs, temp))\n",
        "            else:\n",
        "                outputs = np.array([x[0] for x in outputs.data.cpu().numpy()])\n",
        "                temp = np.dstack((outputs, labels, \n",
        "                                   user_data.data.cpu().numpy()[:,-1],\n",
        "                                   dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "\n",
        "            if test_lr is None:\n",
        "                test_lr = temp\n",
        "            else:\n",
        "                test_lr = np.concatenate((test_lr, temp), axis=0)\n",
        "                \n",
        "    train_df = pd.DataFrame({'output':train_lr[:,0],'true':train_lr[:,1], \n",
        "                                   'user':train_lr[:,2], 'date':train_lr[:,3], 'pred_max':train_lr[:,4]})\n",
        "    test_df = pd.DataFrame({'output':test_lr[:,0],'true':test_lr[:,1], \n",
        "                                       'user':test_lr[:,2], 'date':test_lr[:,3], 'pred_max':test_lr[:,4]})\n",
        "    \n",
        "    mean_conf, mean_prec, mean_recall = mean_pred_test(combination, train_df, test_df)\n",
        "    LR_conf, LR_prec, LR_recall = LR(combination, train_df, test_df)\n",
        "    \n",
        "    return LR_conf, LR_prec, LR_recall, mean_conf, mean_prec, mean_recall"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yJuycwBmK4ss",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ]
    },
    {
      "metadata": {
        "id": "b485NEQsQylN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tryint(s):\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return s\n",
        "     \n",
        "def alphanum_key(s):\n",
        "    \"\"\" Turn a string into a list of string and number chunks.\n",
        "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
        "    \"\"\"\n",
        "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
        "\n",
        "def sort_nicely(l):\n",
        "    \"\"\" Sort the given list in the way that humans expect.\n",
        "    \n",
        "    amazing <3 RegExp\n",
        "    \"\"\"\n",
        "    l.sort(key=alphanum_key)\n",
        "    return l\n",
        "\n",
        "def load_matrices(combination):\n",
        "    model_type = combination[0]#['keywords_tfidf_p', 'keywords_tfidf', 'custom_features'][2]\n",
        "    mode = combination[1]#['count', 'binary'][0]\n",
        "    INPUT_SIZE_CNN = 25 if model_type == 'keywords_clusters' else 126 if 'custom' in model_type else 500\n",
        "    \n",
        "    #train, test = None, None\n",
        "    \n",
        "    train_classes, train_user_data, train_matrices, train_dates, train_btc_meta_data, train_classes_t = None, None, None, None, None, None\n",
        "    base_str = '/content/gdrive/My Drive/Colab Notebooks/data/btc/{}/{}/train/'.format(model_type, mode)\n",
        "    files = os.listdir(base_str)\n",
        "    for train_file in sort_nicely(files):\n",
        "        if 'btc' in train_file:\n",
        "            temp = np.load(base_str + train_file)\n",
        "            if train_btc_meta_data is None:\n",
        "                train_btc_meta_data = temp\n",
        "            else:\n",
        "                train_btc_meta_data = np.concatenate((train_btc_meta_data, temp), axis=0)\n",
        "        elif 'tc_up' in train_file:\n",
        "            temp = np.load(base_str + train_file)\n",
        "            if train_classes_t is None:\n",
        "                train_classes_t = temp\n",
        "            else:\n",
        "                train_classes_t = np.concatenate((train_classes_t, temp), axis=0)\n",
        "        elif 'c_up' in train_file:\n",
        "            temp = np.load(base_str + train_file)\n",
        "            if train_classes is None:\n",
        "                train_classes = temp\n",
        "            else:\n",
        "                train_classes = np.concatenate((train_classes, temp), axis=0)\n",
        "        elif 'md_up' in train_file:\n",
        "            temp = np.load(base_str + train_file)\n",
        "            if train_user_data is None:\n",
        "                train_user_data = temp\n",
        "            else:\n",
        "                train_user_data = np.vstack((train_user_data, temp))\n",
        "        elif 'dp_up' in train_file:\n",
        "            temp = np.load(base_str + train_file)\n",
        "            if train_matrices is None:\n",
        "                train_matrices = temp\n",
        "            else:\n",
        "                train_matrices = np.vstack((train_matrices, temp))\n",
        "        elif 'dt_up' in train_file:\n",
        "            temp = np.load(base_str + train_file)\n",
        "            if train_dates is None:\n",
        "                train_dates = temp\n",
        "            else:\n",
        "                train_dates = np.concatenate((train_dates, temp), axis=0)\n",
        "\n",
        "    test_classes, test_user_data, test_matrices, test_dates, test_btc_meta_data, test_classes_t = None, None, None, None, None, None\n",
        "    base_str = '/content/gdrive/My Drive/Colab Notebooks/data/btc/{}/{}/test/'.format(model_type, mode)\n",
        "\n",
        "    for test_file in sort_nicely(os.listdir(base_str)):\n",
        "        if 'btc' in test_file:\n",
        "            temp = np.load(base_str + test_file)\n",
        "            if test_btc_meta_data is None:\n",
        "                test_btc_meta_data = temp\n",
        "            else:\n",
        "                test_btc_meta_data = np.concatenate((test_btc_meta_data, temp), axis=0)\n",
        "        elif 'tc_up' in test_file:\n",
        "            temp = np.load(base_str + test_file)\n",
        "            if test_classes_t is None:\n",
        "                test_classes_t = temp\n",
        "            else:\n",
        "                test_classes_t = np.concatenate((test_classes_t, temp), axis=0)\n",
        "        elif 'c_up' in test_file:\n",
        "            temp = np.load(base_str + test_file)\n",
        "            if test_classes is None:\n",
        "                test_classes = temp\n",
        "            else:\n",
        "                test_classes = np.concatenate((test_classes, temp), axis=0)\n",
        "        elif 'md_up' in test_file:\n",
        "            temp = np.load(base_str + test_file)\n",
        "            if test_user_data is None:\n",
        "                test_user_data = temp\n",
        "            else:\n",
        "                test_user_data = np.vstack((test_user_data, temp))\n",
        "        elif 'dp_up' in test_file:\n",
        "            temp = np.load(base_str + test_file)\n",
        "            if test_matrices is None:\n",
        "                test_matrices = temp\n",
        "            else:\n",
        "                test_matrices = np.vstack((test_matrices, temp))\n",
        "        elif 'dt_up' in test_file:\n",
        "            temp = np.load(base_str + test_file)\n",
        "            if test_dates is None:\n",
        "                test_dates = temp\n",
        "            else:\n",
        "                test_dates = np.concatenate((test_dates, temp), axis=0)\n",
        "        \n",
        "    # Normalize\n",
        "    # Re-assign tokens for userIds as this makes normalization easier (no precision errors).\n",
        "    unique_users = np.unique(np.concatenate((np.unique(train_user_data[:,-1]), np.unique(test_user_data[:,-1]))))\n",
        "    user_ids = {}\n",
        "    for i, v in enumerate(unique_users):\n",
        "        user_ids[v] = (i+1)\n",
        "\n",
        "    for i, v in enumerate(train_user_data):\n",
        "        train_user_data[i,-1] = user_ids[v[-1]]\n",
        "\n",
        "    for i, v in enumerate(test_user_data):\n",
        "        test_user_data[i,-1] = user_ids[v[-1]]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_user_data)\n",
        "    train_user_data = scaler.transform(train_user_data)\n",
        "    test_user_data = scaler.transform(test_user_data)\n",
        "\n",
        "    scalers = {}\n",
        "    for i in range(train_btc_meta_data.shape[1]):\n",
        "        scalers[i] = StandardScaler()\n",
        "        train_btc_meta_data[:, i, :] = scalers[i].fit_transform(train_btc_meta_data[:, i, :]) \n",
        "\n",
        "    for i in range(test_btc_meta_data.shape[1]):\n",
        "        test_btc_meta_data[:, i, :] = scalers[i].transform(test_btc_meta_data[:, i, :]) \n",
        "\n",
        "    scalers = {}\n",
        "    for i in range(train_matrices.shape[1]):\n",
        "        scalers[i] = StandardScaler()\n",
        "        train_matrices[:, i, :] = scalers[i].fit_transform(train_matrices[:, i, :]) \n",
        "\n",
        "    for i in range(test_matrices.shape[1]):\n",
        "        test_matrices[:, i, :] = scalers[i].transform(test_matrices[:, i, :]) \n",
        "\n",
        "\n",
        "    # Remove train/test users that are not in both sets.\n",
        "    indices = []\n",
        "    test_user_ids = np.unique(test_user_data[:,-1])\n",
        "    for i in range(len(train_user_data[:,-1])):\n",
        "        if train_user_data[i,-1] not in test_user_ids:\n",
        "            indices.append(i)\n",
        "\n",
        "    len(indices) / len(train_user_data[:,-1])\n",
        "\n",
        "    train_matrices = np.delete(train_matrices, indices, axis=0)\n",
        "    train_classes_t = np.delete(train_classes_t, indices)\n",
        "    train_classes = np.delete(train_classes, indices)\n",
        "    train_user_data = np.delete(train_user_data, indices, axis=0)\n",
        "    train_btc_meta_data = np.delete(train_btc_meta_data, indices, axis=0)\n",
        "    train_dates = np.delete(train_dates, indices)\n",
        "    train_matrices.shape, train_classes_t.shape, train_user_data.shape\n",
        "\n",
        "    indices = []\n",
        "    train_user_ids = np.unique(train_user_data[:,-1])\n",
        "    for i in range(len(test_user_data[:,-1])):\n",
        "        if test_user_data[i,-1] not in train_user_ids:\n",
        "            indices.append(i)\n",
        "\n",
        "    len(indices) / len(test_user_data[:,-1])\n",
        "\n",
        "    test_matrices = np.delete(test_matrices, indices, axis=0)\n",
        "    test_classes_t = np.delete(test_classes_t, indices)\n",
        "    test_classes = np.delete(test_classes, indices)\n",
        "    test_user_data = np.delete(test_user_data, indices, axis=0)\n",
        "    test_btc_meta_data = np.delete(test_btc_meta_data, indices, axis=0)\n",
        "    test_dates = np.delete(test_dates, indices)\n",
        "    test_matrices.shape, test_classes_t.shape, test_user_data.shape\n",
        "\n",
        "    return train_classes, train_user_data, train_matrices, train_dates, train_btc_meta_data, train_classes_t, test_classes, test_user_data, test_matrices, test_dates, test_btc_meta_data, test_classes_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqsmNbpbHB0V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ['keywords_tfidf_p', 'count', True, False]\n",
        "def load_data_kfold(combination):\n",
        "    # Load matrices\n",
        "    train_classes, train_user_data, train_matrices, train_dates, train_btc_meta_data, train_classes_t, test_classes, test_user_data, test_matrices, test_dates, test_btc_meta_data, test_classes_t = load_matrices(combination)\n",
        "    # Preprocess data (remove test/train and normalize).\n",
        "    \n",
        "    # create tensors etc\n",
        "    tensor_1 = torch.from_numpy(test_matrices)\n",
        "    tensor_2 = torch.from_numpy(test_classes_t) if multi_class else torch.from_numpy(test_classes)\n",
        "    tensor_3 = torch.from_numpy(test_user_data.astype(float))\n",
        "    tensor_4 = torch.from_numpy(test_dates.astype(float))\n",
        "    tensor_5 = torch.from_numpy(test_btc_meta_data.astype(float))\n",
        "\n",
        "    test = torch.utils.data.TensorDataset(tensor_1, tensor_2, tensor_3, tensor_4, tensor_5)\n",
        "    \n",
        "    tensor_1 = torch.from_numpy(train_matrices)\n",
        "    tensor_2 = torch.from_numpy(train_classes_t) if multi_class else torch.from_numpy(train_classes)\n",
        "    tensor_3 = torch.from_numpy(train_user_data.astype(float))\n",
        "    tensor_4 = torch.from_numpy(train_dates.astype(float))\n",
        "    tensor_5 = torch.from_numpy(train_btc_meta_data.astype(float))\n",
        "\n",
        "    full_dataset = torch.utils.data.TensorDataset(tensor_1, tensor_2, tensor_3, tensor_4, tensor_5)\n",
        "    \n",
        "    del train_classes, train_user_data, train_matrices, train_dates, train_btc_meta_data, train_classes_t, test_classes, test_user_data, test_matrices, test_dates, test_btc_meta_data, test_classes_t\n",
        "    \n",
        "    return full_dataset, test "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dhn37Vq7K8vp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate user specific data"
      ]
    },
    {
      "metadata": {
        "id": "SW6vWn4JLCGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
        "\n",
        "def validate_model(val_loader, multi_class, batch_size, combination):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = []\n",
        "        y = []\n",
        "        losses = []\n",
        "        for images, labels, user_data, dates, btc_meta_data in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            user_data = user_data.to(device)\n",
        "            btc_meta_data = btc_meta_data.to(device)\n",
        "            outputs = model(images, user_data, btc_meta_data, batch_size, combination[2], combination[3])\n",
        "            if multi_class:\n",
        "                loss = criterion(outputs, torch.max(labels, 1)[1])\n",
        "                pred.extend(torch.max(outputs, 1)[1].data.cpu().numpy())\n",
        "                y.extend(torch.max(labels, 1)[1].data.cpu().numpy())\n",
        "            else:\n",
        "                loss = criterion(outputs, labels.float())\n",
        "                #pred.extend(outputs.data.cpu().numpy())\n",
        "                pred.extend([1 if x > 0.5 else 0 for x in outputs.data.cpu().numpy()])\n",
        "                y.extend(labels.data.cpu().numpy())\n",
        "            losses.append(loss.data.cpu().numpy())\n",
        "            \n",
        "        losses = np.mean(losses)\n",
        "      \n",
        "        if multi_class:\n",
        "            auc = accuracy_score(y, pred)\n",
        "        else:\n",
        "            auc = [precision_score(y, pred), recall_score(y, pred)]#average_precision_score(y, pred)\n",
        "\n",
        "        print('Epoch [{}/{}], val MAP/acc: {}, val loss: {}'\n",
        "              .format(epoch + 1, num_epochs, auc, losses))\n",
        "        \n",
        "        return losses, auc, y, pred\n",
        "\n",
        "def train_model(train_loader, multi_class, batch_size, combination):\n",
        "    losses = []\n",
        "    pred = []\n",
        "    y = []\n",
        "    for i, (images, labels, user_data, dates, btc_meta_data) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        user_data = user_data.to(device)\n",
        "        btc_meta_data = btc_meta_data.to(device)\n",
        "        outputs = model(images, user_data, btc_meta_data, batch_size, combination[2], combination[3])\n",
        "        \n",
        "        if multi_class:\n",
        "            loss = criterion(outputs, torch.max(labels, 1)[1])\n",
        "            pred.extend(torch.max(outputs, 1)[1].data.cpu().numpy())\n",
        "            y.extend(torch.max(labels, 1)[1].data.cpu().numpy())\n",
        "        else:\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            #pred.extend(outputs.data.cpu().numpy())\n",
        "            pred.extend([1 if x > 0.5 else 0 for x in outputs.data.cpu().numpy()])\n",
        "            y.extend(labels.data.cpu().numpy())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.data.cpu().numpy())\n",
        "        \n",
        "        if i % int(len(train_loader) / 3)  == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, np.mean(losses)))\n",
        "\n",
        "    if multi_class:\n",
        "        auc = accuracy_score(y, pred)\n",
        "    else:\n",
        "        auc = [precision_score(y, pred), recall_score(y, pred)]#average_precision_score(y, pred)\n",
        "\n",
        "    return np.mean(losses), auc, y, pred\n",
        "\n",
        "def test_model(test_loader, multi_class, batch_size, combination):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = []\n",
        "        y = []\n",
        "        for images, labels, user_data, dates, btc_meta_data in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            user_data = user_data.to(device)\n",
        "            btc_meta_data = btc_meta_data.to(device)\n",
        "            outputs = model(images, user_data, btc_meta_data, batch_size, combination[2], combination[3])\n",
        "            if multi_class:\n",
        "                pred.extend(torch.max(outputs, 1)[1].data.cpu().numpy())\n",
        "                y.extend(torch.max(labels, 1)[1].data.cpu().numpy())\n",
        "            else:\n",
        "                #pred.extend(outputs.data.cpu().numpy())\n",
        "                pred.extend([1 if x > 0.5 else 0 for x in outputs.data.cpu().numpy()])\n",
        "                y.extend(labels.data.cpu().numpy())   \n",
        "      \n",
        "        if multi_class:\n",
        "            auc = accuracy_score(y, pred)\n",
        "        else:\n",
        "            auc = [precision_score(y, pred), recall_score(y, pred)]#average_precision_score(y, pred)\n",
        "\n",
        "        print('Test score: {}'.format(auc))\n",
        "        return auc, y, pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E7fk-6q_JBwg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ]
    },
    {
      "metadata": {
        "id": "nXtUKCGUJD_Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # Todo: add batchnorm for all layers?\n",
        "        self.conv1 = nn.Conv1d(INPUT_SIZE_CNN, 256, kernel_size=5, stride=1, padding=2) #+ 7\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(256, 128, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        \n",
        "        self.conv3 = nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        \n",
        "        self.conv4 = nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        \n",
        "        self.fc1 = nn.Linear(5*64, 128)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.bn6 = nn.BatchNorm1d(32)\n",
        "        self.drop2 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.fc3 = nn.Linear(FCN_INPUT_SIZE, num_classes) # +  size_user_info\n",
        "        self.bn7 = nn.BatchNorm1d(num_classes)\n",
        "        self.drop3 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.out_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, user_data, btc_meta_data, batch_size, use_btc, use_user):\n",
        "        if use_btc:\n",
        "            x = torch.cat((x, btc_meta_data), 1)\n",
        "        x = x.view(batch_size, INPUT_SIZE_CNN, 7).float() #+ btc_meta_data.shape[1]\n",
        "        \n",
        "        x = self.bn1(F.relu(self.conv1(x)))\n",
        "\n",
        "        x = self.bn2(F.relu(self.conv2(x)))\n",
        "        x = self.bn3(F.relu(self.conv3(x)))\n",
        "        x = self.bn4(F.relu(self.conv4(x)))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        x = self.drop1(self.bn5(F.relu(self.fc1(x))))\n",
        "        x = self.drop2(self.bn6(F.relu(self.fc2(x))))\n",
        "        \n",
        "        if use_user:\n",
        "            x = torch.cat((x, user_data.float()), 1)\n",
        "        x = self.drop3(self.bn7(self.fc3(x)))\n",
        "        \n",
        "        if not multi_class:\n",
        "            x = self.out_act(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hXDNL7l2I7d4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LOOPIE"
      ]
    },
    {
      "metadata": {
        "id": "j2AGZKdtaRag",
        "colab_type": "code",
        "outputId": "96fd441a-4ad5-4626-fd15-d3eae40e6b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1269
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 35\n",
        "batch_sizes = [32, 64, 128, 256, 512]\n",
        "n_folds = 5\n",
        "multi_class = False\n",
        "num_classes = 1\n",
        "size_user_info = 8\n",
        "size_btc_md = 6\n",
        "patience_early_stopping = 5\n",
        "patience_reduce_lr = 2\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# Add combinations, make sure custom_features has 'count' due to mapping structure.\n",
        "# 20 tests in total, 8 each for TFIDF/TFIDF-P, 4 for custom features.\n",
        "# model_type = ['keywords_tfidf_p', 'keywords_tfidf', 'custom_features'][2]\n",
        "# Booleans being the usage of btc meta-data and user meta-data or not.\n",
        "combinations = [#['custom_features', 'count', False, False],\n",
        "               #['custom_features', 'count', True, False],\n",
        "               #['custom_features', 'count', False, True],\n",
        "               #['custom_features', 'count', True, True],\n",
        "                \n",
        "               ['keywords_tfidf', 'binary', False, False],\n",
        "               #['keywords_tfidf', 'binary', True, False],\n",
        "               ['keywords_tfidf', 'binary', False, True],\n",
        "               #['keywords_tfidf', 'binary', True, True],\n",
        "               \n",
        "               ['keywords_tfidf', 'count', False, False],\n",
        "               #['keywords_tfidf', 'count', True, False],\n",
        "               ['keywords_tfidf', 'count', False, True],\n",
        "               #['keywords_tfidf', 'count', True, True],\n",
        "               \n",
        "               ['keywords_tfidf_p', 'binary', False, False],\n",
        "               #['keywords_tfidf_p', 'binary', True, False],\n",
        "               ['keywords_tfidf_p', 'binary', False, True],\n",
        "               #['keywords_tfidf_p', 'binary', True, True],\n",
        "               \n",
        "               ['keywords_tfidf_p', 'count', False, False],\n",
        "               #['keywords_tfidf_p', 'count', True, False],\n",
        "               ['keywords_tfidf_p', 'count', False, True]]#,\n",
        "               #['keywords_tfidf_p', 'count', True, True]]\n",
        "\n",
        "for combination in combinations:\n",
        "    print('-------------------------------')\n",
        "    print('-------------------------------')\n",
        "    print('STARTING NEW COMBINATION {}'.format(combination))\n",
        "    print('-------------------------------')\n",
        "    print('-------------------------------')\n",
        "    \n",
        "    if 'full_dataset' in globals():\n",
        "        del full_dataset\n",
        "        \n",
        "    full_dataset, test_dataset = load_data_kfold(combination)\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
        "    results = {}\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        INPUT_SIZE_CNN = 126 if 'custom' in combination[0] else 500\n",
        "        FCN_INPUT_SIZE = 32\n",
        "        \n",
        "        if combination[2] & combination[3]:\n",
        "            INPUT_SIZE_CNN = INPUT_SIZE_CNN + size_btc_md\n",
        "            FCN_INPUT_SIZE = FCN_INPUT_SIZE + size_user_info\n",
        "        elif combination[2] & (not combination[3]):\n",
        "            INPUT_SIZE_CNN = INPUT_SIZE_CNN + size_btc_md\n",
        "        elif (not combination[2]) & combination[3]:\n",
        "            FCN_INPUT_SIZE = FCN_INPUT_SIZE + size_user_info\n",
        "        \n",
        "        # Add learning rates that match the batch sizes.\n",
        "        print('-------------------------------')\n",
        "        print('Starting training for new batch size {}'.format(batch_size))\n",
        "        print('-------------------------------')\n",
        "        train_scores = []\n",
        "        val_scores = []\n",
        "        test_scores = []\n",
        "\n",
        "        test_agg = []\n",
        "        mean_conf_matrices = []\n",
        "        LR_conf_matrices = []\n",
        "        \n",
        "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              drop_last=True,\n",
        "                                              shuffle=False)\n",
        "               \n",
        "        for train_indexes, validation_indexes in kf.split(full_dataset):\n",
        "            train = torch.utils.data.dataset.Subset(full_dataset, train_indexes)\n",
        "            validation = torch.utils.data.dataset.Subset(full_dataset, validation_indexes)            \n",
        "            \n",
        "            # Add parameters for different model_combinations.\n",
        "            model = ConvNet(num_classes).to(device)\n",
        "            criterion = nn.BCELoss()\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            early_stopping = EarlyStopping(patience=patience_early_stopping, \n",
        "                                           verbose=True)\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                       'min', verbose=True, \n",
        "                                                       patience=patience_reduce_lr)\n",
        "\n",
        "            # Set train and validation data loaders.\n",
        "            train_loader = torch.utils.data.DataLoader(dataset=train,\n",
        "                                                       batch_size=batch_size,\n",
        "                                                       drop_last=True,\n",
        "                                                       shuffle=True)\n",
        "            val_loader = torch.utils.data.DataLoader(dataset=validation,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      drop_last=True,\n",
        "                                                      shuffle=True)\n",
        "            del train, validation\n",
        "            \n",
        "            total_step = len(train_loader)\n",
        "            print('-------------------------------')\n",
        "            print('Starting training for new fold')\n",
        "            print('-------------------------------')\n",
        "            for epoch in range(num_epochs):\n",
        "                train_loss, train_score, _, _ = train_model(train_loader, multi_class,\n",
        "                                                         batch_size, combination)\n",
        "                val_loss, val_score, _, _ = validate_model(val_loader, multi_class, \n",
        "                                                         batch_size, combination)\n",
        "                print('At end of epoch, average (training) loss: {}, score: {} '.format(train_loss, train_score))\n",
        "                print('At end of epoch, average (validation) loss: {}, score: {} '.format(val_loss, val_score))\n",
        "                early_stopping(val_loss, model)\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "                if early_stopping.early_stop:\n",
        "                    print(\"EARLY STOPPAGE AFTER {} EPOCHS\".format(epoch))\n",
        "                    break\n",
        "            test_score, y, pred = test_model(test_loader, multi_class, batch_size, combination)\n",
        "            LR_conf, LR_prec, LR_recall, mean_conf, mean_prec, mean_recall = aggregate_evaluation_test(combination)\n",
        "            \n",
        "            print('Finished fold, scores: {}'.format([LR_prec, LR_recall, mean_prec, mean_recall]))\n",
        "            \n",
        "            train_scores.append(train_score)\n",
        "            val_scores.append(val_score)\n",
        "            test_scores.append(test_score)\n",
        "            test_agg.append([LR_prec, LR_recall, mean_prec, mean_recall])\n",
        "            mean_conf_matrices.append(mean_conf)\n",
        "            LR_conf_matrices.append(LR_conf)\n",
        "\n",
        "        # Take means and add to dict.\n",
        "        results[batch_size] = [np.mean(train_scores, axis=0), np.mean(val_scores, axis=0), \n",
        "                               np.mean(test_scores, axis=0), np.mean(test_agg, axis=0)]\n",
        "\n",
        "        # Save confusion matrix as mean of all folds.\n",
        "        conf_matrix(combination, batch_size, np.mean(LR_conf_matrices, axis=0), 'LR')   \n",
        "        conf_matrix(combination, batch_size, np.mean(mean_conf_matrices, axis=0), 'mean')\n",
        "        \n",
        "        print('-------------------------------')\n",
        "        print('{}-fold validation has been executed, mean LR_prec, LR_recall, mean_prec, mean_recall: {}'.\n",
        "              format(n_folds, np.mean(test_agg, axis=0)))\n",
        "        \n",
        "        \n",
        "    # Save results for model type and all batch size combinations.\n",
        "    np.save('/content/gdrive/My Drive/Colab Notebooks/data/btc/results/{}_{}_{}_{}'.format(combination[0], combination[1], int(combination[2]), int(combination[3])), np.array(results))\n",
        "    print('Saved results')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------\n",
            "-------------------------------\n",
            "STARTING NEW COMBINATION ['keywords_tfidf', 'binary', False, False]\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "Starting training for new batch size 32\n",
            "-------------------------------\n",
            "-------------------------------\n",
            "Starting training for new fold\n",
            "-------------------------------\n",
            "Epoch [1/35], Step [1/287], Loss: 0.6927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/35], Step [96/287], Loss: 0.6922\n",
            "Epoch [1/35], Step [191/287], Loss: 0.6919\n",
            "Epoch [1/35], Step [286/287], Loss: 0.6912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/35], val MAP/acc: [0.0, 0.0], val loss: 0.6878663301467896\n",
            "At end of epoch, average (training) loss: 0.6911812424659729, score: [0.5093457943925234, 0.02687376725838264] \n",
            "At end of epoch, average (validation) loss: 0.6878663301467896, score: [0.0, 0.0] \n",
            "Epoch [2/35], Step [1/287], Loss: 0.6892\n",
            "Epoch [2/35], Step [96/287], Loss: 0.6879\n",
            "Epoch [2/35], Step [191/287], Loss: 0.6884\n",
            "Epoch [2/35], Step [286/287], Loss: 0.6878\n",
            "Epoch [2/35], val MAP/acc: [0.7142857142857143, 0.0051813471502590676], val loss: 0.6842732429504395\n",
            "At end of epoch, average (training) loss: 0.687824010848999, score: [0.35, 0.001726689689195856] \n",
            "At end of epoch, average (validation) loss: 0.6842732429504395, score: [0.7142857142857143, 0.0051813471502590676] \n",
            "Validation loss decreased (inf --> 0.684273).\n",
            "Epoch [3/35], Step [1/287], Loss: 0.6920\n",
            "Epoch [3/35], Step [96/287], Loss: 0.6789\n",
            "Epoch [3/35], Step [191/287], Loss: 0.6813\n",
            "Epoch [3/35], Step [286/287], Loss: 0.6827\n",
            "Epoch [3/35], val MAP/acc: [1.0, 0.00205761316872428], val loss: 0.6829509735107422\n",
            "At end of epoch, average (training) loss: 0.682714581489563, score: [0.531055900621118, 0.04219096965210955] \n",
            "At end of epoch, average (validation) loss: 0.6829509735107422, score: [1.0, 0.00205761316872428] \n",
            "Validation loss decreased (0.684273 --> 0.682951).\n",
            "Epoch [4/35], Step [1/287], Loss: 0.6608\n",
            "Epoch [4/35], Step [96/287], Loss: 0.6377\n",
            "Epoch [4/35], Step [191/287], Loss: 0.6427\n",
            "Epoch [4/35], Step [286/287], Loss: 0.6458\n",
            "Epoch [4/35], val MAP/acc: [0.44756554307116103, 0.246900826446281], val loss: 0.6900669932365417\n",
            "At end of epoch, average (training) loss: 0.6459493041038513, score: [0.6109560362875087, 0.4318125770653514] \n",
            "At end of epoch, average (validation) loss: 0.6900669932365417, score: [0.44756554307116103, 0.246900826446281] \n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [5/35], Step [1/287], Loss: 0.6008\n",
            "Epoch [5/35], Step [96/287], Loss: 0.5358\n",
            "Epoch [5/35], Step [191/287], Loss: 0.5571\n",
            "Epoch [5/35], Step [286/287], Loss: 0.5688\n",
            "Epoch [5/35], val MAP/acc: [0.4573529411764706, 0.31995884773662553], val loss: 0.7357897758483887\n",
            "At end of epoch, average (training) loss: 0.5683287382125854, score: [0.6709183673469388, 0.5838677849037988] \n",
            "At end of epoch, average (validation) loss: 0.7357897758483887, score: [0.4573529411764706, 0.31995884773662553] \n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [6/35], Step [1/287], Loss: 0.6258\n",
            "Epoch [6/35], Step [96/287], Loss: 0.4755\n",
            "Epoch [6/35], Step [191/287], Loss: 0.4906\n",
            "Epoch [6/35], Step [286/287], Loss: 0.5019\n",
            "Epoch [6/35], val MAP/acc: [0.44216867469879517, 0.37913223140495866], val loss: 0.8728134632110596\n",
            "At end of epoch, average (training) loss: 0.5022984743118286, score: [0.7148362235067437, 0.6406018746916625] \n",
            "At end of epoch, average (validation) loss: 0.8728134632110596, score: [0.44216867469879517, 0.37913223140495866] \n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch     5: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch [7/35], Step [1/287], Loss: 0.4747\n",
            "Epoch [7/35], Step [96/287], Loss: 0.4080\n",
            "Epoch [7/35], Step [191/287], Loss: 0.4068\n",
            "Epoch [7/35], Step [286/287], Loss: 0.4065\n",
            "Epoch [7/35], val MAP/acc: [0.4432748538011696, 0.38871794871794874], val loss: 1.200559377670288\n",
            "At end of epoch, average (training) loss: 0.4062266945838928, score: [0.7819280417467729, 0.7020961775585697] \n",
            "At end of epoch, average (validation) loss: 1.200559377670288, score: [0.4432748538011696, 0.38871794871794874] \n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch [8/35], Step [1/287], Loss: 0.2972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C3HJtXIG39gp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Process final results"
      ]
    },
    {
      "metadata": {
        "id": "20IGXoXG4Aul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_str = '/content/gdrive/My Drive/Colab Notebooks/data/btc/results/'\n",
        "files = os.listdir(base_str)\n",
        "\n",
        "results = {}\n",
        "for file in files:\n",
        "    results[file.strip('.npy')] = np.load(base_str + file)[()] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3gCXakzV41PI",
        "colab_type": "code",
        "outputId": "7ae566f6-0605-4552-eafa-687dd2323d61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "# method, batch_size, train_map, val_map, test_map, test_lr, test_mean_pred.\n",
        "columns = ['Experiment', 'Batch_size', 'type', 'btc_meta_data', 'user_meta_data', 'Train_MAP', 'Val_MAP', 'Test_MAP', 'MAP_Test_LR', 'MAP_test_mean']\n",
        "final = []\n",
        "for k in results.keys():\n",
        "    for batch_size in [32, 64, 128, 256, 512]:\n",
        "        k_replaced = k.replace('keywords_', '').replace('_count_', '').replace('_binary_', '')[:-3]\n",
        "\n",
        "        t = 'Binary' if 'binary' in k else 'Count'\n",
        "        b = 'Yes' if k[-3] == '1' else 'No'\n",
        "        u = 'Yes' if k[-1] == '1' else 'No'\n",
        "        \n",
        "        temp = [k_replaced, batch_size, t, b, u]\n",
        "        temp.extend(results[k][batch_size])\n",
        "        # train_map, val_map, test_map, test_lr, test_mean_pred.\n",
        "        final.append(temp)\n",
        "        \n",
        "# Booleans being the usage of btc meta-data and user meta-data or not.\n",
        "final = pd.DataFrame.from_records(final, columns=columns)\n",
        "final['Experiment'][final['Experiment'] == 'tfidf_p'] = 'Partial TF-IDF'\n",
        "final['Experiment'][final['Experiment'] == 'tfidf'] = 'TF-IDF'\n",
        "final = final[final['Experiment'] != 'custom_features']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NHGUiZI55ia5",
        "colab_type": "code",
        "outputId": "876b2d88-bbf8-4a7c-ec2a-c11d9eda4147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "# Personally went through all combinations of experiments, types, btc meta data and user meta data to find the max scores.\n",
        "a = final[(final['Experiment'] == 'Partial TF-IDF') & (final['type'] == 'Count') \n",
        "      & (final['btc_meta_data'] == 'Yes') & (final['user_meta_data'] == 'Yes')]\n",
        "\n",
        "a.sort_values(['MAP_Test_LR', 'MAP_test_mean'], ascending=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Experiment</th>\n",
              "      <th>Batch_size</th>\n",
              "      <th>type</th>\n",
              "      <th>btc_meta_data</th>\n",
              "      <th>user_meta_data</th>\n",
              "      <th>Train_MAP</th>\n",
              "      <th>Val_MAP</th>\n",
              "      <th>Test_MAP</th>\n",
              "      <th>MAP_Test_LR</th>\n",
              "      <th>MAP_test_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Partial TF-IDF</td>\n",
              "      <td>128</td>\n",
              "      <td>Count</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.452274</td>\n",
              "      <td>0.452363</td>\n",
              "      <td>0.427581</td>\n",
              "      <td>0.427886</td>\n",
              "      <td>0.428822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Partial TF-IDF</td>\n",
              "      <td>256</td>\n",
              "      <td>Count</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.451999</td>\n",
              "      <td>0.452126</td>\n",
              "      <td>0.425247</td>\n",
              "      <td>0.423729</td>\n",
              "      <td>0.431415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Partial TF-IDF</td>\n",
              "      <td>64</td>\n",
              "      <td>Count</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.452011</td>\n",
              "      <td>0.452011</td>\n",
              "      <td>0.427307</td>\n",
              "      <td>0.423729</td>\n",
              "      <td>0.415532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Partial TF-IDF</td>\n",
              "      <td>32</td>\n",
              "      <td>Count</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.452015</td>\n",
              "      <td>0.452081</td>\n",
              "      <td>0.427307</td>\n",
              "      <td>0.423729</td>\n",
              "      <td>0.408286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Partial TF-IDF</td>\n",
              "      <td>512</td>\n",
              "      <td>Count</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.452017</td>\n",
              "      <td>0.452002</td>\n",
              "      <td>0.421082</td>\n",
              "      <td>0.413793</td>\n",
              "      <td>0.404960</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Experiment  Batch_size   type btc_meta_data user_meta_data  Train_MAP  \\\n",
              "97  Partial TF-IDF         128  Count           Yes            Yes   0.452274   \n",
              "98  Partial TF-IDF         256  Count           Yes            Yes   0.451999   \n",
              "96  Partial TF-IDF          64  Count           Yes            Yes   0.452011   \n",
              "95  Partial TF-IDF          32  Count           Yes            Yes   0.452015   \n",
              "99  Partial TF-IDF         512  Count           Yes            Yes   0.452017   \n",
              "\n",
              "     Val_MAP  Test_MAP  MAP_Test_LR  MAP_test_mean  \n",
              "97  0.452363  0.427581     0.427886       0.428822  \n",
              "98  0.452126  0.425247     0.423729       0.431415  \n",
              "96  0.452011  0.427307     0.423729       0.415532  \n",
              "95  0.452081  0.427307     0.423729       0.408286  \n",
              "99  0.452002  0.421082     0.413793       0.404960  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "pUXnWRX8fpRb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trading strategy"
      ]
    },
    {
      "metadata": {
        "id": "8Q-u5FZdicm5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Perform forward pass on training set to generate data for Logistic Regression and majority vote"
      ]
    },
    {
      "metadata": {
        "id": "h2cAcEgQibwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=full_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "train_lr = None\n",
        "\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    for images, labels, user_data, dates, btc_meta_data in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        user_data = user_data.to(device)\n",
        "        btc_meta_data = btc_meta_data.to(device)\n",
        "        outputs = model(images, user_data, btc_meta_data)\n",
        "        predicted = torch.max(outputs, 1)[1].data.cpu().numpy()\n",
        "\n",
        "        if multi_class:\n",
        "            labels = torch.max(labels, 1)[1].data.cpu().numpy()\n",
        "            temp = np.dstack((labels, user_data.data.cpu().numpy()[:,-1],\n",
        "                           dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "            temp = np.hstack((outputs, temp))\n",
        "            \n",
        "        else:\n",
        "            outputs = np.array([x[0] for x in outputs.data.cpu().numpy()])\n",
        "            temp = np.dstack((outputs, labels, \n",
        "                               user_data.data.cpu().numpy()[:,-1],\n",
        "                               dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "        if train_lr is None:\n",
        "            train_lr = temp\n",
        "        else:\n",
        "            train_lr = np.concatenate((train_lr, temp), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fR4_etFQ79j7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "test_lr = None\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, user_data, dates, btc_meta_data in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        user_data = user_data.to(device)\n",
        "        btc_meta_data = btc_meta_data.to(device)\n",
        "        outputs = model(images, user_data, btc_meta_data)\n",
        "        predicted = torch.max(outputs, 1)[1].data.cpu().numpy()\n",
        "\n",
        "        if multi_class:\n",
        "            labels = torch.max(labels, 1)[1].data.cpu().numpy()\n",
        "            temp = np.dstack((labels, user_data.data.cpu().numpy()[:,-1],\n",
        "                           dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "            temp = np.hstack((outputs, temp))\n",
        "        else:\n",
        "            outputs = np.array([x[0] for x in outputs.data.cpu().numpy()])\n",
        "            temp = np.dstack((outputs, labels, \n",
        "                               user_data.data.cpu().numpy()[:,-1],\n",
        "                               dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "        \n",
        "        if test_lr is None:\n",
        "            test_lr = temp\n",
        "        else:\n",
        "            test_lr = np.concatenate((test_lr, temp), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WuPr_kfWs9Ma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create dataframe for LR and MV"
      ]
    },
    {
      "metadata": {
        "id": "ntUFSq78tABD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "if multi_class:\n",
        "    train_df = pd.DataFrame({'output1':train_lr[:,0], 'output2':train_lr[:,1], 'output3':train_lr[:,2], 'true':train_lr[:,3], \n",
        "                                   'user':train_lr[:,4], 'date':train_lr[:,5], 'pred_max':train_lr[:,6]})\n",
        "    test_df = pd.DataFrame({'output1':test_lr[:,0], 'output2':test_lr[:,1], 'output3':test_lr[:,2], 'true':test_lr[:,3], \n",
        "                                   'user':test_lr[:,4], 'date':test_lr[:,5], 'pred_max':test_lr[:,6]})\n",
        "else:\n",
        "    train_df = pd.DataFrame({'output':train_lr[:,0],'true':train_lr[:,1], \n",
        "                                       'user':train_lr[:,2], 'date':train_lr[:,3], 'pred_max':train_lr[:,4]})\n",
        "    test_df = pd.DataFrame({'output':test_lr[:,0],'true':test_lr[:,1], \n",
        "                                       'user':test_lr[:,2], 'date':test_lr[:,3], 'pred_max':test_lr[:,4]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dKxr35Scu6MA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Reset vars that we do not need anymore.\n",
        "# train_classes, train_user_data, train_matrices, train_dates, train_btc_meta_data, train_classes_t = None, None, None, None, None, None\n",
        "# test_classes, test_user_data, test_matrices, test_dates, test_btc_meta_data, test_classes_t = None, None, None, None, None, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6yyRUuOVQKn8",
        "colab_type": "code",
        "outputId": "94d8d124-f5f8-4595-e237-c5dceb06ae40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "cell_type": "code",
      "source": [
        "import seaborn \n",
        "\n",
        "seaborn.distplot(train_df['output'][train_df['date'] == 17548.0], bins=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f205743cdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAEGCAYAAABB8K+FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f205b0b6d68>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "q7cc1r0iv68h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Thresholding training (only for binary classification)"
      ]
    },
    {
      "metadata": {
        "id": "97z4tOppv7Ec",
        "colab_type": "code",
        "outputId": "bfb97e50-633a-4d0a-bf75-92001c34d40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "data_MV = train_df[['date', 'output', 'true', 'pred_max']]\n",
        "cnt_MV = 0\n",
        "cnt_max = 0\n",
        "pairs = []\n",
        "dates = data_MV['date'].unique()\n",
        "score = 0\n",
        "for i in range(20, 80):\n",
        "    threshold = i / 100\n",
        "    cnt_MV = 0\n",
        "    cnt_max = 0\n",
        "    pairs = []\n",
        "    for date in dates:\n",
        "        temp = data_MV[data_MV['date'] == date]\n",
        "        y_true = temp['true'].unique()[0]\n",
        "        MV = temp['pred_max'].value_counts().index[0]\n",
        "\n",
        "        max_all = temp['output'].mean()\n",
        "        max_all = 1 if max_all >= threshold else 0\n",
        "\n",
        "        if y_true == MV:\n",
        "            cnt_MV += 1\n",
        "        if y_true == max_all:\n",
        "            cnt_max += 1\n",
        "        else:\n",
        "            pairs.append([max_all, y_true])\n",
        "    new_score = cnt_max / len(dates)\n",
        "    if new_score > score:\n",
        "        best_thresh = threshold\n",
        "        score = new_score\n",
        "print('The best score was {} with threshold {}'.format(score, best_thresh))\n",
        "print('Performing MV resulted in an acc of: {}'.format(cnt_MV / len(dates)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best score was 0.9921722113502935 with threshold 0.45\n",
            "Performing MV resulted in an acc of: 0.5362035225048923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CFOK6CrUYuMm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use best threshold for training set on test set to measure performance OR get mean per date for multi-class"
      ]
    },
    {
      "metadata": {
        "id": "4LQEBa6ZYtvF",
        "colab_type": "code",
        "outputId": "b3b59e92-7465-4fdb-adae-0601d92e4b9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1374
        }
      },
      "cell_type": "code",
      "source": [
        "#data_MV = test_df[['date', 'output', 'true', 'pred_max']]\n",
        "cnt_max = 0\n",
        "pairs = []\n",
        "dates = test_df['date'].unique()\n",
        "y_test = []\n",
        "y_pred = []\n",
        "for date in dates:\n",
        "    temp = test_df[test_df['date'] == date]\n",
        "    y_true = temp['true'].unique()[0]\n",
        "    \n",
        "    MV = temp['pred_max'].value_counts().index[0]\n",
        "    \n",
        "    if multi_class:\n",
        "        # Mean per column\n",
        "        max_out = temp[['output1', 'output2', 'output3']].mean(axis=0).values\n",
        "        max_all = np.argmax(max_out)\n",
        "    else:\n",
        "        max_out = temp['output'].mean()\n",
        "        max_all = 1 if max_out > best_thresh else 0\n",
        "    \n",
        "    y_test.append(y_true)\n",
        "    y_pred.append(max_all)\n",
        "    if y_true == max_all:\n",
        "        cnt_max += 1\n",
        "        print('correct', y_true, max_out)\n",
        "    else:\n",
        "        print('wrong', y_true, max_out)\n",
        "        pairs.append([max_all, y_true])\n",
        "\n",
        "average_precision_score(y_test, y_pred_prob)\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "class_names=[0,1] # name  of classes\n",
        "fig, ax = plt.subplots()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "\n",
        "# create heatmap\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "correct 0.0 0.4212333805189865\n",
            "correct 0.0 0.4249274776266525\n",
            "correct 0.0 0.4045314504047116\n",
            "correct 0.0 0.41927296124764685\n",
            "correct 0.0 0.4276204223051895\n",
            "correct 0.0 0.39052849712646764\n",
            "correct 0.0 0.3985815997422369\n",
            "correct 0.0 0.44702020708217144\n",
            "correct 0.0 0.4319154014317533\n",
            "wrong 0.0 0.45230078644903965\n",
            "wrong 1.0 0.4081326952815233\n",
            "correct 0.0 0.4235356306579404\n",
            "correct 0.0 0.4303461099854732\n",
            "wrong 1.0 0.44127278593493674\n",
            "correct 0.0 0.4188311281602244\n",
            "wrong 1.0 0.40785386330265555\n",
            "correct 0.0 0.442664228676432\n",
            "wrong 1.0 0.421235700153253\n",
            "correct 0.0 0.4441434705083503\n",
            "wrong 1.0 0.437079162125044\n",
            "wrong 1.0 0.4329614365874132\n",
            "correct 1.0 0.45418644773061\n",
            "correct 0.0 0.43514654703470396\n",
            "correct 1.0 0.45495849962628143\n",
            "correct 0.0 0.4067084078009907\n",
            "correct 0.0 0.42949879106975913\n",
            "wrong 1.0 0.4441632110336641\n",
            "correct 0.0 0.39626939624304336\n",
            "correct 0.0 0.41901625769335865\n",
            "correct 0.0 0.4315545199504654\n",
            "wrong 1.0 0.4497457687765849\n",
            "wrong 1.0 0.4471274614897247\n",
            "correct 0.0 0.4462776312569596\n",
            "wrong 0.0 0.464332412754362\n",
            "wrong 1.0 0.3908235147455079\n",
            "wrong 0.0 0.4664307588161865\n",
            "correct 0.0 0.4221105260917544\n",
            "wrong 0.0 0.47352216064363645\n",
            "correct 0.0 0.44899716906714254\n",
            "wrong 1.0 0.4427646261856941\n",
            "wrong 1.0 0.4221013990035275\n",
            "wrong 1.0 0.4204092130866536\n",
            "wrong 1.0 0.43532831145016854\n",
            "correct 0.0 0.4351293155363861\n",
            "correct 1.0 0.4718458667305717\n",
            "correct 0.0 0.42523980583591287\n",
            "wrong 1.0 0.42591884756425213\n",
            "wrong 0.0 0.4658009595939693\n",
            "wrong 1.0 0.41694194671520174\n",
            "correct 0.0 0.4357985627036298\n",
            "wrong 1.0 0.42622942650867873\n",
            "wrong 0.0 0.4603677188214047\n",
            "wrong 1.0 0.44319362009017965\n",
            "correct 0.0 0.4420134282886458\n",
            "correct 1.0 0.4630360366431956\n",
            "wrong 1.0 0.42031418998456616\n",
            "wrong 0.0 0.45784564346685797\n",
            "wrong 1.0 0.4312363068901853\n",
            "wrong 1.0 0.4451363298417834\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 257.44, 'Predicted label')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEzCAYAAAAIFcVFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxZj10wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "0w00r7I1p0Ck",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create dataset LR multiclass"
      ]
    },
    {
      "metadata": {
        "id": "lFeVKtdnp75p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "We create a matrix with all zeroes and replace the user predictions if there\n",
        "are any. If there's none, then we simply leave it at zero.\n",
        "\n",
        "As we have three predictions per user/day, our matrix is 3x as wide as\n",
        "in our binary case, ex:\n",
        "\n",
        "[user_1_pred_c1, user_1_pred_c2, user_1_pred_c3, user_2_pred_c1, ..]\n",
        "'''\n",
        "\n",
        "\n",
        "# # Get all users\n",
        "train_users = train_df['user'].unique()\n",
        "test_users = test_df['user'].unique()\n",
        "# print(train_users)\n",
        "# print(test_users)\n",
        "users = np.unique(np.concatenate((train_users, test_users)))\n",
        "# # Assign users a unique id from 0 to n unique ids, \n",
        "# # create a dictionairy using training data and re-use during test time.\n",
        "user_ids_lr = {}\n",
        "cnt = 0\n",
        "for i, v in enumerate(users):\n",
        "    if cnt == 0:\n",
        "        add = 0\n",
        "    elif cnt == 1:\n",
        "        add = 2\n",
        "    else: \n",
        "        add = 3\n",
        "    cnt += 1\n",
        "    \n",
        "    user_ids_lr[v] = i + add\n",
        "\n",
        "# Loop over dates, check if user has prediction\n",
        "X_train = None\n",
        "y_train = []\n",
        "cnt = 0\n",
        "for date in train_df['date'].unique():\n",
        "    cnt += 1\n",
        "    temp = train_df[train_df['date'] == date]\n",
        "    arr = np.zeros(len(users) * 3)\n",
        "    for row in temp.itertuples():\n",
        "        usr = row[7]\n",
        "        pred1 = row[2]\n",
        "        pred2 = row[3]\n",
        "        pred3 = row[4]\n",
        "        lbl = row[6]\n",
        "        arr[user_ids_lr[usr]] = pred1\n",
        "        arr[user_ids_lr[usr]+1] = pred2\n",
        "        arr[user_ids_lr[usr]+2] = pred3\n",
        "        \n",
        "    y_train.append([lbl])\n",
        "\n",
        "    if X_train is None:\n",
        "        X_train = arr\n",
        "    else:\n",
        "        X_train = np.vstack((X_train, arr))\n",
        "\n",
        "y_train = np.array(y_train)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hSwvOrgUuArW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loop over dates, check if user has prediction\n",
        "X_test = None\n",
        "y_test = []\n",
        "cnt = 0\n",
        "for date in test_df['date'].unique():\n",
        "    cnt += 1\n",
        "    temp = test_df[test_df['date'] == date]\n",
        "    arr = np.zeros(len(users) * 3)\n",
        "    for row in temp.itertuples():\n",
        "        usr = row[7]\n",
        "        pred1 = row[2]\n",
        "        pred2 = row[3]\n",
        "        pred3 = row[4]\n",
        "        lbl = row[6]\n",
        "        arr[user_ids_lr[usr]] = pred1\n",
        "        arr[user_ids_lr[usr]+1] = pred2\n",
        "        arr[user_ids_lr[usr]+2] = pred3\n",
        "        \n",
        "    y_test.append([lbl])\n",
        "    if X_test is None:\n",
        "        X_test = arr\n",
        "    else:\n",
        "        X_test = np.vstack((X_test, arr))\n",
        "\n",
        "y_test = np.array(y_test)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qHZlLPAiFdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create dataset LR binary class"
      ]
    },
    {
      "metadata": {
        "id": "0fEtBHStVFSy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "For logistic regression we'd the following input:\n",
        "[user_pred_1, .... user_pred_n]\n",
        "\n",
        "We create a matrix with all zeroes and replace the user predictions if there\n",
        "are any. If there's none, then we simply leave it at zero.\n",
        "\n",
        "So we get a matrix per day with predictions for all users.\n",
        "'''\n",
        "\n",
        "\n",
        "# # Get all users\n",
        "train_users = train_df['user'].unique()\n",
        "test_users = test_df['user'].unique()\n",
        "\n",
        "users = np.concatenate((train_users, test_users))\n",
        "user_ids_lr = {}\n",
        "for i, v in enumerate(users):\n",
        "    user_ids_lr[v] = i\n",
        "\n",
        "# Loop over dates, check if user has prediction\n",
        "X_train = None\n",
        "y_train = []\n",
        "cnt = 0\n",
        "for date in train_df['date'].unique():\n",
        "    cnt += 1\n",
        "    temp = train_df[train_df['date'] == date]\n",
        "    arr = np.zeros(len(users))\n",
        "    for row in temp.itertuples():\n",
        "        usr = row[5]\n",
        "        pred = row[2]\n",
        "        lbl = row[4]\n",
        "        arr[user_ids_lr[usr]] = pred\n",
        "    y_train.append([lbl])\n",
        "    if X_train is None:\n",
        "        X_train = arr\n",
        "    else:\n",
        "        X_train = np.vstack((X_train, arr))\n",
        "\n",
        "y_train = np.array(y_train)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3J0LSZwTv9s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Loop over dates, check if user has prediction\n",
        "X_test = None\n",
        "y_test = []\n",
        "\n",
        "for date in test_df['date'].unique():\n",
        "    cnt += 1\n",
        "    temp = test_df[test_df['date'] == date]\n",
        "    arr = np.zeros(len(users))\n",
        "    for row in temp.itertuples():\n",
        "        usr = row[5]\n",
        "        pred = row[2]\n",
        "        lbl = row[4]\n",
        "        try:\n",
        "            arr[user_ids_lr[usr]] = pred\n",
        "        except:\n",
        "            #print('User {} not found'.format(usr))\n",
        "            cnt_n += 1\n",
        "    y_test.append([lbl])\n",
        "    if X_test is None:\n",
        "        X_test = arr\n",
        "    else:\n",
        "        X_test = np.vstack((X_test, arr))\n",
        "\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNimcQC82Oog",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Perform Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "OXSs7t5-wC67",
        "colab_type": "code",
        "outputId": "fa63ebad-21c0-424c-e57b-f29f31c79448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# instantiate the model (using the default parameters)\n",
        "logreg = LogisticRegression(C=0.8)\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_train)\n",
        "\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_train, y_pred)\n",
        "cnf_matrix\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[25,  9],\n",
              "       [18,  7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "metadata": {
        "id": "0fC8rTFFTEX0",
        "colab_type": "code",
        "outputId": "a365a47c-3216-497f-e73c-61ca972234f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "# Predict on test\n",
        "y_pred = logreg.predict(X_test)\n",
        "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(average_precision_score(y_test, y_pred_prob))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.45718148070639314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[25,  9],\n",
              "       [18,  7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "D3DAOND83O0a",
        "colab_type": "code",
        "outputId": "e140f391-99b4-446e-d9e9-84242621d5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix\n",
        "\n",
        "class_names=[0,1] # name  of classes\n",
        "fig, ax = plt.subplots()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "\n",
        "\n",
        "\n",
        "# create heatmap\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 257.44, 'Predicted label')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEzCAYAAAAIFcVFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}