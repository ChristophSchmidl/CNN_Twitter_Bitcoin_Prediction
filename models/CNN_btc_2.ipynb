{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_btc_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "fYM9pGY3PNEM",
        "Q3yq8bytPUa4",
        "LHfXJv19PUbK",
        "R8ZUFjzAPUba",
        "Ci9PWyjRPUbW",
        "j_Sg1xu6LxqK",
        "C-QvA-Tif6Ea",
        "CV-R2o0ox4fg",
        "7qHZlLPAiFdL",
        "VNimcQC82Oog"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickvanhulst/twitter_bitcoin_analysis/blob/master/models/CNN_btc_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SBj8vZYdPqLi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "QCCVsRY6PpRK",
        "colab_type": "code",
        "outputId": "752cdbcf-0ee2-4f3a-d329-c41e717abd33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "!pip install git+git://github.com/mickvanhulst/livelossplot.git --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5b2f8000 @  0x7eff15d412a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "0.4.0\n",
            "True\n",
            "Collecting git+git://github.com/mickvanhulst/livelossplot.git\n",
            "  Cloning git://github.com/mickvanhulst/livelossplot.git to /tmp/pip-req-build-xhy7yijq\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.2.2) (2.1.2)\n",
            "Requirement already satisfied, skipping upgrade: notebook in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.2.2) (5.2.2)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (2018.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.2.2) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (5.4.0)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.3.2)\n",
            "Requirement already satisfied, skipping upgrade: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (5.2.3)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.5.3)\n",
            "Requirement already satisfied, skipping upgrade: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.2.2) (4.6.1)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (1.4.2)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (0.2.3)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (0.5.0)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.2.2) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->livelossplot==0.2.2) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->livelossplot==0.2.2) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->livelossplot==0.2.2) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->livelossplot==0.2.2) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->livelossplot==0.2.2) (17.0.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook->livelossplot==0.2.2) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.2.2) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (1.0.15)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (40.6.2)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (4.6.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot==0.2.2) (0.1.7)\n",
            "Building wheels for collected packages: livelossplot\n",
            "  Running setup.py bdist_wheel for livelossplot ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-id3un99d/wheels/73/e2/09/79ee9f5b0be18dc9c6ee17f0dc181708c1cf9513c70053ca92\n",
            "Successfully built livelossplot\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fYM9pGY3PNEM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test if GPU enabled"
      ]
    },
    {
      "metadata": {
        "id": "1H4KKVizOwi7",
        "colab_type": "code",
        "outputId": "daa31dff-6174-4737-da3f-b7af8e1f3ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "N0EyqtdYPUa1"
      },
      "cell_type": "markdown",
      "source": [
        "# Load packages"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OnJQN5ZkPUa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9696a377-665b-4918-816d-2ff752984c15"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as utils\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Q3yq8bytPUa4"
      },
      "cell_type": "markdown",
      "source": [
        "# Mount Drive and open matrices"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "39963b47-b53d-45bb-cc8b-62f91c95dd92",
        "id": "yaEOMZRMPUa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kbe6Xz6Jdwyu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tryint(s):\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return s\n",
        "     \n",
        "def alphanum_key(s):\n",
        "    \"\"\" Turn a string into a list of string and number chunks.\n",
        "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
        "    \"\"\"\n",
        "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
        "\n",
        "def sort_nicely(l):\n",
        "    \"\"\" Sort the given list in the way that humans expect.\n",
        "    \n",
        "    amazing <3 RegExp\n",
        "    \"\"\"\n",
        "    l.sort(key=alphanum_key)\n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9dW6gscJPUa_",
        "outputId": "fb176252-06ca-410e-8cb2-8ce2029f3e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_type = ['keywords_tfidf', 'keywords_clusters'][1]\n",
        "mode = ['count', 'binary'][1]\n",
        "train, test = None, None\n",
        "train_classes, train_user_data, train_matrices, train_dates = None, None, None, None\n",
        "\n",
        "base_str = '/content/gdrive/My Drive/Colab Notebooks/data/btc/{}/{}/train/'.format(model_type, mode)\n",
        "files = os.listdir(base_str)\n",
        "for train_file in sort_nicely(files):\n",
        "    if 'c_up' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_classes is None:\n",
        "            train_classes = temp\n",
        "        else:\n",
        "            train_classes = np.concatenate((train_classes, temp), axis=0)\n",
        "    elif 'md_up' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_user_data is None:\n",
        "            train_user_data = temp\n",
        "        else:\n",
        "            train_user_data = np.vstack((train_user_data, temp))\n",
        "    elif 'dp_up' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_matrices is None:\n",
        "            train_matrices = temp\n",
        "        else:\n",
        "            train_matrices = np.vstack((train_matrices, temp))\n",
        "    elif 'dt_up' in train_file:\n",
        "        temp = np.load(base_str + train_file)\n",
        "        if train_dates is None:\n",
        "            train_dates = temp\n",
        "        else:\n",
        "            train_dates = np.concatenate((train_dates, temp), axis=0)\n",
        "\n",
        "test_classes, test_user_data, test_matrices, test_dates = None, None, None, None\n",
        "base_str = '/content/gdrive/My Drive/Colab Notebooks/data/btc/{}/{}/test/'.format(model_type, mode)\n",
        "\n",
        "for test_file in sort_nicely(os.listdir(base_str)):\n",
        "    if 'c_up' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_classes is None:\n",
        "            test_classes = temp\n",
        "        else:\n",
        "            test_classes = np.concatenate((test_classes, temp), axis=0)\n",
        "    elif 'md_up' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_user_data is None:\n",
        "            test_user_data = temp\n",
        "        else:\n",
        "            test_user_data = np.vstack((test_user_data, temp))\n",
        "    elif 'dp_up' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_matrices is None:\n",
        "            test_matrices = temp\n",
        "        else:\n",
        "            test_matrices = np.vstack((test_matrices, temp))\n",
        "    elif 'dt_up' in test_file:\n",
        "        temp = np.load(base_str + test_file)\n",
        "        if test_dates is None:\n",
        "            test_dates = temp\n",
        "        else:\n",
        "            test_dates = np.concatenate((test_dates, temp), axis=0)\n",
        "            \n",
        "print('Test shapes: ', test_classes.shape, test_user_data.shape, \n",
        "      test_matrices.shape, test_dates.shape)\n",
        "\n",
        "print('Train shapes: ', train_classes.shape, train_user_data.shape, \n",
        "      train_matrices.shape, train_dates.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test shapes:  (22695,) (22695, 8) (22695, 200, 7) (22695,)\n",
            "Train shapes:  (144578,) (144578, 8) (144578, 200, 7) (144578,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HGXopB4KnpHH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Re-assign tokens for train, set unknown test to zero.\n",
        "# Get all users\n",
        "unique_users = np.unique(train_user_data[:,-1])\n",
        "\n",
        "# Assign users a unique id from 0 to n unique ids, \n",
        "# create a dictionairy using training data and re-use during test time.\n",
        "# If users do not exist in training dict, then we assign a userId that\n",
        "# doesn't occur anywhere, so that it doesn't influence our predictions.\n",
        "# Normalize the userIds before hand.\n",
        "user_ids = {}\n",
        "for i, v in enumerate(unique_users):\n",
        "    user_ids[v] = (i+1) / (len(unique_users))\n",
        "\n",
        "for i, v in enumerate(train_user_data):\n",
        "    train_user_data[i,-1] = user_ids[v[-1]]\n",
        "    \n",
        "    \n",
        "for i, v in enumerate(test_user_data):\n",
        "    try:\n",
        "        test_user_data[i,-1] = user_ids[v[-1]]\n",
        "    except:\n",
        "        test_user_data[i,-1] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TPGKHnAwo-Sg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Normalize\n",
        "train_user_data[:,:-1] = train_user_data[:,:-1] / train_user_data[:,:-1].max(axis=0)\n",
        "test_user_data[:,:-1] = test_user_data[:,:-1] / train_user_data[:,:-1].max(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66UXTexmLWt",
        "colab_type": "code",
        "outputId": "6c7164c4-9998-4fbb-f5a9-bbc4c0c529ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cnt_n = 0\n",
        "cnt_y = 0\n",
        "for i in np.unique(test_user_data[:,-1]):\n",
        "    if i in train_user_data[:,-1]:\n",
        "        cnt_y += 1\n",
        "    else:\n",
        "        cnt_n += 1\n",
        "        \n",
        "cnt_n, cnt_y, train_user_data.min(), train_user_data.mean()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 5929, 0.0, 0.1811176429511396)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LHfXJv19PUbK"
      },
      "cell_type": "markdown",
      "source": [
        "# Device config\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Vz5EK_lsPUbK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "R8ZUFjzAPUba"
      },
      "cell_type": "markdown",
      "source": [
        "# Process data and set parameters"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wA1Jtv8wPUbb",
        "outputId": "43871d14-bc93-4e5e-d47e-f28f739edd31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 50\n",
        "num_classes = 1\n",
        "batch_size = 4\n",
        "learning_rate = 0.1\n",
        "size_user_info = 8\n",
        "\n",
        "print('''Occurences of zero/one vs total amount of \n",
        "classes for train and test set:\n",
        "-------------------------------''')\n",
        "for d in [train_classes, test_classes]:\n",
        "    occ_zero = len([x for x in d if x == 0])\n",
        "    occ_one = len(d) - occ_zero\n",
        "    print(occ_one, occ_zero, len(d))\n",
        "\n",
        "tensor_1 = torch.from_numpy(train_matrices)\n",
        "tensor_2 = torch.from_numpy(train_classes)\n",
        "tensor_3 = torch.from_numpy(train_user_data.astype(float))\n",
        "tensor_4 = torch.from_numpy(train_dates.astype(float))\n",
        "full_dataset = torch.utils.data.TensorDataset(tensor_1, tensor_2, tensor_3, tensor_4)\n",
        "\n",
        "tensor_1 = torch.from_numpy(test_matrices)\n",
        "tensor_2 = torch.from_numpy(test_classes)\n",
        "tensor_3 = torch.from_numpy(test_user_data.astype(float))\n",
        "tensor_4 = torch.from_numpy(test_dates.astype(float))\n",
        "\n",
        "test = torch.utils.data.TensorDataset(tensor_1, tensor_2, tensor_3, tensor_4)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test,\n",
        "                                          batch_size=batch_size,\n",
        "                                          drop_last=True,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Occurences of zero/one vs total amount of \n",
            "classes for train and test set:\n",
            "-------------------------------\n",
            "65104 79474 144578\n",
            "8149 14546 22695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zEhs--jkfUGa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ci9PWyjRPUbW"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5ZuzphFRPUbW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(200, 12, kernel_size=5, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(12, 5, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(5*5, 10)\n",
        "        self.fc3 = nn.Linear(10 + size_user_info, num_classes)\n",
        "        self.out_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, user_data):\n",
        "        x = x.view(batch_size, 200, 7).float()\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.cat((x, user_data.float()), 1)\n",
        "        x = self.fc3(x)\n",
        "        x = self.out_act(x)\n",
        "        return x\n",
        "\n",
        "model = ConvNet(num_classes).to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7mFcxQnvbvHp",
        "colab_type": "code",
        "outputId": "4b7ce2fc-bd40-4a3e-e7c6-d19bec9733ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "pytorch_total_params"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12476"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "metadata": {
        "id": "j_Sg1xu6LxqK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (Optional) Load model"
      ]
    },
    {
      "metadata": {
        "id": "RJkcal8DLyIF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base_str = '/content/gdrive/My Drive/Colab Notebooks/data/btc'\n",
        "model = model.load_state_dict(torch.load('{}/model_{}_{}_{}.ckpt'.format(\n",
        "    base_str, num_epochs, batch_size, 'cluster')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-QvA-Tif6Ea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Methods"
      ]
    },
    {
      "metadata": {
        "id": "cUXpr9gDb2jG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate_model(val_loader):\n",
        "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        losses = []\n",
        "        for images, labels, user_data, dates in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            user_data = user_data.to(device)\n",
        "            outputs = model(images, user_data)\n",
        "            predicted = (0.5 > outputs).float() * 1\n",
        "            predicted = [x[0] for x in predicted.data.cpu().numpy()]\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            losses.append(loss.data.cpu().numpy())\n",
        "            \n",
        "            total += labels.size(0)\n",
        "            equals = np.sum(predicted == labels.data.cpu().numpy())\n",
        "            correct += equals\n",
        "        acc = 100 * correct / total\n",
        "        losses = np.mean(losses)\n",
        "        print('Epoch [{}/{}], val acc: {}, val loss: {}'\n",
        "              .format(epoch + 1, num_epochs, acc, losses))\n",
        "        return losses, acc\n",
        "\n",
        "def train_model(train_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "    for i, (images, labels, user_data, dates) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        user_data = user_data.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images, user_data)\n",
        "        predicted = (0.5 > outputs).float() * 1\n",
        "        predicted = [x[0] for x in predicted.data.cpu().numpy()]\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        losses.append(loss.data.cpu().numpy())\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        equals = np.sum(predicted == labels.data.cpu().numpy())\n",
        "        correct += equals\n",
        "\n",
        "        if i % int(len(train_loader) / 3)  == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "    return np.mean(losses), 100 * correct / total\n",
        "\n",
        "\n",
        "def test_model(test_loader):\n",
        "    # Test the model\n",
        "#     X_test_lr, y_test_lr = [], []\n",
        "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels, user_data, dates in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            user_data = user_data.to(device)\n",
        "            outputs = model(images, user_data)\n",
        "            predicted = (0.5 > outputs).float() * 1\n",
        "            predicted = [x[0] for x in predicted.data.cpu().numpy()]\n",
        "#             X_test_lr.extend(np.concatenate((outputs.data.cpu().numpy(), \n",
        "#                                               user_data.data.cpu().numpy()), \n",
        "#                                              axis=1))\n",
        "#             y_test_lr.extend(labels.data.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            equals = np.sum(predicted == labels.data.cpu().numpy())\n",
        "            correct += equals\n",
        "        acc = 100 * correct / total\n",
        "        print('Test Accuracy of the model on the test images: {} %'.format(acc))\n",
        "        \n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "caOTV5E5aLf5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## K-Fold CV"
      ]
    },
    {
      "metadata": {
        "id": "j2AGZKdtaRag",
        "colab_type": "code",
        "outputId": "90679c48-93f1-4c80-9ba9-6ce2e1c4b363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
        "                                                       verbose=True, patience=8)\n",
        "n_folds = 2\n",
        "kf = KFold(n_splits=n_folds, shuffle=True)\n",
        "fold_val_losses = []\n",
        "saved_plots = []\n",
        "file_name_base = '/content/gdrive/My Drive/Colab Notebooks/data/btc/plots/{}_fold_'.format(time.time())\n",
        "fold = 0\n",
        "for train_indexes, validation_indexes in kf.split(full_dataset):\n",
        "    fold += 1\n",
        "    liveloss = PlotLosses(save_img=True, file_name=file_name_base + str(fold) + '.png')\n",
        "\n",
        "    train = torch.utils.data.dataset.Subset(full_dataset, train_indexes)\n",
        "    validation = torch.utils.data.dataset.Subset(full_dataset, validation_indexes)\n",
        "    \n",
        "    model = ConvNet(num_classes).to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        \n",
        "    # Set train and validation data loaders.\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train,\n",
        "                                               batch_size=batch_size,\n",
        "                                               drop_last=True,\n",
        "                                               shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=validation,\n",
        "                                              batch_size=batch_size,\n",
        "                                              drop_last=True,\n",
        "                                              shuffle=True)\n",
        "    total_step = len(train_loader)\n",
        "    print('-------------------------------')\n",
        "    print('Starting training for new fold')\n",
        "    print('-------------------------------')\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_model(train_loader)\n",
        "        val_loss, val_acc = validate_model(val_loader)\n",
        "        scheduler.step(val_loss)\n",
        "        liveloss.update({\n",
        "            'log loss': train_loss,\n",
        "            'val_log loss': val_loss,\n",
        "            'accuracy': train_acc,\n",
        "            'val_accuracy': val_acc\n",
        "        })\n",
        "        liveloss.draw()\n",
        "    \n",
        "    print('Final validation loss for current fold: {}'.format(val_loss))\n",
        "    fold_val_losses.append(val_loss)\n",
        "    saved_plots.append(liveloss)\n",
        "    \n",
        "print('-------------------------------')\n",
        "print('{}-fold validation has been executed, mean validation loss: {}'.\n",
        "      format(n_folds, np.mean(fold_val_losses)))\n",
        "\n",
        "np.save('/content/gdrive/My Drive/Colab Notebooks/data/btc/plots_crossval_{}_{}_{}.npy'.\n",
        "        format(num_epochs, batch_size, time.time()), np.array(saved_plots))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------\n",
            "Starting training for new fold\n",
            "-------------------------------\n",
            "Epoch [1/50], Step [1/18072], Loss: 0.6744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])) is deprecated. Please ensure they have the same size.\n",
            "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [6025/18072], Loss: 0.7567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CV-R2o0ox4fg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Note: unfortunately the function above overwrites older plots. However, I've saved them into the list below. So you can access them by simply accessing the corresponding index."
      ]
    },
    {
      "metadata": {
        "id": "Vya2JCCMwbd_",
        "colab_type": "code",
        "outputId": "9a8562cb-ffe3-4e88-c6c5-beae59c00fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "cell_type": "code",
      "source": [
        "saved_plots[1].draw()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAE1CAYAAAD6akEFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsqkeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd680140080>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "log loss:\n",
            "training   (min:    0.696, max:    0.707, cur:    0.707)\n",
            "validation (min:    0.688, max:    0.738, cur:    0.694)\n",
            "\n",
            "accuracy:\n",
            "training   (min:   46.925, max:   47.906, cur:   46.925)\n",
            "validation (min:   44.697, max:   51.025, cur:   50.624)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dagH0eMcPUbd"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Train and test final model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8tX7-se4PUbe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Init model\n",
        "model = ConvNet(num_classes).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Set train and validation data loaders.\n",
        "len_train = int(len(full_dataset) * 0.8)\n",
        "len_val = len(full_dataset) - len_train\n",
        "train, validation = torch.utils.data.dataset.random_split(full_dataset, [len_train, len_val])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train,\n",
        "                                           batch_size=batch_size,\n",
        "                                           drop_last=True,\n",
        "                                           shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=validation,\n",
        "                                          batch_size=batch_size,\n",
        "                                          drop_last=True,\n",
        "                                          shuffle=True)\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
        "                                                       verbose=True, patience=2)\n",
        "liveloss = PlotLosses(save_img=True, file_name='/content/gdrive/My Drive/Colab Notebooks/data/btc/plots/{}_result_train.png'.format(time.time()))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_model(train_loader)\n",
        "    val_loss, val_acc = validate_model(val_loader)\n",
        "    scheduler.step(val_loss)\n",
        "    liveloss.update({\n",
        "        'log loss': train_loss,\n",
        "        'val_log loss': val_loss,\n",
        "        'accuracy': train_acc,\n",
        "        'val_accuracy': val_acc\n",
        "    })\n",
        "    liveloss.draw()\n",
        "\n",
        "print('Final validation loss for current fold: {}'.format(val_loss))\n",
        "fold_val_losses.append(val_loss)\n",
        "    \n",
        "acc = test_model(val_loader)\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/data/btc/models/model_{}_{}_{}_{}_{}.ckpt'.format(\n",
        "    num_epochs, batch_size, model_type, acc, time.time()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pUXnWRX8fpRb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trading strategy"
      ]
    },
    {
      "metadata": {
        "id": "8Q-u5FZdicm5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Perform forward pass on training set to generate data for Logistic Regression and majority vote"
      ]
    },
    {
      "metadata": {
        "id": "h2cAcEgQibwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train,batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "train_lr = None\n",
        "\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    for images, labels, user_data, dates in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        user_data = user_data.to(device)\n",
        "        outputs = model(images, user_data)\n",
        "        predicted = (0.5 > outputs).float() * 1\n",
        "        \n",
        "        predicted = np.array([x[0] for x in predicted.data.cpu().numpy()])\n",
        "        outputs = np.array([x[0] for x in outputs.data.cpu().numpy()])\n",
        "        labels = np.array([x for x in labels.data.cpu().numpy()])\n",
        "        temp = np.dstack((outputs, labels, \n",
        "                               user_data.data.cpu().numpy()[:,-1],\n",
        "                               dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "        if train_lr is None:\n",
        "            train_lr = temp\n",
        "        else:\n",
        "            train_lr = np.concatenate((train_lr, temp), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fR4_etFQ79j7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "test_lr = None\n",
        "\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    for images, labels, user_data, dates in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        user_data = user_data.to(device)\n",
        "        outputs = model(images, user_data)\n",
        "        predicted = (0.5 > outputs).float() * 1\n",
        "        \n",
        "        predicted = np.array([x[0] for x in predicted.data.cpu().numpy()])\n",
        "        outputs = np.array([x[0] for x in outputs.data.cpu().numpy()])\n",
        "        #labels = np.array([x for x in labels.data.cpu().numpy()])\n",
        "        temp = np.dstack((outputs, labels.data.cpu().numpy(), \n",
        "                               user_data.data.cpu().numpy()[:,-1],\n",
        "                               dates.data.cpu().numpy(), predicted)).squeeze()\n",
        "        if test_lr is None:\n",
        "            test_lr = temp\n",
        "        else:\n",
        "            test_lr = np.concatenate((test_lr, temp), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WuPr_kfWs9Ma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create dataframe for LR and MV"
      ]
    },
    {
      "metadata": {
        "id": "ntUFSq78tABD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame({'output':train_lr[:,0],'true':train_lr[:,1], \n",
        "                                   'user':train_lr[:,2], 'date':train_lr[:,3], 'pred_max':train_lr[:,4]})\n",
        "test_df = pd.DataFrame({'output':test_lr[:,0],'true':test_lr[:,1], \n",
        "                                   'user':test_lr[:,2], 'date':test_lr[:,3], 'pred_max':test_lr[:,4]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6yyRUuOVQKn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import seaborn \n",
        "\n",
        "seaborn.distplot(train_df['output'][train_df['date'] == 17548.0], bins=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7cc1r0iv68h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Perform Majority Vote and predict mean per date"
      ]
    },
    {
      "metadata": {
        "id": "97z4tOppv7Ec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_MV = train_df[['date', 'output', 'true', 'pred_max']]\n",
        "cnt_MV = 0\n",
        "cnt_max = 0\n",
        "pairs = []\n",
        "dates = data_MV['date'].unique()\n",
        "score = 0\n",
        "for i in range(35, 75):\n",
        "    threshold = i / 100\n",
        "    cnt_MV = 0\n",
        "    cnt_max = 0\n",
        "    pairs = []\n",
        "    for date in dates:\n",
        "        temp = data_MV[data_MV['date'] == date]\n",
        "        y_true = temp['true'].unique()[0]\n",
        "        MV = temp['pred_max'].value_counts().index[0]\n",
        "        max_all = temp['output'].mean()\n",
        "        max_all = 1 if max_all > threshold else 0\n",
        "\n",
        "        if y_true == MV:\n",
        "            cnt_MV += 1\n",
        "        if y_true == max_all:\n",
        "            cnt_max += 1\n",
        "            #print('correct', y_true, temp['output'].mean())\n",
        "        else:\n",
        "            #print('wrong', y_true, temp['output'].mean())\n",
        "            pairs.append([max_all, y_true])\n",
        "    new_score = cnt_max / len(dates)\n",
        "    if new_score > score:\n",
        "        best_thresh = threshold\n",
        "        score = new_score\n",
        "#print(cnt_MV / len(dates), cnt_max / len(dates), len(dates))\n",
        "print('The best score was {} with threshold {}'.format(score, best_thresh))\n",
        "print('Performing MV resulted in an acc of: {}'.format(cnt_MV / len(dates)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFOK6CrUYuMm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use best threshold for training set on test set to measure performance"
      ]
    },
    {
      "metadata": {
        "id": "4LQEBa6ZYtvF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_MV = test_df[['date', 'output', 'true', 'pred_max']]\n",
        "cnt_max = 0\n",
        "pairs = []\n",
        "dates = data_MV['date'].unique()\n",
        "\n",
        "for date in dates:\n",
        "    temp = data_MV[data_MV['date'] == date]\n",
        "    y_true = temp['true'].unique()[0]\n",
        "    MV = temp['pred_max'].value_counts().index[0]\n",
        "    max_all = temp['output'].mean()\n",
        "    max_all = 1 if max_all > best_thresh else 0\n",
        "\n",
        "    if y_true == max_all:\n",
        "        cnt_max += 1\n",
        "        print('correct', y_true, temp['output'].mean())\n",
        "    else:\n",
        "        print('wrong', y_true, temp['output'].mean())\n",
        "        pairs.append([max_all, y_true])\n",
        "\n",
        "print('The score on the test set is: {} with threshold {}'.format(cnt_max / len(dates), best_thresh))\n",
        "print(len(pairs), len([x for x in pairs if x[0] == 1]), len([x for x in pairs if x[0] == 0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7qHZlLPAiFdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create required dataset for Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "0fEtBHStVFSy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "For logistic regression we'd the following input:\n",
        "[user_pred_1, .... user_pred_n]\n",
        "\n",
        "We create a matrix with all zeroes and replace the user predictions if there\n",
        "are any. If there's none, then we simply leave it at zero.\n",
        "\n",
        "So we get a matrix per day with predictions for all users.\n",
        "'''\n",
        "\n",
        "\n",
        "# # Get all users\n",
        "users = train_df['user'].unique()\n",
        "\n",
        "# # Assign users a unique id from 0 to n unique ids, \n",
        "# # create a dictionairy using training data and re-use during test time.\n",
        "user_ids_lr = {}\n",
        "for i, v in enumerate(users):\n",
        "    user_ids_lr[v] = i\n",
        "\n",
        "# Loop over dates, check if user has prediction\n",
        "X_train = None\n",
        "y_train = []\n",
        "cnt = 0\n",
        "for date in train_df['date'].unique():\n",
        "    cnt += 1\n",
        "    temp = train_df[train_df['date'] == date]\n",
        "    arr = np.zeros(len(users))\n",
        "    for row in temp.itertuples():\n",
        "        usr = row[5]\n",
        "        pred = row[2]\n",
        "        lbl = row[4]\n",
        "        arr[user_ids_lr[usr]] = pred\n",
        "    y_train.append([lbl])\n",
        "    if X_train is None:\n",
        "        X_train = arr\n",
        "    else:\n",
        "        X_train = np.vstack((X_train, arr))\n",
        "\n",
        "y_train = np.array(y_train)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e3J0LSZwTv9s",
        "colab_type": "code",
        "outputId": "ebcacd3b-283b-4043-c15d-0a2f07a125af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12400
        }
      },
      "cell_type": "code",
      "source": [
        "# Loop over dates, check if user has prediction\n",
        "X_test = None\n",
        "y_test = []\n",
        "cnt = 0\n",
        "for date in test_df['date'].unique():\n",
        "    cnt += 1\n",
        "    temp = test_df[test_df['date'] == date]\n",
        "    arr = np.zeros(len(users))\n",
        "    for row in temp.itertuples():\n",
        "        usr = row[5]\n",
        "        pred = row[2]\n",
        "        lbl = row[4]\n",
        "        try:\n",
        "            arr[user_ids_lr[usr]] = pred\n",
        "        except:\n",
        "            print('User {} not found'.format(usr))\n",
        "    y_test.append([lbl])\n",
        "    if X_test is None:\n",
        "        X_test = arr\n",
        "    else:\n",
        "        X_test = np.vstack((X_test, arr))\n",
        "\n",
        "y_test = np.array(y_test)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User 1779.0 not found\n",
            "User 0.16769836803601576 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.9994372537985369 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.9994372537985369 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.9994372537985369 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.9994372537985369 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.4181204276871131 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.4181204276871131 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.050647158131682614 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.7822172200337648 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.050647158131682614 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.48958919527293193 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.4181204276871131 not found\n",
            "User 1779.0 not found\n",
            "User 0.7822172200337648 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.7822172200337648 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.27968486212718063 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.48958919527293193 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.27968486212718063 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.4181204276871131 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.27968486212718063 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.4181204276871131 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 0.558244231851435 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n",
            "User 1779.0 not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VNimcQC82Oog",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Perform Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "OXSs7t5-wC67",
        "colab_type": "code",
        "outputId": "0e57229e-8a46-4b38-cf87-63431a3c9aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "# instantiate the model (using the default parameters)\n",
        "logreg = LogisticRegression(C=0.55)\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_train)\n",
        "\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_train, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[257,  23],\n",
              "       [ 60, 187]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "metadata": {
        "id": "0fC8rTFFTEX0",
        "colab_type": "code",
        "outputId": "db41ef24-ceff-4cb1-c8a6-80c790586c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16,  0],\n",
              "       [ 6,  1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "metadata": {
        "id": "HENI1DubUnfL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "D3DAOND83O0a",
        "colab_type": "code",
        "outputId": "1560497e-6ebd-4825-9d41-90fd6f070ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "class_names=[0,1] # name  of classes\n",
        "fig, ax = plt.subplots()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks, class_names)\n",
        "plt.yticks(tick_marks, class_names)\n",
        "\n",
        "# create heatmap\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,257.44,'Predicted label')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAFACAYAAACFo7oqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd71880b0b8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}